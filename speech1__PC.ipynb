{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from scipy.io import wavfile\n",
        "import os.path\n",
        "import IPython.display\n",
        "import seaborn as sns\n",
        "import librosa\n",
        "import librosa.display\n",
        "import soundfile\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Conv1D, MaxPooling1D, Flatten, BatchNormalization\n",
        "from keras import optimizers,regularizers\n",
        "\n",
        "import warnings"
      ],
      "metadata": {
        "id": "ia-kxB92fN7m"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYqfTVd-ACo1",
        "outputId": "4ab03905-97b7-482f-8d80-c1e98239c297"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import wave\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pydub import AudioSegment\n",
        "from IPython.display import Audio, display\n",
        "import librosa as lib\n",
        "import librosa.display\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41Ep1fpfSEWa",
        "outputId": "07409e5d-07e1-4766-c3e4-1cd36e1ab206"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "23ihbA4UQk6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12f72f3c-4850-42d5-812e-efc529ea2d85"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "PATH = \"/content/drive/MyDrive/Colab Notebooks/Crema\"\n",
        "AUDIO_PATH = \"/content/drive/MyDrive/Colab Notebooks/Crema/1001_DFA_ANG_XX.wav\""
      ],
      "metadata": {
        "id": "bG0Iuy17RwCX"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zeroCrossingRate(audio):\n",
        "  return lib.feature.zero_crossing_rate(audio)"
      ],
      "metadata": {
        "id": "hZRUzrRHI3x4"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def energy(audio):\n",
        "  # using a spectrogram will give a more accurate representation\n",
        "  # of energy over time because its frames can be windowed\n",
        "  S, phase = lib.magphase(lib.stft(audio))\n",
        "  return lib.feature.rms(S=S).mean()"
      ],
      "metadata": {
        "id": "YUj1b0stI8Jp"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def melSpectrogram(audio, sr):\n",
        "  mel_spectrogram = lib.feature.melspectrogram(y=audio, sr=sr, n_fft=200)\n",
        "  log_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
        "  return log_mel_spectrogram.mean()"
      ],
      "metadata": {
        "id": "t03kfNaMJEUx"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chromaStft(audio, sr):\n",
        "  stft = np.abs(librosa.stft(audio))\n",
        "  return librosa.feature.chroma_stft(S=stft, sr=sr, n_fft=200).mean()"
      ],
      "metadata": {
        "id": "RFqdKI4dj-sx"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mfcc(data, sr):\n",
        "  return librosa.feature.mfcc(y=data, sr=sr, n_fft=200).mean()"
      ],
      "metadata": {
        "id": "bwJma-zak5ai"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tonnetz(data, sr):\n",
        "  return librosa.feature.tonnetz(y=data, sr=sr).mean();"
      ],
      "metadata": {
        "id": "fHiHGyRoqeAa"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def playAudio(audio_file):\n",
        "  audio = AudioSegment.from_wav(audio_file)\n",
        "  # Play the audio\n",
        "  audio.export('temp_audio.wav', format='wav')\n",
        "  audio_data = open('temp_audio.wav', 'rb').read()\n",
        "  display(Audio(audio_data))\n",
        "  # Delete the temporary audio file\n",
        "  os.remove('temp_audio.wav')"
      ],
      "metadata": {
        "id": "GZLRcaVEinfV"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_waveform(audio, sr):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.plot(audio)\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.title('Waveform')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "VErUAYc_mIzR"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadAndListenToAudiolib(dataset_path, class_name):\n",
        "  D, Y = [], []\n",
        "  # defining the regular expression\n",
        "  audio_files = glob.glob(os.path.join(dataset_path, f\"*{class_name}*.wav\"))\n",
        "  print(class_name)\n",
        "  # play the audio\n",
        "  '''playAudio(audio_files[0])'''\n",
        "  # plot the spectrum\n",
        "  '''audio, sr = lib.load(audio_files[0])'''\n",
        "  '''visualize_waveform(audio, sr)'''\n",
        "  # plotWaveform(audio_files[0], class_name)\n",
        "  for audio_file in audio_files: \n",
        "    # load the audio file\n",
        "    audio, sr = lib.load(audio_file)\n",
        "    # extract zero crossing rate\n",
        "    zcr = zeroCrossingRate(audio)\n",
        "    # extract energy\n",
        "    rms = energy(audio)\n",
        "    # extract mel spectrogram\n",
        "    #mel_spec = melSpectrogram(audio, sr)\n",
        "    # chroma stft\n",
        "    cs = chromaStft(audio, sr)\n",
        "    mfc = mfcc(audio, sr)\n",
        "    '''ton = tonnetz(audio, sr)'''\n",
        "    combined_features = np.concatenate(([rms, cs, mfc], (np.pad(zcr[0], (0, 300 - len(zcr[0]))))))\n",
        "    print(len(combined_features))\n",
        "    '''combined_features = [zcr, rms, mel_spec, cs, mfc, ton]'''\n",
        "    # print(combined_features)\n",
        "    '''D.append(combined_features)'''\n",
        "    D.append(combined_features)\n",
        "    Y.append(class_name)\n",
        "  return D, Y"
      ],
      "metadata": {
        "id": "IsHsSI2rrbMa"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadDatalib(PATH):\n",
        "  D, Y = [], []\n",
        "  classes = [\"SAD\", \"ANG\", \"DIS\", \"FEA\", \"HAP\", \"NEU\"]\n",
        "  for cls in classes: \n",
        "    d, y = loadAndListenToAudiolib(PATH, cls)\n",
        "    # D = np.concatenate((D, d))\n",
        "    D.extend(d)\n",
        "    Y = np.concatenate((Y, y))\n",
        "  return D, Y"
      ],
      "metadata": {
        "id": "sHubX5-dsmWa"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "D, Y = loadDatalib(PATH)"
      ],
      "metadata": {
        "id": "OcBJBL3tLS2O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "046ac275-1db7-46fa-df41-d5cc39b2790e"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SAD\n",
            "303\n",
            "303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/librosa/feature/spectral.py:2157: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
            "  mel_basis = filters.mel(sr=sr, n_fft=n_fft, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/librosa/core/pitch.py:102: UserWarning: Trying to estimate tuning from empty frequency set.\n",
            "  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "DIS\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "FEA\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "HAP\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "NEU\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n",
            "303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "D = np.array(D)\n",
        "D.shape"
      ],
      "metadata": {
        "id": "srtVtP5lIdrm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f85b452-1b0a-4ec3-a6ca-a5152b048a5d"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7452, 303)"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y.shape"
      ],
      "metadata": {
        "id": "X6FCElq7I01d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b74e494d-39fc-489d-9f9d-0c637b4e9038"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7452,)"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(Y)"
      ],
      "metadata": {
        "id": "pKCQdLtHJU3g"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "APTvq_yvW_6P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "ac5ba7b2-cfc5-41f0-f1f3-4a10df96f25e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     0\n",
              "0  SAD\n",
              "1  SAD\n",
              "2  SAD\n",
              "3  SAD\n",
              "4  SAD"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6978203f-9ad8-4e9f-a5ff-637580c23a4d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SAD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SAD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SAD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SAD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SAD</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6978203f-9ad8-4e9f-a5ff-637580c23a4d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6978203f-9ad8-4e9f-a5ff-637580c23a4d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6978203f-9ad8-4e9f-a5ff-637580c23a4d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the get_dummies() method to one-hot encode the labels\n",
        "one_hot_labels = pd.get_dummies(df)\n",
        "\n",
        "# Convert the DataFrame to a NumPy array\n",
        "one_hot_labels = one_hot_labels.to_numpy()\n",
        "\n",
        "print(one_hot_labels.shape)"
      ],
      "metadata": {
        "id": "hSuuqU8YW-pj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c66b8d8c-000b-41e5-a595-66e1c4aa9325"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7452, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = one_hot_labels"
      ],
      "metadata": {
        "id": "A5oOIKMWXEyr"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y.shape"
      ],
      "metadata": {
        "id": "UfZhBxf5XQbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73ffb507-99f2-47f5-ab02-63bb4a7b15a4"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7452, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(D, Y, test_size=0.3, random_state=69, shuffle=True, stratify=Y)"
      ],
      "metadata": {
        "id": "_8p_f52KRIHo"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "YpxgNsNNYY6L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "902e5a65-cc44-4473-bd54-09b874b52b4d"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5216, 303)\n",
            "(2236, 303)\n",
            "(5216, 6)\n",
            "(2236, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))"
      ],
      "metadata": {
        "id": "Y1sK4j5Ubnhu"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape, X_test.shape)"
      ],
      "metadata": {
        "id": "9w6_MobXNagQ",
        "outputId": "0f743ac2-c0f5-4775-bd1b-2d6b120b6145",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5216, 303, 1) (2236, 303, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unicheck = np.unique(y_test, axis=0)"
      ],
      "metadata": {
        "id": "MNkEcVh8Yh9X"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unicheck"
      ],
      "metadata": {
        "id": "AVdqdhg6ZMmg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7bf9e6d-5b31-49ef-dc1f-0353d0013165"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 1, 0],\n",
              "       [0, 0, 0, 1, 0, 0],\n",
              "       [0, 0, 1, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Apply PCA to the data\n",
        "pca = PCA(n_components=100)\n",
        "D_pca = pca.fit_transform(D)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(D_pca, Y, test_size=0.3, random_state=69, shuffle=True, stratify=Y)\n",
        "'''\n",
        "X_train, X_test, y_train, y_test = train_test_split(D, Y, test_size=0.3, random_state=69, shuffle=True, stratify=Y)\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))'''\n",
        "model = Sequential()\n",
        "model.add(Conv1D(128, 3, input_shape=(X_train.shape[1], 1), kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "model.add(Conv1D(256, 3, kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "model.add(Conv1D(512, 3, kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "model.add(Conv1D(1024, 3,  kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(256, kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(128, kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer=optimizers.Adam(lr=0.0005),\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "# Specify thecallbacks for the model\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = model.fit(X_train, y_train,\n",
        "                    validation_data=(X_train, y_train),\n",
        "                    epochs=50,\n",
        "                    batch_size=32,\n",
        "                    callbacks=callbacks)\n",
        "'''model.compile(loss='categorical_crossentropy',\n",
        "             optimizer=optimizers.Adam(lr=0.0005),\n",
        "             metrics=['accuracy'])\n",
        "'''\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "1hbH-j7P6ZZ1",
        "outputId": "bbfba7e8-59f6-4614-83e5-0501d43c0856",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163/163 [==============================] - 4s 14ms/step - loss: 3.9512 - accuracy: 0.3269 - val_loss: 3.9271 - val_accuracy: 0.1708\n",
            "Epoch 2/50\n",
            "163/163 [==============================] - 3s 20ms/step - loss: 3.2940 - accuracy: 0.3537 - val_loss: 3.4352 - val_accuracy: 0.1787\n",
            "Epoch 3/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 2.8902 - accuracy: 0.3673 - val_loss: 2.9722 - val_accuracy: 0.2065\n",
            "Epoch 4/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 2.6320 - accuracy: 0.3696 - val_loss: 2.4771 - val_accuracy: 0.4070\n",
            "Epoch 5/50\n",
            "163/163 [==============================] - 2s 15ms/step - loss: 2.4378 - accuracy: 0.3811 - val_loss: 2.3019 - val_accuracy: 0.4172\n",
            "Epoch 6/50\n",
            "163/163 [==============================] - 2s 11ms/step - loss: 2.3034 - accuracy: 0.3988 - val_loss: 2.1655 - val_accuracy: 0.4379\n",
            "Epoch 7/50\n",
            "163/163 [==============================] - 3s 17ms/step - loss: 2.1781 - accuracy: 0.3993 - val_loss: 2.0979 - val_accuracy: 0.4133\n",
            "Epoch 8/50\n",
            "163/163 [==============================] - 2s 12ms/step - loss: 2.1064 - accuracy: 0.3969 - val_loss: 2.1169 - val_accuracy: 0.3706\n",
            "Epoch 9/50\n",
            "163/163 [==============================] - 2s 12ms/step - loss: 2.0289 - accuracy: 0.4084 - val_loss: 1.9926 - val_accuracy: 0.4011\n",
            "Epoch 10/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 1.9647 - accuracy: 0.4022 - val_loss: 1.8912 - val_accuracy: 0.4300\n",
            "Epoch 11/50\n",
            "163/163 [==============================] - 3s 19ms/step - loss: 1.8976 - accuracy: 0.4084 - val_loss: 1.9205 - val_accuracy: 0.3587\n",
            "Epoch 12/50\n",
            "163/163 [==============================] - 2s 12ms/step - loss: 1.8576 - accuracy: 0.4116 - val_loss: 1.8079 - val_accuracy: 0.4235\n",
            "Epoch 13/50\n",
            "163/163 [==============================] - 2s 12ms/step - loss: 1.8163 - accuracy: 0.4078 - val_loss: 1.7601 - val_accuracy: 0.4266\n",
            "Epoch 14/50\n",
            "163/163 [==============================] - 2s 15ms/step - loss: 1.7832 - accuracy: 0.4133 - val_loss: 1.7348 - val_accuracy: 0.4346\n",
            "Epoch 15/50\n",
            "163/163 [==============================] - 2s 12ms/step - loss: 1.7598 - accuracy: 0.4227 - val_loss: 1.7167 - val_accuracy: 0.4266\n",
            "Epoch 16/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 1.7290 - accuracy: 0.4262 - val_loss: 1.7216 - val_accuracy: 0.4084\n",
            "Epoch 17/50\n",
            "163/163 [==============================] - 2s 15ms/step - loss: 1.6987 - accuracy: 0.4337 - val_loss: 1.6935 - val_accuracy: 0.4473\n",
            "Epoch 18/50\n",
            "163/163 [==============================] - 2s 12ms/step - loss: 1.6839 - accuracy: 0.4433 - val_loss: 1.6372 - val_accuracy: 0.4634\n",
            "Epoch 19/50\n",
            "163/163 [==============================] - 2s 11ms/step - loss: 1.6653 - accuracy: 0.4410 - val_loss: 1.6390 - val_accuracy: 0.4762\n",
            "Epoch 20/50\n",
            "163/163 [==============================] - 2s 11ms/step - loss: 1.6396 - accuracy: 0.4544 - val_loss: 1.6605 - val_accuracy: 0.4519\n",
            "Epoch 21/50\n",
            "163/163 [==============================] - 2s 11ms/step - loss: 1.6348 - accuracy: 0.4586 - val_loss: 1.5707 - val_accuracy: 0.4831\n",
            "Epoch 22/50\n",
            "163/163 [==============================] - 2s 12ms/step - loss: 1.6222 - accuracy: 0.4638 - val_loss: 1.7018 - val_accuracy: 0.3905\n",
            "Epoch 23/50\n",
            "163/163 [==============================] - 4s 23ms/step - loss: 1.6093 - accuracy: 0.4762 - val_loss: 1.5410 - val_accuracy: 0.5098\n",
            "Epoch 24/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 1.5879 - accuracy: 0.4927 - val_loss: 1.7950 - val_accuracy: 0.4049\n",
            "Epoch 25/50\n",
            "163/163 [==============================] - 2s 11ms/step - loss: 1.5758 - accuracy: 0.5025 - val_loss: 1.5643 - val_accuracy: 0.5000\n",
            "Epoch 26/50\n",
            "163/163 [==============================] - 2s 12ms/step - loss: 1.5448 - accuracy: 0.5234 - val_loss: 1.5221 - val_accuracy: 0.5439\n",
            "Epoch 27/50\n",
            "163/163 [==============================] - 2s 12ms/step - loss: 1.5467 - accuracy: 0.5343 - val_loss: 1.6250 - val_accuracy: 0.4801\n",
            "Epoch 28/50\n",
            "163/163 [==============================] - 2s 12ms/step - loss: 1.5143 - accuracy: 0.5508 - val_loss: 1.6067 - val_accuracy: 0.5437\n",
            "Epoch 29/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 1.4826 - accuracy: 0.5736 - val_loss: 1.4367 - val_accuracy: 0.5867\n",
            "Epoch 30/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 1.4594 - accuracy: 0.5966 - val_loss: 1.4716 - val_accuracy: 0.5924\n",
            "Epoch 31/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 1.3981 - accuracy: 0.6206 - val_loss: 1.3719 - val_accuracy: 0.6317\n",
            "Epoch 32/50\n",
            "163/163 [==============================] - 2s 12ms/step - loss: 1.3373 - accuracy: 0.6637 - val_loss: 1.3295 - val_accuracy: 0.6756\n",
            "Epoch 33/50\n",
            "163/163 [==============================] - 2s 12ms/step - loss: 1.3139 - accuracy: 0.6873 - val_loss: 1.3963 - val_accuracy: 0.6438\n",
            "Epoch 34/50\n",
            "163/163 [==============================] - 2s 11ms/step - loss: 1.2499 - accuracy: 0.7147 - val_loss: 1.3570 - val_accuracy: 0.6689\n",
            "Epoch 35/50\n",
            "163/163 [==============================] - 2s 12ms/step - loss: 1.2087 - accuracy: 0.7393 - val_loss: 1.5793 - val_accuracy: 0.5847\n",
            "Epoch 36/50\n",
            "163/163 [==============================] - 3s 18ms/step - loss: 1.1704 - accuracy: 0.7630 - val_loss: 1.2524 - val_accuracy: 0.7364\n",
            "Epoch 37/50\n",
            "163/163 [==============================] - 2s 12ms/step - loss: 1.1063 - accuracy: 0.7926 - val_loss: 1.3453 - val_accuracy: 0.7003\n",
            "Epoch 38/50\n",
            "163/163 [==============================] - 2s 11ms/step - loss: 1.0815 - accuracy: 0.8041 - val_loss: 1.2647 - val_accuracy: 0.7278\n",
            "Epoch 39/50\n",
            "163/163 [==============================] - 2s 11ms/step - loss: 1.0103 - accuracy: 0.8372 - val_loss: 1.0901 - val_accuracy: 0.7893\n",
            "Epoch 40/50\n",
            "163/163 [==============================] - 2s 11ms/step - loss: 1.0088 - accuracy: 0.8393 - val_loss: 1.1801 - val_accuracy: 0.7617\n",
            "Epoch 41/50\n",
            "163/163 [==============================] - 2s 12ms/step - loss: 0.9700 - accuracy: 0.8493 - val_loss: 1.0526 - val_accuracy: 0.8267\n",
            "Epoch 42/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 0.9467 - accuracy: 0.8635 - val_loss: 1.1051 - val_accuracy: 0.7981\n",
            "Epoch 43/50\n",
            "163/163 [==============================] - 2s 15ms/step - loss: 0.8504 - accuracy: 0.8972 - val_loss: 1.5574 - val_accuracy: 0.6863\n",
            "Epoch 44/50\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 0.9020 - accuracy: 0.8781 - val_loss: 1.0512 - val_accuracy: 0.8305\n",
            "Epoch 45/50\n",
            "163/163 [==============================] - 2s 11ms/step - loss: 0.8639 - accuracy: 0.8992 - val_loss: 0.8694 - val_accuracy: 0.8934\n",
            "Epoch 46/50\n",
            "163/163 [==============================] - 2s 12ms/step - loss: 0.8729 - accuracy: 0.8995 - val_loss: 1.0282 - val_accuracy: 0.8384\n",
            "Epoch 47/50\n",
            "163/163 [==============================] - 2s 11ms/step - loss: 0.8380 - accuracy: 0.9135 - val_loss: 1.0856 - val_accuracy: 0.8125\n",
            "Epoch 48/50\n",
            "163/163 [==============================] - 2s 12ms/step - loss: 0.8356 - accuracy: 0.9068 - val_loss: 0.9762 - val_accuracy: 0.8537\n",
            "Epoch 49/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 0.8040 - accuracy: 0.9176 - val_loss: 1.5054 - val_accuracy: 0.6990\n",
            "Epoch 50/50\n",
            "163/163 [==============================] - 2s 14ms/step - loss: 0.7950 - accuracy: 0.9250 - val_loss: 0.8041 - val_accuracy: 0.9133\n",
            "Model: \"sequential_47\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_186 (Conv1D)         (None, 98, 128)           512       \n",
            "                                                                 \n",
            " batch_normalization_317 (Ba  (None, 98, 128)          512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_317 (Activation)  (None, 98, 128)          0         \n",
            "                                                                 \n",
            " max_pooling1d_186 (MaxPooli  (None, 49, 128)          0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_187 (Conv1D)         (None, 47, 256)           98560     \n",
            "                                                                 \n",
            " batch_normalization_318 (Ba  (None, 47, 256)          1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_318 (Activation)  (None, 47, 256)          0         \n",
            "                                                                 \n",
            " max_pooling1d_187 (MaxPooli  (None, 23, 256)          0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_188 (Conv1D)         (None, 21, 512)           393728    \n",
            "                                                                 \n",
            " batch_normalization_319 (Ba  (None, 21, 512)          2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_319 (Activation)  (None, 21, 512)          0         \n",
            "                                                                 \n",
            " max_pooling1d_188 (MaxPooli  (None, 10, 512)          0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_189 (Conv1D)         (None, 8, 1024)           1573888   \n",
            "                                                                 \n",
            " batch_normalization_320 (Ba  (None, 8, 1024)          4096      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_320 (Activation)  (None, 8, 1024)          0         \n",
            "                                                                 \n",
            " max_pooling1d_189 (MaxPooli  (None, 4, 1024)          0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " flatten_46 (Flatten)        (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_184 (Dense)           (None, 512)               2097664   \n",
            "                                                                 \n",
            " batch_normalization_321 (Ba  (None, 512)              2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_321 (Activation)  (None, 512)              0         \n",
            "                                                                 \n",
            " dropout_138 (Dropout)       (None, 512)               0         \n",
            "                                                                 \n",
            " dense_185 (Dense)           (None, 256)               131328    \n",
            "                                                                 \n",
            " batch_normalization_322 (Ba  (None, 256)              1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_322 (Activation)  (None, 256)              0         \n",
            "                                                                 \n",
            " dropout_139 (Dropout)       (None, 256)               0         \n",
            "                                                                 \n",
            " dense_186 (Dense)           (None, 128)               32896     \n",
            "                                                                 \n",
            " batch_normalization_323 (Ba  (None, 128)              512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_323 (Activation)  (None, 128)              0         \n",
            "                                                                 \n",
            " dropout_140 (Dropout)       (None, 128)               0         \n",
            "                                                                 \n",
            " dense_187 (Dense)           (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,340,614\n",
            "Trainable params: 4,334,982\n",
            "Non-trainable params: 5,632\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################################################################################################\n",
        "######## NOT USED\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Define the hyperparameters to tune\n",
        "#learning_rates = [0.1, 0.01, 0.001]\n",
        "regularization_params = [0.0001,0.001, 0.01]\n",
        "dropout_rates = [ 0.1,0.2,0.25, 0.3,0.4,0.5]\n",
        "#batch_size = 32\n",
        "\n",
        "# Loop over all combinations of hyperparameters\n",
        "#for lr in learning_rates:\n",
        "for reg_param in regularization_params:\n",
        "        for dr in dropout_rates:\n",
        "            model = Sequential()\n",
        "            model.add(Conv1D(128, 3, input_shape=(X_train.shape[1], 1), kernel_regularizer=regularizers.l2(reg_param)))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Activation('relu'))\n",
        "            model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "            model.add(Conv1D(256, 3, kernel_regularizer=regularizers.l2(reg_param)))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Activation('relu'))\n",
        "            model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "            model.add(Conv1D(512, 3, kernel_regularizer=regularizers.l2(reg_param)))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Activation('relu'))\n",
        "            model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "            model.add(Conv1D(1024, 3,  kernel_regularizer=regularizers.l2(reg_param)))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Activation('relu'))\n",
        "            model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "            model.add(Flatten())\n",
        "            model.add(Dense(512, kernel_regularizer=regularizers.l2(reg_param)))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Activation('relu'))\n",
        "            model.add(Dropout(dr))\n",
        "\n",
        "            model.add(Dense(256, kernel_regularizer=regularizers.l2(reg_param)))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Activation('relu'))\n",
        "            model.add(Dropout(dr))\n",
        "\n",
        "            model.add(Dense(128, kernel_regularizer=regularizers.l2(reg_param)))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Activation('relu'))\n",
        "            model.add(Dropout(dr))\n",
        "\n",
        "            model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "            model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=optimizers.Adam(lr=0.0005),\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "            model.summary()\n",
        "            # Train the model with the current hyperparameters\n",
        "            checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "            history = model.fit(X_train, y_train,\n",
        "                                batch_size=128,\n",
        "                                epochs=50,\n",
        "                                validation_data=(X_test, y_test),\n",
        "                                callbacks=[checkpoint])\n",
        "\n",
        "            # Evaluate the model on the test set\n",
        "            test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "            print(f\"reg_param={reg_param}, dr={dr}, test_acc={test_acc:.4f}\")\n",
        "           "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ll5QLXaah5b6",
        "outputId": "a21f4594-02a4-4f67-c430-6b33cdfd1718"
      },
      "execution_count": 64,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_52 (Conv1D)          (None, 301, 128)          512       \n",
            "                                                                 \n",
            " batch_normalization_84 (Bat  (None, 301, 128)         512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_84 (Activation)  (None, 301, 128)          0         \n",
            "                                                                 \n",
            " max_pooling1d_52 (MaxPoolin  (None, 150, 128)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_53 (Conv1D)          (None, 148, 256)          98560     \n",
            "                                                                 \n",
            " batch_normalization_85 (Bat  (None, 148, 256)         1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_85 (Activation)  (None, 148, 256)          0         \n",
            "                                                                 \n",
            " max_pooling1d_53 (MaxPoolin  (None, 74, 256)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_54 (Conv1D)          (None, 72, 512)           393728    \n",
            "                                                                 \n",
            " batch_normalization_86 (Bat  (None, 72, 512)          2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_86 (Activation)  (None, 72, 512)           0         \n",
            "                                                                 \n",
            " max_pooling1d_54 (MaxPoolin  (None, 36, 512)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_55 (Conv1D)          (None, 34, 1024)          1573888   \n",
            "                                                                 \n",
            " batch_normalization_87 (Bat  (None, 34, 1024)         4096      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_87 (Activation)  (None, 34, 1024)          0         \n",
            "                                                                 \n",
            " max_pooling1d_55 (MaxPoolin  (None, 17, 1024)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 17408)             0         \n",
            "                                                                 \n",
            " dense_52 (Dense)            (None, 512)               8913408   \n",
            "                                                                 \n",
            " batch_normalization_88 (Bat  (None, 512)              2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_88 (Activation)  (None, 512)               0         \n",
            "                                                                 \n",
            " dropout_39 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_53 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " batch_normalization_89 (Bat  (None, 256)              1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_89 (Activation)  (None, 256)               0         \n",
            "                                                                 \n",
            " dropout_40 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_54 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " batch_normalization_90 (Bat  (None, 128)              512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_90 (Activation)  (None, 128)               0         \n",
            "                                                                 \n",
            " dropout_41 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_55 (Dense)            (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,156,358\n",
            "Trainable params: 11,150,726\n",
            "Non-trainable params: 5,632\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.8291 - accuracy: 0.3689\n",
            "Epoch 1: val_accuracy improved from -inf to 0.24284, saving model to best_model.h5\n",
            "41/41 [==============================] - 5s 74ms/step - loss: 1.8291 - accuracy: 0.3689 - val_loss: 2.1557 - val_accuracy: 0.2428\n",
            "Epoch 2/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7050 - accuracy: 0.3945\n",
            "Epoch 2: val_accuracy did not improve from 0.24284\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.7083 - accuracy: 0.3923 - val_loss: 4.3648 - val_accuracy: 0.1726\n",
            "Epoch 3/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6462 - accuracy: 0.4225\n",
            "Epoch 3: val_accuracy did not improve from 0.24284\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.6466 - accuracy: 0.4212 - val_loss: 5.3730 - val_accuracy: 0.1704\n",
            "Epoch 4/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6293 - accuracy: 0.4277\n",
            "Epoch 4: val_accuracy did not improve from 0.24284\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.6285 - accuracy: 0.4293 - val_loss: 5.2824 - val_accuracy: 0.1704\n",
            "Epoch 5/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.5974 - accuracy: 0.4402\n",
            "Epoch 5: val_accuracy did not improve from 0.24284\n",
            "41/41 [==============================] - 2s 55ms/step - loss: 1.5974 - accuracy: 0.4402 - val_loss: 5.7617 - val_accuracy: 0.1704\n",
            "Epoch 6/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5642 - accuracy: 0.4582\n",
            "Epoch 6: val_accuracy improved from 0.24284 to 0.24553, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 67ms/step - loss: 1.5649 - accuracy: 0.4590 - val_loss: 4.4964 - val_accuracy: 0.2455\n",
            "Epoch 7/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5506 - accuracy: 0.4568\n",
            "Epoch 7: val_accuracy did not improve from 0.24553\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.5475 - accuracy: 0.4580 - val_loss: 3.3744 - val_accuracy: 0.2419\n",
            "Epoch 8/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5363 - accuracy: 0.4643\n",
            "Epoch 8: val_accuracy improved from 0.24553 to 0.32871, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 67ms/step - loss: 1.5372 - accuracy: 0.4647 - val_loss: 1.9759 - val_accuracy: 0.3287\n",
            "Epoch 9/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5051 - accuracy: 0.4713\n",
            "Epoch 9: val_accuracy did not improve from 0.32871\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5077 - accuracy: 0.4707 - val_loss: 2.9731 - val_accuracy: 0.2330\n",
            "Epoch 10/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4824 - accuracy: 0.4799\n",
            "Epoch 10: val_accuracy did not improve from 0.32871\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.4805 - accuracy: 0.4806 - val_loss: 2.4190 - val_accuracy: 0.2267\n",
            "Epoch 11/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4547 - accuracy: 0.4934\n",
            "Epoch 11: val_accuracy did not improve from 0.32871\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.4538 - accuracy: 0.4939 - val_loss: 2.6171 - val_accuracy: 0.2089\n",
            "Epoch 12/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4331 - accuracy: 0.5035\n",
            "Epoch 12: val_accuracy did not improve from 0.32871\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.4327 - accuracy: 0.5050 - val_loss: 3.4039 - val_accuracy: 0.2178\n",
            "Epoch 13/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3941 - accuracy: 0.5303\n",
            "Epoch 13: val_accuracy did not improve from 0.32871\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.3955 - accuracy: 0.5297 - val_loss: 2.9002 - val_accuracy: 0.2513\n",
            "Epoch 14/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3705 - accuracy: 0.5273\n",
            "Epoch 14: val_accuracy did not improve from 0.32871\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.3716 - accuracy: 0.5272 - val_loss: 2.9724 - val_accuracy: 0.2415\n",
            "Epoch 15/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3315 - accuracy: 0.5561\n",
            "Epoch 15: val_accuracy did not improve from 0.32871\n",
            "41/41 [==============================] - 2s 61ms/step - loss: 1.3330 - accuracy: 0.5552 - val_loss: 2.5315 - val_accuracy: 0.2411\n",
            "Epoch 16/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.2953 - accuracy: 0.5662\n",
            "Epoch 16: val_accuracy did not improve from 0.32871\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.2942 - accuracy: 0.5675 - val_loss: 3.5669 - val_accuracy: 0.1682\n",
            "Epoch 17/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.2614 - accuracy: 0.5826\n",
            "Epoch 17: val_accuracy did not improve from 0.32871\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.2604 - accuracy: 0.5840 - val_loss: 3.0217 - val_accuracy: 0.2317\n",
            "Epoch 18/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.2227 - accuracy: 0.5957\n",
            "Epoch 18: val_accuracy did not improve from 0.32871\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.2227 - accuracy: 0.5957 - val_loss: 3.4467 - val_accuracy: 0.2844\n",
            "Epoch 19/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.1763 - accuracy: 0.6133\n",
            "Epoch 19: val_accuracy improved from 0.32871 to 0.33542, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 66ms/step - loss: 1.1756 - accuracy: 0.6135 - val_loss: 3.5623 - val_accuracy: 0.3354\n",
            "Epoch 20/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.1356 - accuracy: 0.6367\n",
            "Epoch 20: val_accuracy improved from 0.33542 to 0.35689, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 70ms/step - loss: 1.1357 - accuracy: 0.6365 - val_loss: 2.2850 - val_accuracy: 0.3569\n",
            "Epoch 21/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.0722 - accuracy: 0.6709\n",
            "Epoch 21: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.0711 - accuracy: 0.6718 - val_loss: 3.9584 - val_accuracy: 0.1708\n",
            "Epoch 22/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.0166 - accuracy: 0.6924\n",
            "Epoch 22: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.0159 - accuracy: 0.6919 - val_loss: 2.6745 - val_accuracy: 0.2402\n",
            "Epoch 23/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.9676 - accuracy: 0.7147\n",
            "Epoch 23: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 0.9676 - accuracy: 0.7147 - val_loss: 3.4928 - val_accuracy: 0.2491\n",
            "Epoch 24/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.9060 - accuracy: 0.7436\n",
            "Epoch 24: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 55ms/step - loss: 0.9044 - accuracy: 0.7444 - val_loss: 3.9883 - val_accuracy: 0.2576\n",
            "Epoch 25/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.8416 - accuracy: 0.7711\n",
            "Epoch 25: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 0.8416 - accuracy: 0.7711 - val_loss: 2.0222 - val_accuracy: 0.2764\n",
            "Epoch 26/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.7769 - accuracy: 0.7965\n",
            "Epoch 26: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 0.7792 - accuracy: 0.7956 - val_loss: 2.3287 - val_accuracy: 0.2339\n",
            "Epoch 27/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.7472 - accuracy: 0.8113\n",
            "Epoch 27: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 0.7477 - accuracy: 0.8108 - val_loss: 3.1124 - val_accuracy: 0.2411\n",
            "Epoch 28/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.6979 - accuracy: 0.8347\n",
            "Epoch 28: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 0.6979 - accuracy: 0.8347 - val_loss: 2.1715 - val_accuracy: 0.3256\n",
            "Epoch 29/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.6452 - accuracy: 0.8562\n",
            "Epoch 29: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 0.6468 - accuracy: 0.8556 - val_loss: 3.5991 - val_accuracy: 0.2639\n",
            "Epoch 30/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.5801 - accuracy: 0.8861\n",
            "Epoch 30: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 0.5825 - accuracy: 0.8852 - val_loss: 2.8627 - val_accuracy: 0.2719\n",
            "Epoch 31/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.5637 - accuracy: 0.8854\n",
            "Epoch 31: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 55ms/step - loss: 0.5634 - accuracy: 0.8859 - val_loss: 3.5526 - val_accuracy: 0.3023\n",
            "Epoch 32/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.5472 - accuracy: 0.8912\n",
            "Epoch 32: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 0.5472 - accuracy: 0.8911 - val_loss: 3.0950 - val_accuracy: 0.2209\n",
            "Epoch 33/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.4802 - accuracy: 0.9195\n",
            "Epoch 33: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 0.4826 - accuracy: 0.9187 - val_loss: 3.0411 - val_accuracy: 0.2826\n",
            "Epoch 34/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.4694 - accuracy: 0.9232\n",
            "Epoch 34: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 0.4692 - accuracy: 0.9233 - val_loss: 6.6496 - val_accuracy: 0.1878\n",
            "Epoch 35/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.4655 - accuracy: 0.9256\n",
            "Epoch 35: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 0.4655 - accuracy: 0.9256 - val_loss: 3.4317 - val_accuracy: 0.2549\n",
            "Epoch 36/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.4392 - accuracy: 0.9367\n",
            "Epoch 36: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 0.4399 - accuracy: 0.9363 - val_loss: 3.2977 - val_accuracy: 0.2849\n",
            "Epoch 37/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.4289 - accuracy: 0.9381\n",
            "Epoch 37: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 0.4288 - accuracy: 0.9379 - val_loss: 4.2128 - val_accuracy: 0.2764\n",
            "Epoch 38/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.4067 - accuracy: 0.9469\n",
            "Epoch 38: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 0.4069 - accuracy: 0.9465 - val_loss: 4.4089 - val_accuracy: 0.2996\n",
            "Epoch 39/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.3834 - accuracy: 0.9603\n",
            "Epoch 39: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 0.3834 - accuracy: 0.9603 - val_loss: 2.7428 - val_accuracy: 0.2858\n",
            "Epoch 40/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.3693 - accuracy: 0.9590\n",
            "Epoch 40: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 0.3688 - accuracy: 0.9592 - val_loss: 3.6716 - val_accuracy: 0.2612\n",
            "Epoch 41/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.3497 - accuracy: 0.9701\n",
            "Epoch 41: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 0.3500 - accuracy: 0.9701 - val_loss: 5.2688 - val_accuracy: 0.2750\n",
            "Epoch 42/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.3460 - accuracy: 0.9688\n",
            "Epoch 42: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 0.3460 - accuracy: 0.9688 - val_loss: 2.9844 - val_accuracy: 0.2920\n",
            "Epoch 43/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.3393 - accuracy: 0.9711\n",
            "Epoch 43: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 0.3419 - accuracy: 0.9697 - val_loss: 3.9047 - val_accuracy: 0.3064\n",
            "Epoch 44/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.3431 - accuracy: 0.9659\n",
            "Epoch 44: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 0.3431 - accuracy: 0.9659 - val_loss: 3.1764 - val_accuracy: 0.3497\n",
            "Epoch 45/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.3496 - accuracy: 0.9623\n",
            "Epoch 45: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 0.3498 - accuracy: 0.9618 - val_loss: 3.4838 - val_accuracy: 0.3260\n",
            "Epoch 46/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.3462 - accuracy: 0.9645\n",
            "Epoch 46: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 0.3460 - accuracy: 0.9647 - val_loss: 3.2581 - val_accuracy: 0.2692\n",
            "Epoch 47/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.3444 - accuracy: 0.9678\n",
            "Epoch 47: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 0.3444 - accuracy: 0.9678 - val_loss: 5.6425 - val_accuracy: 0.1972\n",
            "Epoch 48/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.3377 - accuracy: 0.9664\n",
            "Epoch 48: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 55ms/step - loss: 0.3373 - accuracy: 0.9668 - val_loss: 4.5892 - val_accuracy: 0.2943\n",
            "Epoch 49/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.3161 - accuracy: 0.9756\n",
            "Epoch 49: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 0.3170 - accuracy: 0.9753 - val_loss: 4.7451 - val_accuracy: 0.2903\n",
            "Epoch 50/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.3209 - accuracy: 0.9727\n",
            "Epoch 50: val_accuracy did not improve from 0.35689\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 0.3209 - accuracy: 0.9724 - val_loss: 4.5605 - val_accuracy: 0.2974\n",
            "reg_param=0.0001, dr=0.1, test_acc=0.2974\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_56 (Conv1D)          (None, 301, 128)          512       \n",
            "                                                                 \n",
            " batch_normalization_91 (Bat  (None, 301, 128)         512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_91 (Activation)  (None, 301, 128)          0         \n",
            "                                                                 \n",
            " max_pooling1d_56 (MaxPoolin  (None, 150, 128)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_57 (Conv1D)          (None, 148, 256)          98560     \n",
            "                                                                 \n",
            " batch_normalization_92 (Bat  (None, 148, 256)         1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_92 (Activation)  (None, 148, 256)          0         \n",
            "                                                                 \n",
            " max_pooling1d_57 (MaxPoolin  (None, 74, 256)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_58 (Conv1D)          (None, 72, 512)           393728    \n",
            "                                                                 \n",
            " batch_normalization_93 (Bat  (None, 72, 512)          2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_93 (Activation)  (None, 72, 512)           0         \n",
            "                                                                 \n",
            " max_pooling1d_58 (MaxPoolin  (None, 36, 512)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_59 (Conv1D)          (None, 34, 1024)          1573888   \n",
            "                                                                 \n",
            " batch_normalization_94 (Bat  (None, 34, 1024)         4096      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_94 (Activation)  (None, 34, 1024)          0         \n",
            "                                                                 \n",
            " max_pooling1d_59 (MaxPoolin  (None, 17, 1024)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_14 (Flatten)        (None, 17408)             0         \n",
            "                                                                 \n",
            " dense_56 (Dense)            (None, 512)               8913408   \n",
            "                                                                 \n",
            " batch_normalization_95 (Bat  (None, 512)              2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_95 (Activation)  (None, 512)               0         \n",
            "                                                                 \n",
            " dropout_42 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " batch_normalization_96 (Bat  (None, 256)              1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_96 (Activation)  (None, 256)               0         \n",
            "                                                                 \n",
            " dropout_43 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " batch_normalization_97 (Bat  (None, 128)              512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_97 (Activation)  (None, 128)               0         \n",
            "                                                                 \n",
            " dropout_44 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_59 (Dense)            (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,156,358\n",
            "Trainable params: 11,150,726\n",
            "Non-trainable params: 5,632\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8987 - accuracy: 0.3404\n",
            "Epoch 1: val_accuracy improved from -inf to 0.19410, saving model to best_model.h5\n",
            "41/41 [==============================] - 5s 76ms/step - loss: 1.8976 - accuracy: 0.3416 - val_loss: 2.0761 - val_accuracy: 0.1941\n",
            "Epoch 2/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.7550 - accuracy: 0.3875\n",
            "Epoch 2: val_accuracy did not improve from 0.19410\n",
            "41/41 [==============================] - 2s 55ms/step - loss: 1.7550 - accuracy: 0.3875 - val_loss: 3.6072 - val_accuracy: 0.1722\n",
            "Epoch 3/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7151 - accuracy: 0.3941\n",
            "Epoch 3: val_accuracy did not improve from 0.19410\n",
            "41/41 [==============================] - 2s 55ms/step - loss: 1.7137 - accuracy: 0.3955 - val_loss: 5.4406 - val_accuracy: 0.1704\n",
            "Epoch 4/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6830 - accuracy: 0.4115\n",
            "Epoch 4: val_accuracy improved from 0.19410 to 0.20483, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 71ms/step - loss: 1.6829 - accuracy: 0.4116 - val_loss: 5.5158 - val_accuracy: 0.2048\n",
            "Epoch 5/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6520 - accuracy: 0.4207\n",
            "Epoch 5: val_accuracy improved from 0.20483 to 0.23435, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 65ms/step - loss: 1.6521 - accuracy: 0.4199 - val_loss: 3.9213 - val_accuracy: 0.2343\n",
            "Epoch 6/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6346 - accuracy: 0.4250\n",
            "Epoch 6: val_accuracy improved from 0.23435 to 0.26342, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 65ms/step - loss: 1.6327 - accuracy: 0.4262 - val_loss: 3.8175 - val_accuracy: 0.2634\n",
            "Epoch 7/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6102 - accuracy: 0.4264\n",
            "Epoch 7: val_accuracy did not improve from 0.26342\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.6108 - accuracy: 0.4256 - val_loss: 3.3943 - val_accuracy: 0.2491\n",
            "Epoch 8/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5828 - accuracy: 0.4424\n",
            "Epoch 8: val_accuracy improved from 0.26342 to 0.27639, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 65ms/step - loss: 1.5838 - accuracy: 0.4421 - val_loss: 2.5746 - val_accuracy: 0.2764\n",
            "Epoch 9/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5719 - accuracy: 0.4500\n",
            "Epoch 9: val_accuracy improved from 0.27639 to 0.31351, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 71ms/step - loss: 1.5723 - accuracy: 0.4492 - val_loss: 2.1914 - val_accuracy: 0.3135\n",
            "Epoch 10/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.5537 - accuracy: 0.4549\n",
            "Epoch 10: val_accuracy did not improve from 0.31351\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.5537 - accuracy: 0.4549 - val_loss: 2.4271 - val_accuracy: 0.2871\n",
            "Epoch 11/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.5317 - accuracy: 0.4618\n",
            "Epoch 11: val_accuracy did not improve from 0.31351\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5317 - accuracy: 0.4618 - val_loss: 2.0038 - val_accuracy: 0.3099\n",
            "Epoch 12/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5151 - accuracy: 0.4654\n",
            "Epoch 12: val_accuracy did not improve from 0.31351\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.5161 - accuracy: 0.4640 - val_loss: 2.0345 - val_accuracy: 0.3104\n",
            "Epoch 13/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5165 - accuracy: 0.4613\n",
            "Epoch 13: val_accuracy did not improve from 0.31351\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.5167 - accuracy: 0.4618 - val_loss: 3.2864 - val_accuracy: 0.2178\n",
            "Epoch 14/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5064 - accuracy: 0.4715\n",
            "Epoch 14: val_accuracy improved from 0.31351 to 0.33274, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 70ms/step - loss: 1.5049 - accuracy: 0.4714 - val_loss: 2.3218 - val_accuracy: 0.3327\n",
            "Epoch 15/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4920 - accuracy: 0.4855\n",
            "Epoch 15: val_accuracy did not improve from 0.33274\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.4932 - accuracy: 0.4862 - val_loss: 2.1858 - val_accuracy: 0.3086\n",
            "Epoch 16/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.4690 - accuracy: 0.4908\n",
            "Epoch 16: val_accuracy improved from 0.33274 to 0.34526, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 69ms/step - loss: 1.4690 - accuracy: 0.4908 - val_loss: 2.1846 - val_accuracy: 0.3453\n",
            "Epoch 17/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4582 - accuracy: 0.4885\n",
            "Epoch 17: val_accuracy improved from 0.34526 to 0.35733, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 68ms/step - loss: 1.4594 - accuracy: 0.4873 - val_loss: 2.4176 - val_accuracy: 0.3573\n",
            "Epoch 18/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4275 - accuracy: 0.5107\n",
            "Epoch 18: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.4289 - accuracy: 0.5104 - val_loss: 2.3522 - val_accuracy: 0.2496\n",
            "Epoch 19/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4174 - accuracy: 0.5092\n",
            "Epoch 19: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.4170 - accuracy: 0.5098 - val_loss: 2.0654 - val_accuracy: 0.3318\n",
            "Epoch 20/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3915 - accuracy: 0.5205\n",
            "Epoch 20: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.3937 - accuracy: 0.5196 - val_loss: 2.1868 - val_accuracy: 0.3479\n",
            "Epoch 21/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3646 - accuracy: 0.5385\n",
            "Epoch 21: val_accuracy improved from 0.35733 to 0.38327, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 70ms/step - loss: 1.3667 - accuracy: 0.5368 - val_loss: 2.1417 - val_accuracy: 0.3833\n",
            "Epoch 22/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3398 - accuracy: 0.5412\n",
            "Epoch 22: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.3399 - accuracy: 0.5410 - val_loss: 2.1454 - val_accuracy: 0.2437\n",
            "Epoch 23/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3179 - accuracy: 0.5549\n",
            "Epoch 23: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.3194 - accuracy: 0.5543 - val_loss: 3.1941 - val_accuracy: 0.1887\n",
            "Epoch 24/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.2869 - accuracy: 0.5757\n",
            "Epoch 24: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.2869 - accuracy: 0.5757 - val_loss: 2.8582 - val_accuracy: 0.2818\n",
            "Epoch 25/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.2572 - accuracy: 0.5783\n",
            "Epoch 25: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.2589 - accuracy: 0.5773 - val_loss: 3.4962 - val_accuracy: 0.2661\n",
            "Epoch 26/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.2243 - accuracy: 0.6049\n",
            "Epoch 26: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.2273 - accuracy: 0.6030 - val_loss: 2.2632 - val_accuracy: 0.3725\n",
            "Epoch 27/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.1979 - accuracy: 0.6064\n",
            "Epoch 27: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.1944 - accuracy: 0.6085 - val_loss: 3.4101 - val_accuracy: 0.2375\n",
            "Epoch 28/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.1549 - accuracy: 0.6250\n",
            "Epoch 28: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.1529 - accuracy: 0.6260 - val_loss: 3.3859 - val_accuracy: 0.2589\n",
            "Epoch 29/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.1084 - accuracy: 0.6500\n",
            "Epoch 29: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.1113 - accuracy: 0.6484 - val_loss: 4.1643 - val_accuracy: 0.2187\n",
            "Epoch 30/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.0786 - accuracy: 0.6654\n",
            "Epoch 30: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.0809 - accuracy: 0.6645 - val_loss: 3.6219 - val_accuracy: 0.1914\n",
            "Epoch 31/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.0252 - accuracy: 0.6922\n",
            "Epoch 31: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.0239 - accuracy: 0.6923 - val_loss: 5.0317 - val_accuracy: 0.2335\n",
            "Epoch 32/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.9784 - accuracy: 0.7105\n",
            "Epoch 32: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 61ms/step - loss: 0.9788 - accuracy: 0.7101 - val_loss: 3.5133 - val_accuracy: 0.2428\n",
            "Epoch 33/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.9410 - accuracy: 0.7314\n",
            "Epoch 33: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 0.9410 - accuracy: 0.7314 - val_loss: 4.0379 - val_accuracy: 0.2911\n",
            "Epoch 34/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.8839 - accuracy: 0.7471\n",
            "Epoch 34: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 0.8863 - accuracy: 0.7458 - val_loss: 4.5773 - val_accuracy: 0.1843\n",
            "Epoch 35/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.8635 - accuracy: 0.7627\n",
            "Epoch 35: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 0.8635 - accuracy: 0.7632 - val_loss: 3.7970 - val_accuracy: 0.2348\n",
            "Epoch 36/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.8291 - accuracy: 0.7748\n",
            "Epoch 36: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 0.8305 - accuracy: 0.7743 - val_loss: 2.9652 - val_accuracy: 0.2625\n",
            "Epoch 37/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.7653 - accuracy: 0.8002\n",
            "Epoch 37: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 0.7670 - accuracy: 0.7997 - val_loss: 3.0153 - val_accuracy: 0.3010\n",
            "Epoch 38/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.7191 - accuracy: 0.8213\n",
            "Epoch 38: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 0.7209 - accuracy: 0.8206 - val_loss: 2.2069 - val_accuracy: 0.3721\n",
            "Epoch 39/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.6954 - accuracy: 0.8354\n",
            "Epoch 39: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 0.6972 - accuracy: 0.8336 - val_loss: 3.6102 - val_accuracy: 0.2035\n",
            "Epoch 40/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.6527 - accuracy: 0.8504\n",
            "Epoch 40: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 0.6536 - accuracy: 0.8497 - val_loss: 4.6061 - val_accuracy: 0.2236\n",
            "Epoch 41/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.6257 - accuracy: 0.8639\n",
            "Epoch 41: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 0.6260 - accuracy: 0.8633 - val_loss: 3.8697 - val_accuracy: 0.3229\n",
            "Epoch 42/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.5971 - accuracy: 0.8773\n",
            "Epoch 42: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 0.5991 - accuracy: 0.8767 - val_loss: 6.8816 - val_accuracy: 0.2589\n",
            "Epoch 43/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.5941 - accuracy: 0.8766\n",
            "Epoch 43: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 0.5968 - accuracy: 0.8750 - val_loss: 4.0903 - val_accuracy: 0.2187\n",
            "Epoch 44/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.5500 - accuracy: 0.8995\n",
            "Epoch 44: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 0.5500 - accuracy: 0.8995 - val_loss: 3.2036 - val_accuracy: 0.2737\n",
            "Epoch 45/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.5237 - accuracy: 0.9059\n",
            "Epoch 45: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 0.5247 - accuracy: 0.9053 - val_loss: 5.4835 - val_accuracy: 0.3055\n",
            "Epoch 46/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.5070 - accuracy: 0.9121\n",
            "Epoch 46: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 0.5087 - accuracy: 0.9112 - val_loss: 4.3883 - val_accuracy: 0.2996\n",
            "Epoch 47/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.4911 - accuracy: 0.9166\n",
            "Epoch 47: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 0.4897 - accuracy: 0.9174 - val_loss: 3.5557 - val_accuracy: 0.2733\n",
            "Epoch 48/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.4901 - accuracy: 0.9151\n",
            "Epoch 48: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 0.4901 - accuracy: 0.9151 - val_loss: 4.8955 - val_accuracy: 0.2227\n",
            "Epoch 49/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.4804 - accuracy: 0.9213\n",
            "Epoch 49: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 0.4796 - accuracy: 0.9218 - val_loss: 3.1063 - val_accuracy: 0.3301\n",
            "Epoch 50/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.4664 - accuracy: 0.9287\n",
            "Epoch 50: val_accuracy did not improve from 0.38327\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 0.4664 - accuracy: 0.9287 - val_loss: 4.5196 - val_accuracy: 0.2428\n",
            "reg_param=0.0001, dr=0.2, test_acc=0.2428\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_60 (Conv1D)          (None, 301, 128)          512       \n",
            "                                                                 \n",
            " batch_normalization_98 (Bat  (None, 301, 128)         512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_98 (Activation)  (None, 301, 128)          0         \n",
            "                                                                 \n",
            " max_pooling1d_60 (MaxPoolin  (None, 150, 128)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_61 (Conv1D)          (None, 148, 256)          98560     \n",
            "                                                                 \n",
            " batch_normalization_99 (Bat  (None, 148, 256)         1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_99 (Activation)  (None, 148, 256)          0         \n",
            "                                                                 \n",
            " max_pooling1d_61 (MaxPoolin  (None, 74, 256)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_62 (Conv1D)          (None, 72, 512)           393728    \n",
            "                                                                 \n",
            " batch_normalization_100 (Ba  (None, 72, 512)          2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_100 (Activation)  (None, 72, 512)          0         \n",
            "                                                                 \n",
            " max_pooling1d_62 (MaxPoolin  (None, 36, 512)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_63 (Conv1D)          (None, 34, 1024)          1573888   \n",
            "                                                                 \n",
            " batch_normalization_101 (Ba  (None, 34, 1024)         4096      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_101 (Activation)  (None, 34, 1024)         0         \n",
            "                                                                 \n",
            " max_pooling1d_63 (MaxPoolin  (None, 17, 1024)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_15 (Flatten)        (None, 17408)             0         \n",
            "                                                                 \n",
            " dense_60 (Dense)            (None, 512)               8913408   \n",
            "                                                                 \n",
            " batch_normalization_102 (Ba  (None, 512)              2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_102 (Activation)  (None, 512)              0         \n",
            "                                                                 \n",
            " dropout_45 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_61 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " batch_normalization_103 (Ba  (None, 256)              1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_103 (Activation)  (None, 256)              0         \n",
            "                                                                 \n",
            " dropout_46 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_62 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " batch_normalization_104 (Ba  (None, 128)              512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_104 (Activation)  (None, 128)              0         \n",
            "                                                                 \n",
            " dropout_47 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_63 (Dense)            (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,156,358\n",
            "Trainable params: 11,150,726\n",
            "Non-trainable params: 5,632\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.9336 - accuracy: 0.3342\n",
            "Epoch 1: val_accuracy improved from -inf to 0.20930, saving model to best_model.h5\n",
            "41/41 [==============================] - 6s 79ms/step - loss: 1.9336 - accuracy: 0.3342 - val_loss: 1.9562 - val_accuracy: 0.2093\n",
            "Epoch 2/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7800 - accuracy: 0.3744\n",
            "Epoch 2: val_accuracy did not improve from 0.20930\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.7828 - accuracy: 0.3735 - val_loss: 4.6831 - val_accuracy: 0.1704\n",
            "Epoch 3/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7418 - accuracy: 0.3875\n",
            "Epoch 3: val_accuracy did not improve from 0.20930\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.7420 - accuracy: 0.3867 - val_loss: 5.7316 - val_accuracy: 0.1704\n",
            "Epoch 4/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6995 - accuracy: 0.3975\n",
            "Epoch 4: val_accuracy did not improve from 0.20930\n",
            "41/41 [==============================] - 2s 55ms/step - loss: 1.7000 - accuracy: 0.3980 - val_loss: 5.9703 - val_accuracy: 0.1704\n",
            "Epoch 5/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6722 - accuracy: 0.4117\n",
            "Epoch 5: val_accuracy improved from 0.20930 to 0.22674, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 71ms/step - loss: 1.6698 - accuracy: 0.4139 - val_loss: 5.0120 - val_accuracy: 0.2267\n",
            "Epoch 6/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6397 - accuracy: 0.4240\n",
            "Epoch 6: val_accuracy did not improve from 0.22674\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.6415 - accuracy: 0.4233 - val_loss: 3.9043 - val_accuracy: 0.1887\n",
            "Epoch 7/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6315 - accuracy: 0.4232\n",
            "Epoch 7: val_accuracy improved from 0.22674 to 0.23748, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 69ms/step - loss: 1.6321 - accuracy: 0.4227 - val_loss: 3.4854 - val_accuracy: 0.2375\n",
            "Epoch 8/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.6133 - accuracy: 0.4302\n",
            "Epoch 8: val_accuracy improved from 0.23748 to 0.24776, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 69ms/step - loss: 1.6133 - accuracy: 0.4302 - val_loss: 2.6967 - val_accuracy: 0.2478\n",
            "Epoch 9/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5985 - accuracy: 0.4352\n",
            "Epoch 9: val_accuracy improved from 0.24776 to 0.31887, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 69ms/step - loss: 1.6000 - accuracy: 0.4340 - val_loss: 2.3381 - val_accuracy: 0.3189\n",
            "Epoch 10/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5907 - accuracy: 0.4283\n",
            "Epoch 10: val_accuracy improved from 0.31887 to 0.32066, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 73ms/step - loss: 1.5920 - accuracy: 0.4268 - val_loss: 2.1313 - val_accuracy: 0.3207\n",
            "Epoch 11/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5900 - accuracy: 0.4303\n",
            "Epoch 11: val_accuracy improved from 0.32066 to 0.32961, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 71ms/step - loss: 1.5869 - accuracy: 0.4314 - val_loss: 2.3495 - val_accuracy: 0.3296\n",
            "Epoch 12/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5470 - accuracy: 0.4580\n",
            "Epoch 12: val_accuracy did not improve from 0.32961\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.5496 - accuracy: 0.4567 - val_loss: 2.4130 - val_accuracy: 0.2777\n",
            "Epoch 13/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5495 - accuracy: 0.4523\n",
            "Epoch 13: val_accuracy did not improve from 0.32961\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.5524 - accuracy: 0.4517 - val_loss: 2.6338 - val_accuracy: 0.3220\n",
            "Epoch 14/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5343 - accuracy: 0.4578\n",
            "Epoch 14: val_accuracy did not improve from 0.32961\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5348 - accuracy: 0.4582 - val_loss: 2.6937 - val_accuracy: 0.3108\n",
            "Epoch 15/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5230 - accuracy: 0.4611\n",
            "Epoch 15: val_accuracy did not improve from 0.32961\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5246 - accuracy: 0.4620 - val_loss: 2.1117 - val_accuracy: 0.3242\n",
            "Epoch 16/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5039 - accuracy: 0.4693\n",
            "Epoch 16: val_accuracy improved from 0.32961 to 0.33766, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 74ms/step - loss: 1.5026 - accuracy: 0.4707 - val_loss: 2.3206 - val_accuracy: 0.3377\n",
            "Epoch 17/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4773 - accuracy: 0.4826\n",
            "Epoch 17: val_accuracy did not improve from 0.33766\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.4782 - accuracy: 0.4826 - val_loss: 2.0459 - val_accuracy: 0.3350\n",
            "Epoch 18/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4670 - accuracy: 0.4816\n",
            "Epoch 18: val_accuracy did not improve from 0.33766\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.4660 - accuracy: 0.4816 - val_loss: 2.5074 - val_accuracy: 0.3135\n",
            "Epoch 19/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4552 - accuracy: 0.4916\n",
            "Epoch 19: val_accuracy did not improve from 0.33766\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.4574 - accuracy: 0.4908 - val_loss: 2.4157 - val_accuracy: 0.2916\n",
            "Epoch 20/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4324 - accuracy: 0.5018\n",
            "Epoch 20: val_accuracy did not improve from 0.33766\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.4331 - accuracy: 0.5021 - val_loss: 2.3191 - val_accuracy: 0.3296\n",
            "Epoch 21/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4130 - accuracy: 0.5111\n",
            "Epoch 21: val_accuracy did not improve from 0.33766\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.4135 - accuracy: 0.5105 - val_loss: 2.6766 - val_accuracy: 0.3202\n",
            "Epoch 22/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3810 - accuracy: 0.5277\n",
            "Epoch 22: val_accuracy did not improve from 0.33766\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.3835 - accuracy: 0.5263 - val_loss: 2.5670 - val_accuracy: 0.2518\n",
            "Epoch 23/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.3731 - accuracy: 0.5289\n",
            "Epoch 23: val_accuracy improved from 0.33766 to 0.34258, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 68ms/step - loss: 1.3731 - accuracy: 0.5289 - val_loss: 2.7429 - val_accuracy: 0.3426\n",
            "Epoch 24/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.3466 - accuracy: 0.5412\n",
            "Epoch 24: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.3466 - accuracy: 0.5412 - val_loss: 3.3200 - val_accuracy: 0.2737\n",
            "Epoch 25/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3309 - accuracy: 0.5500\n",
            "Epoch 25: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.3308 - accuracy: 0.5497 - val_loss: 3.8359 - val_accuracy: 0.2227\n",
            "Epoch 26/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.3062 - accuracy: 0.5594\n",
            "Epoch 26: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.3062 - accuracy: 0.5594 - val_loss: 3.5086 - val_accuracy: 0.2433\n",
            "Epoch 27/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.2567 - accuracy: 0.5887\n",
            "Epoch 27: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.2574 - accuracy: 0.5882 - val_loss: 2.8580 - val_accuracy: 0.2643\n",
            "Epoch 28/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.2376 - accuracy: 0.5934\n",
            "Epoch 28: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.2367 - accuracy: 0.5941 - val_loss: 3.7251 - val_accuracy: 0.2527\n",
            "Epoch 29/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.2199 - accuracy: 0.5875\n",
            "Epoch 29: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.2192 - accuracy: 0.5884 - val_loss: 2.6119 - val_accuracy: 0.2996\n",
            "Epoch 30/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.1596 - accuracy: 0.6334\n",
            "Epoch 30: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.1594 - accuracy: 0.6336 - val_loss: 2.5345 - val_accuracy: 0.3001\n",
            "Epoch 31/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.1386 - accuracy: 0.6354\n",
            "Epoch 31: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.1383 - accuracy: 0.6359 - val_loss: 3.4732 - val_accuracy: 0.2527\n",
            "Epoch 32/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.0842 - accuracy: 0.6691\n",
            "Epoch 32: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.0845 - accuracy: 0.6701 - val_loss: 2.1626 - val_accuracy: 0.2987\n",
            "Epoch 33/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.0625 - accuracy: 0.6738\n",
            "Epoch 33: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.0638 - accuracy: 0.6733 - val_loss: 2.8579 - val_accuracy: 0.2987\n",
            "Epoch 34/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.0261 - accuracy: 0.6900\n",
            "Epoch 34: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.0255 - accuracy: 0.6902 - val_loss: 2.7819 - val_accuracy: 0.3336\n",
            "Epoch 35/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.9773 - accuracy: 0.7136\n",
            "Epoch 35: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 0.9773 - accuracy: 0.7136 - val_loss: 2.2717 - val_accuracy: 0.3314\n",
            "Epoch 36/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.9276 - accuracy: 0.7287\n",
            "Epoch 36: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 0.9276 - accuracy: 0.7287 - val_loss: 3.6839 - val_accuracy: 0.3175\n",
            "Epoch 37/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.9138 - accuracy: 0.7393\n",
            "Epoch 37: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 0.9138 - accuracy: 0.7402 - val_loss: 3.2686 - val_accuracy: 0.2576\n",
            "Epoch 38/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.8605 - accuracy: 0.7629\n",
            "Epoch 38: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 0.8599 - accuracy: 0.7632 - val_loss: 2.3793 - val_accuracy: 0.2755\n",
            "Epoch 39/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.8183 - accuracy: 0.7779\n",
            "Epoch 39: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 0.8215 - accuracy: 0.7759 - val_loss: 2.9015 - val_accuracy: 0.3207\n",
            "Epoch 40/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.7924 - accuracy: 0.7951\n",
            "Epoch 40: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 0.7961 - accuracy: 0.7939 - val_loss: 2.3390 - val_accuracy: 0.3292\n",
            "Epoch 41/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.7685 - accuracy: 0.8062\n",
            "Epoch 41: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 0.7690 - accuracy: 0.8064 - val_loss: 2.7167 - val_accuracy: 0.3278\n",
            "Epoch 42/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.7066 - accuracy: 0.8363\n",
            "Epoch 42: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 0.7075 - accuracy: 0.8361 - val_loss: 2.2897 - val_accuracy: 0.3287\n",
            "Epoch 43/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.6673 - accuracy: 0.8471\n",
            "Epoch 43: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 0.6680 - accuracy: 0.8466 - val_loss: 2.4702 - val_accuracy: 0.3287\n",
            "Epoch 44/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.6448 - accuracy: 0.8588\n",
            "Epoch 44: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 0.6459 - accuracy: 0.8587 - val_loss: 3.5942 - val_accuracy: 0.3175\n",
            "Epoch 45/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.6354 - accuracy: 0.8586\n",
            "Epoch 45: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 0.6377 - accuracy: 0.8583 - val_loss: 3.4751 - val_accuracy: 0.3381\n",
            "Epoch 46/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.6141 - accuracy: 0.8723\n",
            "Epoch 46: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 0.6152 - accuracy: 0.8714 - val_loss: 3.0185 - val_accuracy: 0.3363\n",
            "Epoch 47/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.5925 - accuracy: 0.8773\n",
            "Epoch 47: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 0.5925 - accuracy: 0.8773 - val_loss: 3.2267 - val_accuracy: 0.3184\n",
            "Epoch 48/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.5608 - accuracy: 0.8913\n",
            "Epoch 48: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 0.5608 - accuracy: 0.8913 - val_loss: 3.1516 - val_accuracy: 0.3287\n",
            "Epoch 49/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.5294 - accuracy: 0.9062\n",
            "Epoch 49: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 0.5309 - accuracy: 0.9051 - val_loss: 2.8880 - val_accuracy: 0.3305\n",
            "Epoch 50/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.5166 - accuracy: 0.9084\n",
            "Epoch 50: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 0.5166 - accuracy: 0.9084 - val_loss: 4.0569 - val_accuracy: 0.2648\n",
            "reg_param=0.0001, dr=0.25, test_acc=0.2648\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_64 (Conv1D)          (None, 301, 128)          512       \n",
            "                                                                 \n",
            " batch_normalization_105 (Ba  (None, 301, 128)         512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_105 (Activation)  (None, 301, 128)         0         \n",
            "                                                                 \n",
            " max_pooling1d_64 (MaxPoolin  (None, 150, 128)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_65 (Conv1D)          (None, 148, 256)          98560     \n",
            "                                                                 \n",
            " batch_normalization_106 (Ba  (None, 148, 256)         1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_106 (Activation)  (None, 148, 256)         0         \n",
            "                                                                 \n",
            " max_pooling1d_65 (MaxPoolin  (None, 74, 256)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_66 (Conv1D)          (None, 72, 512)           393728    \n",
            "                                                                 \n",
            " batch_normalization_107 (Ba  (None, 72, 512)          2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_107 (Activation)  (None, 72, 512)          0         \n",
            "                                                                 \n",
            " max_pooling1d_66 (MaxPoolin  (None, 36, 512)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_67 (Conv1D)          (None, 34, 1024)          1573888   \n",
            "                                                                 \n",
            " batch_normalization_108 (Ba  (None, 34, 1024)         4096      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_108 (Activation)  (None, 34, 1024)         0         \n",
            "                                                                 \n",
            " max_pooling1d_67 (MaxPoolin  (None, 17, 1024)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_16 (Flatten)        (None, 17408)             0         \n",
            "                                                                 \n",
            " dense_64 (Dense)            (None, 512)               8913408   \n",
            "                                                                 \n",
            " batch_normalization_109 (Ba  (None, 512)              2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_109 (Activation)  (None, 512)              0         \n",
            "                                                                 \n",
            " dropout_48 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_65 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " batch_normalization_110 (Ba  (None, 256)              1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_110 (Activation)  (None, 256)              0         \n",
            "                                                                 \n",
            " dropout_49 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_66 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " batch_normalization_111 (Ba  (None, 128)              512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_111 (Activation)  (None, 128)              0         \n",
            "                                                                 \n",
            " dropout_50 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_67 (Dense)            (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,156,358\n",
            "Trainable params: 11,150,726\n",
            "Non-trainable params: 5,632\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.9756 - accuracy: 0.3307\n",
            "Epoch 1: val_accuracy improved from -inf to 0.22630, saving model to best_model.h5\n",
            "41/41 [==============================] - 6s 79ms/step - loss: 1.9756 - accuracy: 0.3307 - val_loss: 2.2377 - val_accuracy: 0.2263\n",
            "Epoch 2/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8120 - accuracy: 0.3611\n",
            "Epoch 2: val_accuracy did not improve from 0.22630\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.8124 - accuracy: 0.3620 - val_loss: 4.6385 - val_accuracy: 0.1704\n",
            "Epoch 3/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7500 - accuracy: 0.3693\n",
            "Epoch 3: val_accuracy did not improve from 0.22630\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.7476 - accuracy: 0.3696 - val_loss: 7.2611 - val_accuracy: 0.1704\n",
            "Epoch 4/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.7216 - accuracy: 0.3898\n",
            "Epoch 4: val_accuracy improved from 0.22630 to 0.26118, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 73ms/step - loss: 1.7216 - accuracy: 0.3898 - val_loss: 7.1151 - val_accuracy: 0.2612\n",
            "Epoch 5/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6860 - accuracy: 0.4000\n",
            "Epoch 5: val_accuracy did not improve from 0.26118\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.6862 - accuracy: 0.3997 - val_loss: 5.8961 - val_accuracy: 0.2522\n",
            "Epoch 6/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6566 - accuracy: 0.4078\n",
            "Epoch 6: val_accuracy did not improve from 0.26118\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.6561 - accuracy: 0.4072 - val_loss: 4.9763 - val_accuracy: 0.1789\n",
            "Epoch 7/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6436 - accuracy: 0.4062\n",
            "Epoch 7: val_accuracy did not improve from 0.26118\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.6418 - accuracy: 0.4076 - val_loss: 4.1350 - val_accuracy: 0.2598\n",
            "Epoch 8/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6207 - accuracy: 0.4217\n",
            "Epoch 8: val_accuracy improved from 0.26118 to 0.26789, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 68ms/step - loss: 1.6218 - accuracy: 0.4216 - val_loss: 3.0544 - val_accuracy: 0.2679\n",
            "Epoch 9/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6104 - accuracy: 0.4291\n",
            "Epoch 9: val_accuracy improved from 0.26789 to 0.27594, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 73ms/step - loss: 1.6128 - accuracy: 0.4277 - val_loss: 3.2916 - val_accuracy: 0.2759\n",
            "Epoch 10/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5990 - accuracy: 0.4248\n",
            "Epoch 10: val_accuracy did not improve from 0.27594\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.6013 - accuracy: 0.4241 - val_loss: 2.7131 - val_accuracy: 0.2500\n",
            "Epoch 11/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5805 - accuracy: 0.4367\n",
            "Epoch 11: val_accuracy improved from 0.27594 to 0.29830, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 69ms/step - loss: 1.5798 - accuracy: 0.4373 - val_loss: 2.0263 - val_accuracy: 0.2983\n",
            "Epoch 12/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5753 - accuracy: 0.4336\n",
            "Epoch 12: val_accuracy improved from 0.29830 to 0.32826, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 68ms/step - loss: 1.5724 - accuracy: 0.4350 - val_loss: 2.4623 - val_accuracy: 0.3283\n",
            "Epoch 13/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5638 - accuracy: 0.4465\n",
            "Epoch 13: val_accuracy did not improve from 0.32826\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5634 - accuracy: 0.4452 - val_loss: 2.4532 - val_accuracy: 0.3153\n",
            "Epoch 14/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5445 - accuracy: 0.4566\n",
            "Epoch 14: val_accuracy did not improve from 0.32826\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.5486 - accuracy: 0.4551 - val_loss: 2.9552 - val_accuracy: 0.2585\n",
            "Epoch 15/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5470 - accuracy: 0.4545\n",
            "Epoch 15: val_accuracy improved from 0.32826 to 0.35733, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 72ms/step - loss: 1.5466 - accuracy: 0.4551 - val_loss: 2.3356 - val_accuracy: 0.3573\n",
            "Epoch 16/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.5236 - accuracy: 0.4599\n",
            "Epoch 16: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5236 - accuracy: 0.4599 - val_loss: 2.6053 - val_accuracy: 0.2987\n",
            "Epoch 17/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.5212 - accuracy: 0.4576\n",
            "Epoch 17: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.5212 - accuracy: 0.4576 - val_loss: 2.4093 - val_accuracy: 0.2616\n",
            "Epoch 18/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5062 - accuracy: 0.4670\n",
            "Epoch 18: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.5054 - accuracy: 0.4680 - val_loss: 2.3968 - val_accuracy: 0.2227\n",
            "Epoch 19/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.4964 - accuracy: 0.4709\n",
            "Epoch 19: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.4964 - accuracy: 0.4709 - val_loss: 2.2830 - val_accuracy: 0.2885\n",
            "Epoch 20/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4668 - accuracy: 0.4797\n",
            "Epoch 20: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.4713 - accuracy: 0.4781 - val_loss: 3.4885 - val_accuracy: 0.1758\n",
            "Epoch 21/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4680 - accuracy: 0.4854\n",
            "Epoch 21: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.4672 - accuracy: 0.4856 - val_loss: 3.9274 - val_accuracy: 0.1722\n",
            "Epoch 22/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.4453 - accuracy: 0.4908\n",
            "Epoch 22: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.4453 - accuracy: 0.4908 - val_loss: 2.8750 - val_accuracy: 0.3171\n",
            "Epoch 23/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4381 - accuracy: 0.4980\n",
            "Epoch 23: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.4399 - accuracy: 0.4971 - val_loss: 2.7342 - val_accuracy: 0.3345\n",
            "Epoch 24/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.4236 - accuracy: 0.5075\n",
            "Epoch 24: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.4236 - accuracy: 0.5075 - val_loss: 2.5126 - val_accuracy: 0.2925\n",
            "Epoch 25/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.4156 - accuracy: 0.5059\n",
            "Epoch 25: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.4156 - accuracy: 0.5059 - val_loss: 2.4009 - val_accuracy: 0.3193\n",
            "Epoch 26/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3978 - accuracy: 0.5152\n",
            "Epoch 26: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.3978 - accuracy: 0.5144 - val_loss: 2.4978 - val_accuracy: 0.3005\n",
            "Epoch 27/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3756 - accuracy: 0.5285\n",
            "Epoch 27: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.3744 - accuracy: 0.5295 - val_loss: 3.3626 - val_accuracy: 0.2822\n",
            "Epoch 28/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.3585 - accuracy: 0.5328\n",
            "Epoch 28: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.3585 - accuracy: 0.5328 - val_loss: 3.5823 - val_accuracy: 0.2688\n",
            "Epoch 29/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3387 - accuracy: 0.5422\n",
            "Epoch 29: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.3395 - accuracy: 0.5422 - val_loss: 2.9183 - val_accuracy: 0.2768\n",
            "Epoch 30/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.2960 - accuracy: 0.5650\n",
            "Epoch 30: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.3006 - accuracy: 0.5635 - val_loss: 2.6665 - val_accuracy: 0.2840\n",
            "Epoch 31/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.2956 - accuracy: 0.5596\n",
            "Epoch 31: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.2963 - accuracy: 0.5590 - val_loss: 3.9162 - val_accuracy: 0.2312\n",
            "Epoch 32/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.2525 - accuracy: 0.5861\n",
            "Epoch 32: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.2560 - accuracy: 0.5840 - val_loss: 2.9149 - val_accuracy: 0.2688\n",
            "Epoch 33/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.2352 - accuracy: 0.5941\n",
            "Epoch 33: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.2356 - accuracy: 0.5939 - val_loss: 4.3473 - val_accuracy: 0.2352\n",
            "Epoch 34/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.1863 - accuracy: 0.6133\n",
            "Epoch 34: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.1892 - accuracy: 0.6122 - val_loss: 3.4019 - val_accuracy: 0.2818\n",
            "Epoch 35/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.1559 - accuracy: 0.6332\n",
            "Epoch 35: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.1586 - accuracy: 0.6321 - val_loss: 2.8247 - val_accuracy: 0.3041\n",
            "Epoch 36/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.1209 - accuracy: 0.6416\n",
            "Epoch 36: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.1233 - accuracy: 0.6405 - val_loss: 2.6722 - val_accuracy: 0.2567\n",
            "Epoch 37/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.1114 - accuracy: 0.6447\n",
            "Epoch 37: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.1128 - accuracy: 0.6438 - val_loss: 2.5825 - val_accuracy: 0.3323\n",
            "Epoch 38/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.0777 - accuracy: 0.6775\n",
            "Epoch 38: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.0789 - accuracy: 0.6762 - val_loss: 4.4271 - val_accuracy: 0.3055\n",
            "Epoch 39/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.0423 - accuracy: 0.6826\n",
            "Epoch 39: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.0420 - accuracy: 0.6837 - val_loss: 3.3748 - val_accuracy: 0.2464\n",
            "Epoch 40/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.9747 - accuracy: 0.7084\n",
            "Epoch 40: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 0.9770 - accuracy: 0.7086 - val_loss: 3.9157 - val_accuracy: 0.2737\n",
            "Epoch 41/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.9679 - accuracy: 0.7160\n",
            "Epoch 41: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 0.9688 - accuracy: 0.7159 - val_loss: 3.1204 - val_accuracy: 0.2791\n",
            "Epoch 42/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.9251 - accuracy: 0.7339\n",
            "Epoch 42: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 0.9251 - accuracy: 0.7339 - val_loss: 5.8598 - val_accuracy: 0.1722\n",
            "Epoch 43/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.9172 - accuracy: 0.7421\n",
            "Epoch 43: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 0.9172 - accuracy: 0.7421 - val_loss: 3.2668 - val_accuracy: 0.3511\n",
            "Epoch 44/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.8708 - accuracy: 0.7674\n",
            "Epoch 44: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 0.8721 - accuracy: 0.7674 - val_loss: 3.4402 - val_accuracy: 0.3278\n",
            "Epoch 45/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.8439 - accuracy: 0.7760\n",
            "Epoch 45: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 0.8463 - accuracy: 0.7759 - val_loss: 5.0427 - val_accuracy: 0.2473\n",
            "Epoch 46/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.8014 - accuracy: 0.7957\n",
            "Epoch 46: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 0.8028 - accuracy: 0.7943 - val_loss: 4.0732 - val_accuracy: 0.2996\n",
            "Epoch 47/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.7571 - accuracy: 0.8129\n",
            "Epoch 47: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 0.7595 - accuracy: 0.8115 - val_loss: 2.6564 - val_accuracy: 0.2406\n",
            "Epoch 48/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.7243 - accuracy: 0.8250\n",
            "Epoch 48: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 0.7231 - accuracy: 0.8257 - val_loss: 4.0028 - val_accuracy: 0.2612\n",
            "Epoch 49/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.7143 - accuracy: 0.8336\n",
            "Epoch 49: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 0.7143 - accuracy: 0.8336 - val_loss: 3.5726 - val_accuracy: 0.3439\n",
            "Epoch 50/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.6877 - accuracy: 0.8438\n",
            "Epoch 50: val_accuracy did not improve from 0.35733\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 0.6883 - accuracy: 0.8436 - val_loss: 3.1686 - val_accuracy: 0.3135\n",
            "reg_param=0.0001, dr=0.3, test_acc=0.3135\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_68 (Conv1D)          (None, 301, 128)          512       \n",
            "                                                                 \n",
            " batch_normalization_112 (Ba  (None, 301, 128)         512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_112 (Activation)  (None, 301, 128)         0         \n",
            "                                                                 \n",
            " max_pooling1d_68 (MaxPoolin  (None, 150, 128)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_69 (Conv1D)          (None, 148, 256)          98560     \n",
            "                                                                 \n",
            " batch_normalization_113 (Ba  (None, 148, 256)         1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_113 (Activation)  (None, 148, 256)         0         \n",
            "                                                                 \n",
            " max_pooling1d_69 (MaxPoolin  (None, 74, 256)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_70 (Conv1D)          (None, 72, 512)           393728    \n",
            "                                                                 \n",
            " batch_normalization_114 (Ba  (None, 72, 512)          2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_114 (Activation)  (None, 72, 512)          0         \n",
            "                                                                 \n",
            " max_pooling1d_70 (MaxPoolin  (None, 36, 512)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_71 (Conv1D)          (None, 34, 1024)          1573888   \n",
            "                                                                 \n",
            " batch_normalization_115 (Ba  (None, 34, 1024)         4096      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_115 (Activation)  (None, 34, 1024)         0         \n",
            "                                                                 \n",
            " max_pooling1d_71 (MaxPoolin  (None, 17, 1024)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_17 (Flatten)        (None, 17408)             0         \n",
            "                                                                 \n",
            " dense_68 (Dense)            (None, 512)               8913408   \n",
            "                                                                 \n",
            " batch_normalization_116 (Ba  (None, 512)              2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_116 (Activation)  (None, 512)              0         \n",
            "                                                                 \n",
            " dropout_51 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_69 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " batch_normalization_117 (Ba  (None, 256)              1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_117 (Activation)  (None, 256)              0         \n",
            "                                                                 \n",
            " dropout_52 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_70 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " batch_normalization_118 (Ba  (None, 128)              512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_118 (Activation)  (None, 128)              0         \n",
            "                                                                 \n",
            " dropout_53 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_71 (Dense)            (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,156,358\n",
            "Trainable params: 11,150,726\n",
            "Non-trainable params: 5,632\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.1221 - accuracy: 0.2979\n",
            "Epoch 1: val_accuracy improved from -inf to 0.33587, saving model to best_model.h5\n",
            "41/41 [==============================] - 6s 87ms/step - loss: 2.1225 - accuracy: 0.2975 - val_loss: 1.8874 - val_accuracy: 0.3359\n",
            "Epoch 2/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.9034 - accuracy: 0.3480\n",
            "Epoch 2: val_accuracy did not improve from 0.33587\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.9002 - accuracy: 0.3480 - val_loss: 4.4529 - val_accuracy: 0.2616\n",
            "Epoch 3/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.8430 - accuracy: 0.3499\n",
            "Epoch 3: val_accuracy did not improve from 0.33587\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.8430 - accuracy: 0.3499 - val_loss: 5.9492 - val_accuracy: 0.1704\n",
            "Epoch 4/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.8088 - accuracy: 0.3687\n",
            "Epoch 4: val_accuracy did not improve from 0.33587\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.8088 - accuracy: 0.3687 - val_loss: 5.8774 - val_accuracy: 0.1731\n",
            "Epoch 5/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7753 - accuracy: 0.3703\n",
            "Epoch 5: val_accuracy did not improve from 0.33587\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.7763 - accuracy: 0.3708 - val_loss: 5.7332 - val_accuracy: 0.1704\n",
            "Epoch 6/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7110 - accuracy: 0.3893\n",
            "Epoch 6: val_accuracy did not improve from 0.33587\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.7119 - accuracy: 0.3890 - val_loss: 4.6027 - val_accuracy: 0.1820\n",
            "Epoch 7/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7049 - accuracy: 0.3963\n",
            "Epoch 7: val_accuracy did not improve from 0.33587\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.7041 - accuracy: 0.3957 - val_loss: 2.7398 - val_accuracy: 0.2021\n",
            "Epoch 8/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6897 - accuracy: 0.3984\n",
            "Epoch 8: val_accuracy did not improve from 0.33587\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.6881 - accuracy: 0.3995 - val_loss: 2.3873 - val_accuracy: 0.2710\n",
            "Epoch 9/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6676 - accuracy: 0.4098\n",
            "Epoch 9: val_accuracy did not improve from 0.33587\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.6682 - accuracy: 0.4093 - val_loss: 3.0922 - val_accuracy: 0.2719\n",
            "Epoch 10/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6559 - accuracy: 0.4088\n",
            "Epoch 10: val_accuracy did not improve from 0.33587\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.6539 - accuracy: 0.4086 - val_loss: 3.2338 - val_accuracy: 0.1883\n",
            "Epoch 11/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6503 - accuracy: 0.4160\n",
            "Epoch 11: val_accuracy improved from 0.33587 to 0.36717, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 68ms/step - loss: 1.6491 - accuracy: 0.4172 - val_loss: 2.2414 - val_accuracy: 0.3672\n",
            "Epoch 12/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6328 - accuracy: 0.4119\n",
            "Epoch 12: val_accuracy did not improve from 0.36717\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.6308 - accuracy: 0.4124 - val_loss: 2.1726 - val_accuracy: 0.3332\n",
            "Epoch 13/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6013 - accuracy: 0.4262\n",
            "Epoch 13: val_accuracy improved from 0.36717 to 0.37254, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 70ms/step - loss: 1.6003 - accuracy: 0.4270 - val_loss: 1.9460 - val_accuracy: 0.3725\n",
            "Epoch 14/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.6198 - accuracy: 0.4275\n",
            "Epoch 14: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.6198 - accuracy: 0.4275 - val_loss: 1.8698 - val_accuracy: 0.3309\n",
            "Epoch 15/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5998 - accuracy: 0.4330\n",
            "Epoch 15: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.6002 - accuracy: 0.4325 - val_loss: 2.5485 - val_accuracy: 0.2393\n",
            "Epoch 16/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.5859 - accuracy: 0.4331\n",
            "Epoch 16: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5859 - accuracy: 0.4331 - val_loss: 1.9853 - val_accuracy: 0.3336\n",
            "Epoch 17/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.5767 - accuracy: 0.4402\n",
            "Epoch 17: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5767 - accuracy: 0.4402 - val_loss: 2.0877 - val_accuracy: 0.3323\n",
            "Epoch 18/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5738 - accuracy: 0.4283\n",
            "Epoch 18: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.5738 - accuracy: 0.4281 - val_loss: 2.0824 - val_accuracy: 0.3502\n",
            "Epoch 19/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.5522 - accuracy: 0.4469\n",
            "Epoch 19: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5522 - accuracy: 0.4469 - val_loss: 1.9711 - val_accuracy: 0.3081\n",
            "Epoch 20/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5471 - accuracy: 0.4420\n",
            "Epoch 20: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5474 - accuracy: 0.4417 - val_loss: 2.6459 - val_accuracy: 0.1945\n",
            "Epoch 21/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.5308 - accuracy: 0.4505\n",
            "Epoch 21: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.5308 - accuracy: 0.4505 - val_loss: 4.6095 - val_accuracy: 0.1722\n",
            "Epoch 22/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5272 - accuracy: 0.4605\n",
            "Epoch 22: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5263 - accuracy: 0.4603 - val_loss: 2.0091 - val_accuracy: 0.2952\n",
            "Epoch 23/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5166 - accuracy: 0.4639\n",
            "Epoch 23: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5178 - accuracy: 0.4626 - val_loss: 2.3335 - val_accuracy: 0.3023\n",
            "Epoch 24/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5066 - accuracy: 0.4621\n",
            "Epoch 24: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.5067 - accuracy: 0.4628 - val_loss: 2.1332 - val_accuracy: 0.2961\n",
            "Epoch 25/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4945 - accuracy: 0.4666\n",
            "Epoch 25: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.4911 - accuracy: 0.4684 - val_loss: 2.0841 - val_accuracy: 0.3099\n",
            "Epoch 26/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5008 - accuracy: 0.4670\n",
            "Epoch 26: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5012 - accuracy: 0.4664 - val_loss: 2.8742 - val_accuracy: 0.2625\n",
            "Epoch 27/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4720 - accuracy: 0.4785\n",
            "Epoch 27: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.4758 - accuracy: 0.4762 - val_loss: 2.7541 - val_accuracy: 0.3198\n",
            "Epoch 28/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4719 - accuracy: 0.4824\n",
            "Epoch 28: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.4726 - accuracy: 0.4820 - val_loss: 2.6508 - val_accuracy: 0.3211\n",
            "Epoch 29/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4597 - accuracy: 0.4896\n",
            "Epoch 29: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.4610 - accuracy: 0.4895 - val_loss: 2.9659 - val_accuracy: 0.2880\n",
            "Epoch 30/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4328 - accuracy: 0.5021\n",
            "Epoch 30: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.4323 - accuracy: 0.5023 - val_loss: 2.9190 - val_accuracy: 0.3046\n",
            "Epoch 31/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4356 - accuracy: 0.5049\n",
            "Epoch 31: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.4373 - accuracy: 0.5033 - val_loss: 2.4581 - val_accuracy: 0.3417\n",
            "Epoch 32/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4078 - accuracy: 0.5156\n",
            "Epoch 32: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.4058 - accuracy: 0.5176 - val_loss: 2.7635 - val_accuracy: 0.2809\n",
            "Epoch 33/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3981 - accuracy: 0.5178\n",
            "Epoch 33: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.4022 - accuracy: 0.5163 - val_loss: 2.7398 - val_accuracy: 0.2665\n",
            "Epoch 34/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3905 - accuracy: 0.5191\n",
            "Epoch 34: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.3899 - accuracy: 0.5190 - val_loss: 2.3451 - val_accuracy: 0.3189\n",
            "Epoch 35/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3701 - accuracy: 0.5328\n",
            "Epoch 35: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.3692 - accuracy: 0.5336 - val_loss: 2.3101 - val_accuracy: 0.2728\n",
            "Epoch 36/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3600 - accuracy: 0.5344\n",
            "Epoch 36: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.3596 - accuracy: 0.5351 - val_loss: 2.9471 - val_accuracy: 0.3072\n",
            "Epoch 37/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3375 - accuracy: 0.5400\n",
            "Epoch 37: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.3366 - accuracy: 0.5397 - val_loss: 2.9799 - val_accuracy: 0.3032\n",
            "Epoch 38/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3124 - accuracy: 0.5574\n",
            "Epoch 38: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.3124 - accuracy: 0.5573 - val_loss: 2.8188 - val_accuracy: 0.2160\n",
            "Epoch 39/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3059 - accuracy: 0.5600\n",
            "Epoch 39: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.3090 - accuracy: 0.5585 - val_loss: 4.4293 - val_accuracy: 0.2513\n",
            "Epoch 40/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.2806 - accuracy: 0.5666\n",
            "Epoch 40: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.2805 - accuracy: 0.5660 - val_loss: 3.8050 - val_accuracy: 0.2719\n",
            "Epoch 41/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.2591 - accuracy: 0.5883\n",
            "Epoch 41: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.2585 - accuracy: 0.5893 - val_loss: 2.5531 - val_accuracy: 0.3166\n",
            "Epoch 42/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.2379 - accuracy: 0.5922\n",
            "Epoch 42: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.2393 - accuracy: 0.5914 - val_loss: 5.1075 - val_accuracy: 0.2272\n",
            "Epoch 43/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.2154 - accuracy: 0.6117\n",
            "Epoch 43: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.2158 - accuracy: 0.6118 - val_loss: 4.2115 - val_accuracy: 0.3225\n",
            "Epoch 44/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.1767 - accuracy: 0.6293\n",
            "Epoch 44: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.1793 - accuracy: 0.6279 - val_loss: 2.4657 - val_accuracy: 0.3260\n",
            "Epoch 45/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.1437 - accuracy: 0.6379\n",
            "Epoch 45: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.1415 - accuracy: 0.6382 - val_loss: 2.8380 - val_accuracy: 0.3542\n",
            "Epoch 46/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.1124 - accuracy: 0.6534\n",
            "Epoch 46: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.1124 - accuracy: 0.6534 - val_loss: 2.8287 - val_accuracy: 0.2607\n",
            "Epoch 47/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.0925 - accuracy: 0.6674\n",
            "Epoch 47: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.0943 - accuracy: 0.6666 - val_loss: 3.8404 - val_accuracy: 0.3023\n",
            "Epoch 48/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.0584 - accuracy: 0.6865\n",
            "Epoch 48: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.0599 - accuracy: 0.6854 - val_loss: 3.7526 - val_accuracy: 0.2925\n",
            "Epoch 49/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.0370 - accuracy: 0.6932\n",
            "Epoch 49: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.0388 - accuracy: 0.6925 - val_loss: 2.7329 - val_accuracy: 0.3345\n",
            "Epoch 50/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.0104 - accuracy: 0.7049\n",
            "Epoch 50: val_accuracy did not improve from 0.37254\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.0130 - accuracy: 0.7034 - val_loss: 3.9403 - val_accuracy: 0.2446\n",
            "reg_param=0.0001, dr=0.4, test_acc=0.2446\n",
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_72 (Conv1D)          (None, 301, 128)          512       \n",
            "                                                                 \n",
            " batch_normalization_119 (Ba  (None, 301, 128)         512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_119 (Activation)  (None, 301, 128)         0         \n",
            "                                                                 \n",
            " max_pooling1d_72 (MaxPoolin  (None, 150, 128)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_73 (Conv1D)          (None, 148, 256)          98560     \n",
            "                                                                 \n",
            " batch_normalization_120 (Ba  (None, 148, 256)         1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_120 (Activation)  (None, 148, 256)         0         \n",
            "                                                                 \n",
            " max_pooling1d_73 (MaxPoolin  (None, 74, 256)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_74 (Conv1D)          (None, 72, 512)           393728    \n",
            "                                                                 \n",
            " batch_normalization_121 (Ba  (None, 72, 512)          2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_121 (Activation)  (None, 72, 512)          0         \n",
            "                                                                 \n",
            " max_pooling1d_74 (MaxPoolin  (None, 36, 512)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_75 (Conv1D)          (None, 34, 1024)          1573888   \n",
            "                                                                 \n",
            " batch_normalization_122 (Ba  (None, 34, 1024)         4096      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_122 (Activation)  (None, 34, 1024)         0         \n",
            "                                                                 \n",
            " max_pooling1d_75 (MaxPoolin  (None, 17, 1024)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_18 (Flatten)        (None, 17408)             0         \n",
            "                                                                 \n",
            " dense_72 (Dense)            (None, 512)               8913408   \n",
            "                                                                 \n",
            " batch_normalization_123 (Ba  (None, 512)              2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_123 (Activation)  (None, 512)              0         \n",
            "                                                                 \n",
            " dropout_54 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_73 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " batch_normalization_124 (Ba  (None, 256)              1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_124 (Activation)  (None, 256)              0         \n",
            "                                                                 \n",
            " dropout_55 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_74 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " batch_normalization_125 (Ba  (None, 128)              512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_125 (Activation)  (None, 128)              0         \n",
            "                                                                 \n",
            " dropout_56 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_75 (Dense)            (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,156,358\n",
            "Trainable params: 11,150,726\n",
            "Non-trainable params: 5,632\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.2028 - accuracy: 0.2682\n",
            "Epoch 1: val_accuracy improved from -inf to 0.19723, saving model to best_model.h5\n",
            "41/41 [==============================] - 7s 107ms/step - loss: 2.1975 - accuracy: 0.2699 - val_loss: 2.0428 - val_accuracy: 0.1972\n",
            "Epoch 2/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.0036 - accuracy: 0.3265\n",
            "Epoch 2: val_accuracy did not improve from 0.19723\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.0036 - accuracy: 0.3265 - val_loss: 5.7270 - val_accuracy: 0.1704\n",
            "Epoch 3/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.9098 - accuracy: 0.3439\n",
            "Epoch 3: val_accuracy did not improve from 0.19723\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.9084 - accuracy: 0.3441 - val_loss: 6.9552 - val_accuracy: 0.1704\n",
            "Epoch 4/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.8473 - accuracy: 0.3574\n",
            "Epoch 4: val_accuracy did not improve from 0.19723\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.8473 - accuracy: 0.3574 - val_loss: 7.5850 - val_accuracy: 0.1704\n",
            "Epoch 5/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8123 - accuracy: 0.3631\n",
            "Epoch 5: val_accuracy did not improve from 0.19723\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.8123 - accuracy: 0.3633 - val_loss: 6.9573 - val_accuracy: 0.1704\n",
            "Epoch 6/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.7901 - accuracy: 0.3704\n",
            "Epoch 6: val_accuracy did not improve from 0.19723\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.7901 - accuracy: 0.3704 - val_loss: 6.6216 - val_accuracy: 0.1704\n",
            "Epoch 7/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7626 - accuracy: 0.3738\n",
            "Epoch 7: val_accuracy did not improve from 0.19723\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.7647 - accuracy: 0.3731 - val_loss: 5.1510 - val_accuracy: 0.1704\n",
            "Epoch 8/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7325 - accuracy: 0.3785\n",
            "Epoch 8: val_accuracy did not improve from 0.19723\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.7321 - accuracy: 0.3786 - val_loss: 4.2978 - val_accuracy: 0.1726\n",
            "Epoch 9/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.7222 - accuracy: 0.3804\n",
            "Epoch 9: val_accuracy did not improve from 0.19723\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.7222 - accuracy: 0.3804 - val_loss: 3.2082 - val_accuracy: 0.1726\n",
            "Epoch 10/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6845 - accuracy: 0.4002\n",
            "Epoch 10: val_accuracy did not improve from 0.19723\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.6853 - accuracy: 0.4001 - val_loss: 2.8482 - val_accuracy: 0.1717\n",
            "Epoch 11/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.6781 - accuracy: 0.3984\n",
            "Epoch 11: val_accuracy improved from 0.19723 to 0.25626, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 69ms/step - loss: 1.6781 - accuracy: 0.3984 - val_loss: 2.0421 - val_accuracy: 0.2563\n",
            "Epoch 12/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.6638 - accuracy: 0.3955\n",
            "Epoch 12: val_accuracy did not improve from 0.25626\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.6638 - accuracy: 0.3955 - val_loss: 2.3010 - val_accuracy: 0.2491\n",
            "Epoch 13/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6514 - accuracy: 0.4072\n",
            "Epoch 13: val_accuracy improved from 0.25626 to 0.34034, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 74ms/step - loss: 1.6511 - accuracy: 0.4057 - val_loss: 2.0010 - val_accuracy: 0.3403\n",
            "Epoch 14/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.6285 - accuracy: 0.4181\n",
            "Epoch 14: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.6285 - accuracy: 0.4181 - val_loss: 2.8068 - val_accuracy: 0.1896\n",
            "Epoch 15/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6291 - accuracy: 0.4186\n",
            "Epoch 15: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.6270 - accuracy: 0.4195 - val_loss: 2.5693 - val_accuracy: 0.2943\n",
            "Epoch 16/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6304 - accuracy: 0.4125\n",
            "Epoch 16: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.6297 - accuracy: 0.4120 - val_loss: 2.1191 - val_accuracy: 0.3403\n",
            "Epoch 17/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.6099 - accuracy: 0.4252\n",
            "Epoch 17: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.6099 - accuracy: 0.4252 - val_loss: 2.2397 - val_accuracy: 0.2585\n",
            "Epoch 18/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.5919 - accuracy: 0.4337\n",
            "Epoch 18: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5919 - accuracy: 0.4337 - val_loss: 2.3480 - val_accuracy: 0.2925\n",
            "Epoch 19/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5942 - accuracy: 0.4309\n",
            "Epoch 19: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.5905 - accuracy: 0.4329 - val_loss: 2.0408 - val_accuracy: 0.3386\n",
            "Epoch 20/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5832 - accuracy: 0.4277\n",
            "Epoch 20: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5818 - accuracy: 0.4281 - val_loss: 2.2510 - val_accuracy: 0.2701\n",
            "Epoch 21/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5811 - accuracy: 0.4268\n",
            "Epoch 21: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5786 - accuracy: 0.4291 - val_loss: 2.0442 - val_accuracy: 0.3274\n",
            "Epoch 22/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5717 - accuracy: 0.4311\n",
            "Epoch 22: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5724 - accuracy: 0.4310 - val_loss: 2.5274 - val_accuracy: 0.2531\n",
            "Epoch 23/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5561 - accuracy: 0.4441\n",
            "Epoch 23: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5557 - accuracy: 0.4434 - val_loss: 3.5790 - val_accuracy: 0.2245\n",
            "Epoch 24/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5448 - accuracy: 0.4461\n",
            "Epoch 24: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5470 - accuracy: 0.4450 - val_loss: 3.4041 - val_accuracy: 0.1928\n",
            "Epoch 25/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5502 - accuracy: 0.4443\n",
            "Epoch 25: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5495 - accuracy: 0.4433 - val_loss: 2.7075 - val_accuracy: 0.2205\n",
            "Epoch 26/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5408 - accuracy: 0.4590\n",
            "Epoch 26: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.5404 - accuracy: 0.4584 - val_loss: 4.2794 - val_accuracy: 0.1708\n",
            "Epoch 27/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5336 - accuracy: 0.4500\n",
            "Epoch 27: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.5342 - accuracy: 0.4500 - val_loss: 2.3403 - val_accuracy: 0.2357\n",
            "Epoch 28/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.5249 - accuracy: 0.4555\n",
            "Epoch 28: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5249 - accuracy: 0.4555 - val_loss: 2.9187 - val_accuracy: 0.2509\n",
            "Epoch 29/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.5204 - accuracy: 0.4632\n",
            "Epoch 29: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5204 - accuracy: 0.4632 - val_loss: 2.9427 - val_accuracy: 0.2777\n",
            "Epoch 30/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5331 - accuracy: 0.4586\n",
            "Epoch 30: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5311 - accuracy: 0.4588 - val_loss: 2.3926 - val_accuracy: 0.3028\n",
            "Epoch 31/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5029 - accuracy: 0.4631\n",
            "Epoch 31: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5071 - accuracy: 0.4611 - val_loss: 2.3844 - val_accuracy: 0.2540\n",
            "Epoch 32/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4930 - accuracy: 0.4836\n",
            "Epoch 32: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.4971 - accuracy: 0.4814 - val_loss: 2.2540 - val_accuracy: 0.3256\n",
            "Epoch 33/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4959 - accuracy: 0.4713\n",
            "Epoch 33: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.4958 - accuracy: 0.4718 - val_loss: 2.7831 - val_accuracy: 0.2786\n",
            "Epoch 34/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4876 - accuracy: 0.4719\n",
            "Epoch 34: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.4873 - accuracy: 0.4707 - val_loss: 2.9577 - val_accuracy: 0.2826\n",
            "Epoch 35/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4804 - accuracy: 0.4859\n",
            "Epoch 35: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.4808 - accuracy: 0.4862 - val_loss: 3.1160 - val_accuracy: 0.2974\n",
            "Epoch 36/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4687 - accuracy: 0.4920\n",
            "Epoch 36: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.4681 - accuracy: 0.4914 - val_loss: 2.9235 - val_accuracy: 0.2719\n",
            "Epoch 37/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4524 - accuracy: 0.4916\n",
            "Epoch 37: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.4551 - accuracy: 0.4904 - val_loss: 2.4630 - val_accuracy: 0.3001\n",
            "Epoch 38/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4405 - accuracy: 0.4955\n",
            "Epoch 38: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.4380 - accuracy: 0.4967 - val_loss: 2.6417 - val_accuracy: 0.3238\n",
            "Epoch 39/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4236 - accuracy: 0.5104\n",
            "Epoch 39: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.4237 - accuracy: 0.5094 - val_loss: 3.2046 - val_accuracy: 0.2178\n",
            "Epoch 40/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4180 - accuracy: 0.5146\n",
            "Epoch 40: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.4187 - accuracy: 0.5148 - val_loss: 3.7485 - val_accuracy: 0.2129\n",
            "Epoch 41/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4121 - accuracy: 0.5090\n",
            "Epoch 41: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.4120 - accuracy: 0.5082 - val_loss: 3.7573 - val_accuracy: 0.1740\n",
            "Epoch 42/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4049 - accuracy: 0.5217\n",
            "Epoch 42: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.4041 - accuracy: 0.5211 - val_loss: 2.3654 - val_accuracy: 0.3305\n",
            "Epoch 43/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3745 - accuracy: 0.5314\n",
            "Epoch 43: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.3773 - accuracy: 0.5309 - val_loss: 2.5985 - val_accuracy: 0.3332\n",
            "Epoch 44/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3672 - accuracy: 0.5428\n",
            "Epoch 44: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.3689 - accuracy: 0.5414 - val_loss: 2.9116 - val_accuracy: 0.3050\n",
            "Epoch 45/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3558 - accuracy: 0.5469\n",
            "Epoch 45: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.3579 - accuracy: 0.5464 - val_loss: 2.9584 - val_accuracy: 0.3265\n",
            "Epoch 46/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3246 - accuracy: 0.5639\n",
            "Epoch 46: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.3256 - accuracy: 0.5640 - val_loss: 4.3078 - val_accuracy: 0.2357\n",
            "Epoch 47/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3091 - accuracy: 0.5627\n",
            "Epoch 47: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.3095 - accuracy: 0.5623 - val_loss: 3.1744 - val_accuracy: 0.3064\n",
            "Epoch 48/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.2887 - accuracy: 0.5789\n",
            "Epoch 48: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.2927 - accuracy: 0.5769 - val_loss: 4.2191 - val_accuracy: 0.1762\n",
            "Epoch 49/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.2756 - accuracy: 0.5869\n",
            "Epoch 49: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.2761 - accuracy: 0.5867 - val_loss: 3.6680 - val_accuracy: 0.2706\n",
            "Epoch 50/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.2481 - accuracy: 0.5992\n",
            "Epoch 50: val_accuracy did not improve from 0.34034\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.2517 - accuracy: 0.5980 - val_loss: 2.6311 - val_accuracy: 0.3041\n",
            "reg_param=0.0001, dr=0.5, test_acc=0.3041\n",
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_76 (Conv1D)          (None, 301, 128)          512       \n",
            "                                                                 \n",
            " batch_normalization_126 (Ba  (None, 301, 128)         512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_126 (Activation)  (None, 301, 128)         0         \n",
            "                                                                 \n",
            " max_pooling1d_76 (MaxPoolin  (None, 150, 128)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_77 (Conv1D)          (None, 148, 256)          98560     \n",
            "                                                                 \n",
            " batch_normalization_127 (Ba  (None, 148, 256)         1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_127 (Activation)  (None, 148, 256)         0         \n",
            "                                                                 \n",
            " max_pooling1d_77 (MaxPoolin  (None, 74, 256)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_78 (Conv1D)          (None, 72, 512)           393728    \n",
            "                                                                 \n",
            " batch_normalization_128 (Ba  (None, 72, 512)          2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_128 (Activation)  (None, 72, 512)          0         \n",
            "                                                                 \n",
            " max_pooling1d_78 (MaxPoolin  (None, 36, 512)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_79 (Conv1D)          (None, 34, 1024)          1573888   \n",
            "                                                                 \n",
            " batch_normalization_129 (Ba  (None, 34, 1024)         4096      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_129 (Activation)  (None, 34, 1024)         0         \n",
            "                                                                 \n",
            " max_pooling1d_79 (MaxPoolin  (None, 17, 1024)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_19 (Flatten)        (None, 17408)             0         \n",
            "                                                                 \n",
            " dense_76 (Dense)            (None, 512)               8913408   \n",
            "                                                                 \n",
            " batch_normalization_130 (Ba  (None, 512)              2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_130 (Activation)  (None, 512)              0         \n",
            "                                                                 \n",
            " dropout_57 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_77 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " batch_normalization_131 (Ba  (None, 256)              1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_131 (Activation)  (None, 256)              0         \n",
            "                                                                 \n",
            " dropout_58 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_78 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " batch_normalization_132 (Ba  (None, 128)              512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_132 (Activation)  (None, 128)              0         \n",
            "                                                                 \n",
            " dropout_59 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_79 (Dense)            (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,156,358\n",
            "Trainable params: 11,150,726\n",
            "Non-trainable params: 5,632\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 3.8963 - accuracy: 0.3623\n",
            "Epoch 1: val_accuracy improved from -inf to 0.18873, saving model to best_model.h5\n",
            "41/41 [==============================] - 5s 79ms/step - loss: 3.8909 - accuracy: 0.3620 - val_loss: 3.9770 - val_accuracy: 0.1887\n",
            "Epoch 2/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 3.3071 - accuracy: 0.4143\n",
            "Epoch 2: val_accuracy did not improve from 0.18873\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 3.3056 - accuracy: 0.4143 - val_loss: 5.5437 - val_accuracy: 0.1704\n",
            "Epoch 3/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 3.0286 - accuracy: 0.4195\n",
            "Epoch 3: val_accuracy did not improve from 0.18873\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 3.0286 - accuracy: 0.4195 - val_loss: 6.6638 - val_accuracy: 0.1704\n",
            "Epoch 4/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.8188 - accuracy: 0.4344\n",
            "Epoch 4: val_accuracy did not improve from 0.18873\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.8189 - accuracy: 0.4344 - val_loss: 6.6916 - val_accuracy: 0.1704\n",
            "Epoch 5/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.6773 - accuracy: 0.4394\n",
            "Epoch 5: val_accuracy improved from 0.18873 to 0.21467, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 66ms/step - loss: 2.6773 - accuracy: 0.4394 - val_loss: 5.9723 - val_accuracy: 0.2147\n",
            "Epoch 6/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.5362 - accuracy: 0.4500\n",
            "Epoch 6: val_accuracy did not improve from 0.21467\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 2.5362 - accuracy: 0.4500 - val_loss: 6.0153 - val_accuracy: 0.2035\n",
            "Epoch 7/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.4376 - accuracy: 0.4605\n",
            "Epoch 7: val_accuracy did not improve from 0.21467\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.4359 - accuracy: 0.4613 - val_loss: 5.7130 - val_accuracy: 0.1704\n",
            "Epoch 8/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.3567 - accuracy: 0.4633\n",
            "Epoch 8: val_accuracy improved from 0.21467 to 0.25089, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 73ms/step - loss: 2.3554 - accuracy: 0.4632 - val_loss: 5.5995 - val_accuracy: 0.2509\n",
            "Epoch 9/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.2730 - accuracy: 0.4672\n",
            "Epoch 9: val_accuracy did not improve from 0.25089\n",
            "41/41 [==============================] - 2s 61ms/step - loss: 2.2728 - accuracy: 0.4689 - val_loss: 3.8869 - val_accuracy: 0.2375\n",
            "Epoch 10/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.2194 - accuracy: 0.4809\n",
            "Epoch 10: val_accuracy did not improve from 0.25089\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 2.2197 - accuracy: 0.4810 - val_loss: 3.4122 - val_accuracy: 0.1744\n",
            "Epoch 11/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.1520 - accuracy: 0.4797\n",
            "Epoch 11: val_accuracy did not improve from 0.25089\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.1517 - accuracy: 0.4814 - val_loss: 5.4670 - val_accuracy: 0.2115\n",
            "Epoch 12/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.1171 - accuracy: 0.4887\n",
            "Epoch 12: val_accuracy did not improve from 0.25089\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 2.1167 - accuracy: 0.4887 - val_loss: 4.9437 - val_accuracy: 0.2048\n",
            "Epoch 13/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.0465 - accuracy: 0.4939\n",
            "Epoch 13: val_accuracy did not improve from 0.25089\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.0449 - accuracy: 0.4950 - val_loss: 5.4257 - val_accuracy: 0.2491\n",
            "Epoch 14/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.0016 - accuracy: 0.5043\n",
            "Epoch 14: val_accuracy improved from 0.25089 to 0.25313, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 73ms/step - loss: 2.0030 - accuracy: 0.5033 - val_loss: 3.5005 - val_accuracy: 0.2531\n",
            "Epoch 15/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.9466 - accuracy: 0.5123\n",
            "Epoch 15: val_accuracy did not improve from 0.25313\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.9476 - accuracy: 0.5123 - val_loss: 4.9396 - val_accuracy: 0.2375\n",
            "Epoch 16/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.9150 - accuracy: 0.5301\n",
            "Epoch 16: val_accuracy improved from 0.25313 to 0.29472, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 69ms/step - loss: 1.9154 - accuracy: 0.5305 - val_loss: 2.9986 - val_accuracy: 0.2947\n",
            "Epoch 17/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8827 - accuracy: 0.5326\n",
            "Epoch 17: val_accuracy did not improve from 0.29472\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.8810 - accuracy: 0.5341 - val_loss: 2.8800 - val_accuracy: 0.2187\n",
            "Epoch 18/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.8545 - accuracy: 0.5305\n",
            "Epoch 18: val_accuracy did not improve from 0.29472\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.8545 - accuracy: 0.5305 - val_loss: 3.4250 - val_accuracy: 0.2142\n",
            "Epoch 19/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7964 - accuracy: 0.5594\n",
            "Epoch 19: val_accuracy did not improve from 0.29472\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.7969 - accuracy: 0.5587 - val_loss: 4.1979 - val_accuracy: 0.1910\n",
            "Epoch 20/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7583 - accuracy: 0.5633\n",
            "Epoch 20: val_accuracy did not improve from 0.29472\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.7570 - accuracy: 0.5638 - val_loss: 4.3084 - val_accuracy: 0.2572\n",
            "Epoch 21/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.7132 - accuracy: 0.5830\n",
            "Epoch 21: val_accuracy did not improve from 0.29472\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.7132 - accuracy: 0.5830 - val_loss: 4.7138 - val_accuracy: 0.2335\n",
            "Epoch 22/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6747 - accuracy: 0.5998\n",
            "Epoch 22: val_accuracy did not improve from 0.29472\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.6795 - accuracy: 0.5972 - val_loss: 4.1610 - val_accuracy: 0.1722\n",
            "Epoch 23/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6320 - accuracy: 0.6168\n",
            "Epoch 23: val_accuracy did not improve from 0.29472\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.6305 - accuracy: 0.6168 - val_loss: 3.4766 - val_accuracy: 0.2549\n",
            "Epoch 24/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5789 - accuracy: 0.6311\n",
            "Epoch 24: val_accuracy did not improve from 0.29472\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5844 - accuracy: 0.6298 - val_loss: 3.4475 - val_accuracy: 0.2460\n",
            "Epoch 25/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5620 - accuracy: 0.6422\n",
            "Epoch 25: val_accuracy improved from 0.29472 to 0.29741, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 71ms/step - loss: 1.5629 - accuracy: 0.6405 - val_loss: 3.8202 - val_accuracy: 0.2974\n",
            "Epoch 26/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4954 - accuracy: 0.6719\n",
            "Epoch 26: val_accuracy did not improve from 0.29741\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.4978 - accuracy: 0.6699 - val_loss: 3.6441 - val_accuracy: 0.2209\n",
            "Epoch 27/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4472 - accuracy: 0.6918\n",
            "Epoch 27: val_accuracy did not improve from 0.29741\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.4481 - accuracy: 0.6921 - val_loss: 3.6221 - val_accuracy: 0.2227\n",
            "Epoch 28/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4191 - accuracy: 0.7094\n",
            "Epoch 28: val_accuracy did not improve from 0.29741\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.4187 - accuracy: 0.7092 - val_loss: 5.4409 - val_accuracy: 0.1708\n",
            "Epoch 29/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3718 - accuracy: 0.7303\n",
            "Epoch 29: val_accuracy did not improve from 0.29741\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.3725 - accuracy: 0.7297 - val_loss: 5.5260 - val_accuracy: 0.1802\n",
            "Epoch 30/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3221 - accuracy: 0.7480\n",
            "Epoch 30: val_accuracy did not improve from 0.29741\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.3267 - accuracy: 0.7456 - val_loss: 4.8082 - val_accuracy: 0.1708\n",
            "Epoch 31/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.2772 - accuracy: 0.7684\n",
            "Epoch 31: val_accuracy did not improve from 0.29741\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.2772 - accuracy: 0.7684 - val_loss: 3.1642 - val_accuracy: 0.2411\n",
            "Epoch 32/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.2561 - accuracy: 0.7725\n",
            "Epoch 32: val_accuracy did not improve from 0.29741\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.2577 - accuracy: 0.7713 - val_loss: 3.8293 - val_accuracy: 0.2625\n",
            "Epoch 33/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.2090 - accuracy: 0.7988\n",
            "Epoch 33: val_accuracy did not improve from 0.29741\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.2120 - accuracy: 0.7968 - val_loss: 5.2694 - val_accuracy: 0.1914\n",
            "Epoch 34/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.1450 - accuracy: 0.8184\n",
            "Epoch 34: val_accuracy did not improve from 0.29741\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.1462 - accuracy: 0.8175 - val_loss: 4.5699 - val_accuracy: 0.2598\n",
            "Epoch 35/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.0878 - accuracy: 0.8400\n",
            "Epoch 35: val_accuracy did not improve from 0.29741\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.0901 - accuracy: 0.8390 - val_loss: 4.7265 - val_accuracy: 0.1740\n",
            "Epoch 36/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.0517 - accuracy: 0.8561\n",
            "Epoch 36: val_accuracy did not improve from 0.29741\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.0513 - accuracy: 0.8562 - val_loss: 4.5338 - val_accuracy: 0.2701\n",
            "Epoch 37/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.0427 - accuracy: 0.8553\n",
            "Epoch 37: val_accuracy did not improve from 0.29741\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.0449 - accuracy: 0.8551 - val_loss: 7.6264 - val_accuracy: 0.1807\n",
            "Epoch 38/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.0280 - accuracy: 0.8625\n",
            "Epoch 38: val_accuracy did not improve from 0.29741\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.0314 - accuracy: 0.8612 - val_loss: 4.2841 - val_accuracy: 0.2250\n",
            "Epoch 39/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.0226 - accuracy: 0.8656\n",
            "Epoch 39: val_accuracy did not improve from 0.29741\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.0242 - accuracy: 0.8648 - val_loss: 8.3393 - val_accuracy: 0.1708\n",
            "Epoch 40/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.9935 - accuracy: 0.8750\n",
            "Epoch 40: val_accuracy did not improve from 0.29741\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 0.9935 - accuracy: 0.8748 - val_loss: 3.5898 - val_accuracy: 0.2245\n",
            "Epoch 41/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.9484 - accuracy: 0.8967\n",
            "Epoch 41: val_accuracy did not improve from 0.29741\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 0.9484 - accuracy: 0.8967 - val_loss: 9.1687 - val_accuracy: 0.2335\n",
            "Epoch 42/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.9317 - accuracy: 0.8934\n",
            "Epoch 42: val_accuracy did not improve from 0.29741\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 0.9317 - accuracy: 0.8934 - val_loss: 4.6084 - val_accuracy: 0.2384\n",
            "Epoch 43/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.8885 - accuracy: 0.9119\n",
            "Epoch 43: val_accuracy did not improve from 0.29741\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 0.8894 - accuracy: 0.9116 - val_loss: 5.6195 - val_accuracy: 0.1941\n",
            "Epoch 44/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.8406 - accuracy: 0.9256\n",
            "Epoch 44: val_accuracy did not improve from 0.29741\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 0.8426 - accuracy: 0.9245 - val_loss: 4.8336 - val_accuracy: 0.2724\n",
            "Epoch 45/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.8376 - accuracy: 0.9211\n",
            "Epoch 45: val_accuracy did not improve from 0.29741\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 0.8386 - accuracy: 0.9210 - val_loss: 7.3225 - val_accuracy: 0.1789\n",
            "Epoch 46/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.8291 - accuracy: 0.9230\n",
            "Epoch 46: val_accuracy did not improve from 0.29741\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 0.8302 - accuracy: 0.9225 - val_loss: 6.2748 - val_accuracy: 0.2473\n",
            "Epoch 47/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.8558 - accuracy: 0.9131\n",
            "Epoch 47: val_accuracy did not improve from 0.29741\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 0.8574 - accuracy: 0.9120 - val_loss: 6.6147 - val_accuracy: 0.1744\n",
            "Epoch 48/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.8560 - accuracy: 0.9152\n",
            "Epoch 48: val_accuracy improved from 0.29741 to 0.33274, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 68ms/step - loss: 0.8569 - accuracy: 0.9153 - val_loss: 3.1577 - val_accuracy: 0.3327\n",
            "Epoch 49/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.8479 - accuracy: 0.9184\n",
            "Epoch 49: val_accuracy did not improve from 0.33274\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 0.8513 - accuracy: 0.9162 - val_loss: 6.0128 - val_accuracy: 0.1784\n",
            "Epoch 50/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.8238 - accuracy: 0.9264\n",
            "Epoch 50: val_accuracy did not improve from 0.33274\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 0.8236 - accuracy: 0.9264 - val_loss: 6.1065 - val_accuracy: 0.1820\n",
            "reg_param=0.001, dr=0.1, test_acc=0.1820\n",
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_80 (Conv1D)          (None, 301, 128)          512       \n",
            "                                                                 \n",
            " batch_normalization_133 (Ba  (None, 301, 128)         512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_133 (Activation)  (None, 301, 128)         0         \n",
            "                                                                 \n",
            " max_pooling1d_80 (MaxPoolin  (None, 150, 128)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_81 (Conv1D)          (None, 148, 256)          98560     \n",
            "                                                                 \n",
            " batch_normalization_134 (Ba  (None, 148, 256)         1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_134 (Activation)  (None, 148, 256)         0         \n",
            "                                                                 \n",
            " max_pooling1d_81 (MaxPoolin  (None, 74, 256)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_82 (Conv1D)          (None, 72, 512)           393728    \n",
            "                                                                 \n",
            " batch_normalization_135 (Ba  (None, 72, 512)          2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_135 (Activation)  (None, 72, 512)          0         \n",
            "                                                                 \n",
            " max_pooling1d_82 (MaxPoolin  (None, 36, 512)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_83 (Conv1D)          (None, 34, 1024)          1573888   \n",
            "                                                                 \n",
            " batch_normalization_136 (Ba  (None, 34, 1024)         4096      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_136 (Activation)  (None, 34, 1024)         0         \n",
            "                                                                 \n",
            " max_pooling1d_83 (MaxPoolin  (None, 17, 1024)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_20 (Flatten)        (None, 17408)             0         \n",
            "                                                                 \n",
            " dense_80 (Dense)            (None, 512)               8913408   \n",
            "                                                                 \n",
            " batch_normalization_137 (Ba  (None, 512)              2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_137 (Activation)  (None, 512)              0         \n",
            "                                                                 \n",
            " dropout_60 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_81 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " batch_normalization_138 (Ba  (None, 256)              1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_138 (Activation)  (None, 256)              0         \n",
            "                                                                 \n",
            " dropout_61 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_82 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " batch_normalization_139 (Ba  (None, 128)              512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_139 (Activation)  (None, 128)              0         \n",
            "                                                                 \n",
            " dropout_62 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_83 (Dense)            (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,156,358\n",
            "Trainable params: 11,150,726\n",
            "Non-trainable params: 5,632\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 4.0059 - accuracy: 0.3372\n",
            "Epoch 1: val_accuracy improved from -inf to 0.17039, saving model to best_model.h5\n",
            "41/41 [==============================] - 6s 87ms/step - loss: 4.0059 - accuracy: 0.3372 - val_loss: 4.5242 - val_accuracy: 0.1704\n",
            "Epoch 2/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 3.4392 - accuracy: 0.3855\n",
            "Epoch 2: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 3.4392 - accuracy: 0.3855 - val_loss: 7.0925 - val_accuracy: 0.1704\n",
            "Epoch 3/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 3.1510 - accuracy: 0.4062\n",
            "Epoch 3: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 3.1469 - accuracy: 0.4072 - val_loss: 8.5009 - val_accuracy: 0.1704\n",
            "Epoch 4/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.9439 - accuracy: 0.4201\n",
            "Epoch 4: val_accuracy improved from 0.17039 to 0.25134, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 67ms/step - loss: 2.9407 - accuracy: 0.4201 - val_loss: 7.8067 - val_accuracy: 0.2513\n",
            "Epoch 5/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.7938 - accuracy: 0.4176\n",
            "Epoch 5: val_accuracy did not improve from 0.25134\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 2.7938 - accuracy: 0.4176 - val_loss: 7.8569 - val_accuracy: 0.1852\n",
            "Epoch 6/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.6578 - accuracy: 0.4310\n",
            "Epoch 6: val_accuracy did not improve from 0.25134\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 2.6578 - accuracy: 0.4310 - val_loss: 7.3448 - val_accuracy: 0.1704\n",
            "Epoch 7/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.5524 - accuracy: 0.4283\n",
            "Epoch 7: val_accuracy did not improve from 0.25134\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.5512 - accuracy: 0.4285 - val_loss: 6.5122 - val_accuracy: 0.1704\n",
            "Epoch 8/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.4668 - accuracy: 0.4303\n",
            "Epoch 8: val_accuracy did not improve from 0.25134\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.4680 - accuracy: 0.4296 - val_loss: 6.4934 - val_accuracy: 0.1704\n",
            "Epoch 9/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.3964 - accuracy: 0.4254\n",
            "Epoch 9: val_accuracy did not improve from 0.25134\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.3947 - accuracy: 0.4250 - val_loss: 3.8691 - val_accuracy: 0.2330\n",
            "Epoch 10/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.3192 - accuracy: 0.4436\n",
            "Epoch 10: val_accuracy improved from 0.25134 to 0.28086, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 68ms/step - loss: 2.3204 - accuracy: 0.4433 - val_loss: 3.5085 - val_accuracy: 0.2809\n",
            "Epoch 11/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.2513 - accuracy: 0.4537\n",
            "Epoch 11: val_accuracy did not improve from 0.28086\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.2517 - accuracy: 0.4530 - val_loss: 4.0192 - val_accuracy: 0.2089\n",
            "Epoch 12/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.1891 - accuracy: 0.4551\n",
            "Epoch 12: val_accuracy did not improve from 0.28086\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 2.1887 - accuracy: 0.4542 - val_loss: 4.0703 - val_accuracy: 0.2066\n",
            "Epoch 13/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.1423 - accuracy: 0.4678\n",
            "Epoch 13: val_accuracy improved from 0.28086 to 0.31887, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 70ms/step - loss: 2.1434 - accuracy: 0.4666 - val_loss: 3.3027 - val_accuracy: 0.3189\n",
            "Epoch 14/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.1036 - accuracy: 0.4617\n",
            "Epoch 14: val_accuracy did not improve from 0.31887\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.1072 - accuracy: 0.4597 - val_loss: 3.6297 - val_accuracy: 0.1775\n",
            "Epoch 15/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.0659 - accuracy: 0.4613\n",
            "Epoch 15: val_accuracy did not improve from 0.31887\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.0665 - accuracy: 0.4618 - val_loss: 3.2571 - val_accuracy: 0.2536\n",
            "Epoch 16/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.0272 - accuracy: 0.4670\n",
            "Epoch 16: val_accuracy did not improve from 0.31887\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.0264 - accuracy: 0.4678 - val_loss: 4.1765 - val_accuracy: 0.2196\n",
            "Epoch 17/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.9914 - accuracy: 0.4697\n",
            "Epoch 17: val_accuracy did not improve from 0.31887\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.9909 - accuracy: 0.4693 - val_loss: 4.9648 - val_accuracy: 0.2428\n",
            "Epoch 18/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.9433 - accuracy: 0.4826\n",
            "Epoch 18: val_accuracy did not improve from 0.31887\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.9436 - accuracy: 0.4822 - val_loss: 5.4904 - val_accuracy: 0.1704\n",
            "Epoch 19/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.9165 - accuracy: 0.4832\n",
            "Epoch 19: val_accuracy did not improve from 0.31887\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.9173 - accuracy: 0.4833 - val_loss: 4.7332 - val_accuracy: 0.2013\n",
            "Epoch 20/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.8977 - accuracy: 0.4850\n",
            "Epoch 20: val_accuracy did not improve from 0.31887\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.8977 - accuracy: 0.4850 - val_loss: 4.2426 - val_accuracy: 0.2612\n",
            "Epoch 21/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8734 - accuracy: 0.4959\n",
            "Epoch 21: val_accuracy did not improve from 0.31887\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.8730 - accuracy: 0.4954 - val_loss: 4.4108 - val_accuracy: 0.2603\n",
            "Epoch 22/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.8543 - accuracy: 0.4962\n",
            "Epoch 22: val_accuracy did not improve from 0.31887\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.8543 - accuracy: 0.4962 - val_loss: 4.8046 - val_accuracy: 0.2607\n",
            "Epoch 23/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8126 - accuracy: 0.5068\n",
            "Epoch 23: val_accuracy did not improve from 0.31887\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.8129 - accuracy: 0.5063 - val_loss: 5.7731 - val_accuracy: 0.2482\n",
            "Epoch 24/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.7901 - accuracy: 0.5140\n",
            "Epoch 24: val_accuracy improved from 0.31887 to 0.32245, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 73ms/step - loss: 1.7901 - accuracy: 0.5140 - val_loss: 3.0808 - val_accuracy: 0.3225\n",
            "Epoch 25/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7915 - accuracy: 0.5031\n",
            "Epoch 25: val_accuracy did not improve from 0.32245\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.7914 - accuracy: 0.5031 - val_loss: 2.9685 - val_accuracy: 0.2607\n",
            "Epoch 26/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7513 - accuracy: 0.5244\n",
            "Epoch 26: val_accuracy did not improve from 0.32245\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.7524 - accuracy: 0.5242 - val_loss: 5.7242 - val_accuracy: 0.1713\n",
            "Epoch 27/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7229 - accuracy: 0.5375\n",
            "Epoch 27: val_accuracy did not improve from 0.32245\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.7231 - accuracy: 0.5370 - val_loss: 4.2414 - val_accuracy: 0.2871\n",
            "Epoch 28/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7134 - accuracy: 0.5338\n",
            "Epoch 28: val_accuracy did not improve from 0.32245\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.7127 - accuracy: 0.5347 - val_loss: 5.2994 - val_accuracy: 0.1744\n",
            "Epoch 29/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6904 - accuracy: 0.5408\n",
            "Epoch 29: val_accuracy did not improve from 0.32245\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.6929 - accuracy: 0.5395 - val_loss: 2.9728 - val_accuracy: 0.2853\n",
            "Epoch 30/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6740 - accuracy: 0.5516\n",
            "Epoch 30: val_accuracy did not improve from 0.32245\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.6739 - accuracy: 0.5521 - val_loss: 3.2234 - val_accuracy: 0.2934\n",
            "Epoch 31/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.6394 - accuracy: 0.5613\n",
            "Epoch 31: val_accuracy did not improve from 0.32245\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.6394 - accuracy: 0.5613 - val_loss: 4.4313 - val_accuracy: 0.2719\n",
            "Epoch 32/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.5823 - accuracy: 0.5905\n",
            "Epoch 32: val_accuracy did not improve from 0.32245\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.5823 - accuracy: 0.5905 - val_loss: 5.6829 - val_accuracy: 0.1758\n",
            "Epoch 33/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.5777 - accuracy: 0.5872\n",
            "Epoch 33: val_accuracy did not improve from 0.32245\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5777 - accuracy: 0.5872 - val_loss: 3.2821 - val_accuracy: 0.2616\n",
            "Epoch 34/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5547 - accuracy: 0.6002\n",
            "Epoch 34: val_accuracy did not improve from 0.32245\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5569 - accuracy: 0.6001 - val_loss: 4.2046 - val_accuracy: 0.2335\n",
            "Epoch 35/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.5345 - accuracy: 0.6045\n",
            "Epoch 35: val_accuracy did not improve from 0.32245\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.5345 - accuracy: 0.6045 - val_loss: 3.4790 - val_accuracy: 0.2835\n",
            "Epoch 36/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5081 - accuracy: 0.6221\n",
            "Epoch 36: val_accuracy did not improve from 0.32245\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.5094 - accuracy: 0.6221 - val_loss: 5.2980 - val_accuracy: 0.1708\n",
            "Epoch 37/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4784 - accuracy: 0.6373\n",
            "Epoch 37: val_accuracy did not improve from 0.32245\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.4768 - accuracy: 0.6380 - val_loss: 4.3354 - val_accuracy: 0.1860\n",
            "Epoch 38/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4419 - accuracy: 0.6551\n",
            "Epoch 38: val_accuracy did not improve from 0.32245\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.4427 - accuracy: 0.6547 - val_loss: 3.7957 - val_accuracy: 0.2428\n",
            "Epoch 39/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4227 - accuracy: 0.6670\n",
            "Epoch 39: val_accuracy did not improve from 0.32245\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.4238 - accuracy: 0.6662 - val_loss: 3.7508 - val_accuracy: 0.2156\n",
            "Epoch 40/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3897 - accuracy: 0.6814\n",
            "Epoch 40: val_accuracy did not improve from 0.32245\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.3884 - accuracy: 0.6825 - val_loss: 3.1837 - val_accuracy: 0.2496\n",
            "Epoch 41/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3417 - accuracy: 0.7020\n",
            "Epoch 41: val_accuracy did not improve from 0.32245\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.3418 - accuracy: 0.7013 - val_loss: 7.3665 - val_accuracy: 0.1704\n",
            "Epoch 42/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3154 - accuracy: 0.7170\n",
            "Epoch 42: val_accuracy did not improve from 0.32245\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.3149 - accuracy: 0.7178 - val_loss: 3.1407 - val_accuracy: 0.3122\n",
            "Epoch 43/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.2826 - accuracy: 0.7320\n",
            "Epoch 43: val_accuracy did not improve from 0.32245\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.2817 - accuracy: 0.7322 - val_loss: 3.9267 - val_accuracy: 0.2062\n",
            "Epoch 44/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.2827 - accuracy: 0.7344\n",
            "Epoch 44: val_accuracy did not improve from 0.32245\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.2867 - accuracy: 0.7322 - val_loss: 5.1809 - val_accuracy: 0.1726\n",
            "Epoch 45/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.2179 - accuracy: 0.7625\n",
            "Epoch 45: val_accuracy did not improve from 0.32245\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.2191 - accuracy: 0.7619 - val_loss: 3.1438 - val_accuracy: 0.2692\n",
            "Epoch 46/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.1898 - accuracy: 0.7764\n",
            "Epoch 46: val_accuracy did not improve from 0.32245\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.1916 - accuracy: 0.7749 - val_loss: 2.6639 - val_accuracy: 0.3081\n",
            "Epoch 47/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.1653 - accuracy: 0.7865\n",
            "Epoch 47: val_accuracy did not improve from 0.32245\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.1661 - accuracy: 0.7864 - val_loss: 3.1668 - val_accuracy: 0.2979\n",
            "Epoch 48/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.1326 - accuracy: 0.7988\n",
            "Epoch 48: val_accuracy did not improve from 0.32245\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.1331 - accuracy: 0.7985 - val_loss: 3.9654 - val_accuracy: 0.2008\n",
            "Epoch 49/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.0908 - accuracy: 0.8186\n",
            "Epoch 49: val_accuracy did not improve from 0.32245\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.0908 - accuracy: 0.8186 - val_loss: 4.8179 - val_accuracy: 0.1726\n",
            "Epoch 50/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.0668 - accuracy: 0.8289\n",
            "Epoch 50: val_accuracy did not improve from 0.32245\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.0679 - accuracy: 0.8282 - val_loss: 3.6395 - val_accuracy: 0.2339\n",
            "reg_param=0.001, dr=0.2, test_acc=0.2339\n",
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_84 (Conv1D)          (None, 301, 128)          512       \n",
            "                                                                 \n",
            " batch_normalization_140 (Ba  (None, 301, 128)         512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_140 (Activation)  (None, 301, 128)         0         \n",
            "                                                                 \n",
            " max_pooling1d_84 (MaxPoolin  (None, 150, 128)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_85 (Conv1D)          (None, 148, 256)          98560     \n",
            "                                                                 \n",
            " batch_normalization_141 (Ba  (None, 148, 256)         1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_141 (Activation)  (None, 148, 256)         0         \n",
            "                                                                 \n",
            " max_pooling1d_85 (MaxPoolin  (None, 74, 256)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_86 (Conv1D)          (None, 72, 512)           393728    \n",
            "                                                                 \n",
            " batch_normalization_142 (Ba  (None, 72, 512)          2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_142 (Activation)  (None, 72, 512)          0         \n",
            "                                                                 \n",
            " max_pooling1d_86 (MaxPoolin  (None, 36, 512)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_87 (Conv1D)          (None, 34, 1024)          1573888   \n",
            "                                                                 \n",
            " batch_normalization_143 (Ba  (None, 34, 1024)         4096      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_143 (Activation)  (None, 34, 1024)         0         \n",
            "                                                                 \n",
            " max_pooling1d_87 (MaxPoolin  (None, 17, 1024)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_21 (Flatten)        (None, 17408)             0         \n",
            "                                                                 \n",
            " dense_84 (Dense)            (None, 512)               8913408   \n",
            "                                                                 \n",
            " batch_normalization_144 (Ba  (None, 512)              2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_144 (Activation)  (None, 512)              0         \n",
            "                                                                 \n",
            " dropout_63 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_85 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " batch_normalization_145 (Ba  (None, 256)              1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_145 (Activation)  (None, 256)              0         \n",
            "                                                                 \n",
            " dropout_64 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_86 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " batch_normalization_146 (Ba  (None, 128)              512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_146 (Activation)  (None, 128)              0         \n",
            "                                                                 \n",
            " dropout_65 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_87 (Dense)            (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,156,358\n",
            "Trainable params: 11,150,726\n",
            "Non-trainable params: 5,632\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 4.0292 - accuracy: 0.3340\n",
            "Epoch 1: val_accuracy improved from -inf to 0.18023, saving model to best_model.h5\n",
            "41/41 [==============================] - 6s 83ms/step - loss: 4.0205 - accuracy: 0.3353 - val_loss: 3.8947 - val_accuracy: 0.1802\n",
            "Epoch 2/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 3.4440 - accuracy: 0.3723\n",
            "Epoch 2: val_accuracy did not improve from 0.18023\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 3.4407 - accuracy: 0.3723 - val_loss: 6.9095 - val_accuracy: 0.1704\n",
            "Epoch 3/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 3.1373 - accuracy: 0.3990\n",
            "Epoch 3: val_accuracy did not improve from 0.18023\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 3.1356 - accuracy: 0.3993 - val_loss: 10.0340 - val_accuracy: 0.1704\n",
            "Epoch 4/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.9587 - accuracy: 0.3951\n",
            "Epoch 4: val_accuracy did not improve from 0.18023\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.9569 - accuracy: 0.3957 - val_loss: 9.1220 - val_accuracy: 0.1704\n",
            "Epoch 5/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.7859 - accuracy: 0.4047\n",
            "Epoch 5: val_accuracy did not improve from 0.18023\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 2.7855 - accuracy: 0.4038 - val_loss: 9.0144 - val_accuracy: 0.1704\n",
            "Epoch 6/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.6729 - accuracy: 0.3998\n",
            "Epoch 6: val_accuracy improved from 0.18023 to 0.18068, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 73ms/step - loss: 2.6711 - accuracy: 0.4003 - val_loss: 5.4441 - val_accuracy: 0.1807\n",
            "Epoch 7/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.5562 - accuracy: 0.4095\n",
            "Epoch 7: val_accuracy did not improve from 0.18068\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.5562 - accuracy: 0.4095 - val_loss: 7.5549 - val_accuracy: 0.1704\n",
            "Epoch 8/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.4689 - accuracy: 0.4193\n",
            "Epoch 8: val_accuracy did not improve from 0.18068\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.4689 - accuracy: 0.4201 - val_loss: 5.5592 - val_accuracy: 0.1704\n",
            "Epoch 9/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.4012 - accuracy: 0.4242\n",
            "Epoch 9: val_accuracy did not improve from 0.18068\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 2.4004 - accuracy: 0.4254 - val_loss: 5.0332 - val_accuracy: 0.1704\n",
            "Epoch 10/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.3488 - accuracy: 0.4264\n",
            "Epoch 10: val_accuracy did not improve from 0.18068\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.3488 - accuracy: 0.4264 - val_loss: 4.9348 - val_accuracy: 0.1735\n",
            "Epoch 11/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.2766 - accuracy: 0.4224\n",
            "Epoch 11: val_accuracy did not improve from 0.18068\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.2766 - accuracy: 0.4224 - val_loss: 4.3277 - val_accuracy: 0.1708\n",
            "Epoch 12/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.2409 - accuracy: 0.4313\n",
            "Epoch 12: val_accuracy did not improve from 0.18068\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 2.2397 - accuracy: 0.4312 - val_loss: 4.7606 - val_accuracy: 0.1708\n",
            "Epoch 13/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.1783 - accuracy: 0.4387\n",
            "Epoch 13: val_accuracy improved from 0.18068 to 0.18470, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 71ms/step - loss: 2.1782 - accuracy: 0.4385 - val_loss: 3.9713 - val_accuracy: 0.1847\n",
            "Epoch 14/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.1309 - accuracy: 0.4463\n",
            "Epoch 14: val_accuracy did not improve from 0.18470\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 2.1309 - accuracy: 0.4463 - val_loss: 3.7444 - val_accuracy: 0.1708\n",
            "Epoch 15/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.0895 - accuracy: 0.4477\n",
            "Epoch 15: val_accuracy improved from 0.18470 to 0.24687, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 68ms/step - loss: 2.0883 - accuracy: 0.4477 - val_loss: 4.5845 - val_accuracy: 0.2469\n",
            "Epoch 16/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.0555 - accuracy: 0.4527\n",
            "Epoch 16: val_accuracy did not improve from 0.24687\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.0586 - accuracy: 0.4502 - val_loss: 4.6707 - val_accuracy: 0.1708\n",
            "Epoch 17/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.0219 - accuracy: 0.4457\n",
            "Epoch 17: val_accuracy did not improve from 0.24687\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.0215 - accuracy: 0.4471 - val_loss: 3.3637 - val_accuracy: 0.1726\n",
            "Epoch 18/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.9992 - accuracy: 0.4441\n",
            "Epoch 18: val_accuracy improved from 0.24687 to 0.26968, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 77ms/step - loss: 1.9967 - accuracy: 0.4452 - val_loss: 4.0369 - val_accuracy: 0.2697\n",
            "Epoch 19/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.9615 - accuracy: 0.4545\n",
            "Epoch 19: val_accuracy did not improve from 0.26968\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.9623 - accuracy: 0.4538 - val_loss: 4.1416 - val_accuracy: 0.1713\n",
            "Epoch 20/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.9414 - accuracy: 0.4670\n",
            "Epoch 20: val_accuracy improved from 0.26968 to 0.27281, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 69ms/step - loss: 1.9438 - accuracy: 0.4661 - val_loss: 2.7045 - val_accuracy: 0.2728\n",
            "Epoch 21/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.9141 - accuracy: 0.4680\n",
            "Epoch 21: val_accuracy did not improve from 0.27281\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.9136 - accuracy: 0.4680 - val_loss: 5.8785 - val_accuracy: 0.2160\n",
            "Epoch 22/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8836 - accuracy: 0.4748\n",
            "Epoch 22: val_accuracy did not improve from 0.27281\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.8840 - accuracy: 0.4741 - val_loss: 4.9217 - val_accuracy: 0.1708\n",
            "Epoch 23/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8740 - accuracy: 0.4676\n",
            "Epoch 23: val_accuracy did not improve from 0.27281\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.8760 - accuracy: 0.4666 - val_loss: 3.0423 - val_accuracy: 0.2026\n",
            "Epoch 24/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8443 - accuracy: 0.4734\n",
            "Epoch 24: val_accuracy did not improve from 0.27281\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.8465 - accuracy: 0.4735 - val_loss: 3.6069 - val_accuracy: 0.1708\n",
            "Epoch 25/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8258 - accuracy: 0.4789\n",
            "Epoch 25: val_accuracy did not improve from 0.27281\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.8246 - accuracy: 0.4789 - val_loss: 3.9589 - val_accuracy: 0.1708\n",
            "Epoch 26/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8125 - accuracy: 0.4768\n",
            "Epoch 26: val_accuracy did not improve from 0.27281\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.8150 - accuracy: 0.4745 - val_loss: 4.7615 - val_accuracy: 0.1708\n",
            "Epoch 27/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8048 - accuracy: 0.4811\n",
            "Epoch 27: val_accuracy did not improve from 0.27281\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.8089 - accuracy: 0.4797 - val_loss: 4.1648 - val_accuracy: 0.1717\n",
            "Epoch 28/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.7860 - accuracy: 0.4873\n",
            "Epoch 28: val_accuracy did not improve from 0.27281\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.7860 - accuracy: 0.4873 - val_loss: 4.0184 - val_accuracy: 0.1749\n",
            "Epoch 29/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7504 - accuracy: 0.4904\n",
            "Epoch 29: val_accuracy did not improve from 0.27281\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.7495 - accuracy: 0.4908 - val_loss: 4.6285 - val_accuracy: 0.1708\n",
            "Epoch 30/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.7616 - accuracy: 0.4877\n",
            "Epoch 30: val_accuracy did not improve from 0.27281\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.7616 - accuracy: 0.4877 - val_loss: 4.0651 - val_accuracy: 0.1708\n",
            "Epoch 31/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7362 - accuracy: 0.4943\n",
            "Epoch 31: val_accuracy did not improve from 0.27281\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.7385 - accuracy: 0.4941 - val_loss: 5.2117 - val_accuracy: 0.1704\n",
            "Epoch 32/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7114 - accuracy: 0.5063\n",
            "Epoch 32: val_accuracy improved from 0.27281 to 0.30501, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 65ms/step - loss: 1.7125 - accuracy: 0.5073 - val_loss: 3.2522 - val_accuracy: 0.3050\n",
            "Epoch 33/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7012 - accuracy: 0.5158\n",
            "Epoch 33: val_accuracy did not improve from 0.30501\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.7005 - accuracy: 0.5163 - val_loss: 3.0037 - val_accuracy: 0.2406\n",
            "Epoch 34/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6667 - accuracy: 0.5277\n",
            "Epoch 34: val_accuracy did not improve from 0.30501\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.6675 - accuracy: 0.5272 - val_loss: 3.5344 - val_accuracy: 0.2683\n",
            "Epoch 35/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6482 - accuracy: 0.5225\n",
            "Epoch 35: val_accuracy did not improve from 0.30501\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.6485 - accuracy: 0.5224 - val_loss: 3.1802 - val_accuracy: 0.2567\n",
            "Epoch 36/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6453 - accuracy: 0.5236\n",
            "Epoch 36: val_accuracy improved from 0.30501 to 0.31082, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 72ms/step - loss: 1.6453 - accuracy: 0.5230 - val_loss: 3.6553 - val_accuracy: 0.3108\n",
            "Epoch 37/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6207 - accuracy: 0.5340\n",
            "Epoch 37: val_accuracy did not improve from 0.31082\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.6190 - accuracy: 0.5351 - val_loss: 3.8203 - val_accuracy: 0.1896\n",
            "Epoch 38/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5952 - accuracy: 0.5475\n",
            "Epoch 38: val_accuracy did not improve from 0.31082\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.5944 - accuracy: 0.5483 - val_loss: 5.0172 - val_accuracy: 0.1704\n",
            "Epoch 39/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.5823 - accuracy: 0.5518\n",
            "Epoch 39: val_accuracy did not improve from 0.31082\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5823 - accuracy: 0.5518 - val_loss: 3.2189 - val_accuracy: 0.2317\n",
            "Epoch 40/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5566 - accuracy: 0.5615\n",
            "Epoch 40: val_accuracy did not improve from 0.31082\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.5589 - accuracy: 0.5612 - val_loss: 3.4632 - val_accuracy: 0.2545\n",
            "Epoch 41/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5389 - accuracy: 0.5701\n",
            "Epoch 41: val_accuracy did not improve from 0.31082\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5414 - accuracy: 0.5694 - val_loss: 5.2871 - val_accuracy: 0.1708\n",
            "Epoch 42/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5419 - accuracy: 0.5699\n",
            "Epoch 42: val_accuracy did not improve from 0.31082\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5412 - accuracy: 0.5694 - val_loss: 2.9632 - val_accuracy: 0.3032\n",
            "Epoch 43/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5073 - accuracy: 0.5846\n",
            "Epoch 43: val_accuracy did not improve from 0.31082\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5076 - accuracy: 0.5840 - val_loss: 3.5865 - val_accuracy: 0.1869\n",
            "Epoch 44/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4707 - accuracy: 0.6078\n",
            "Epoch 44: val_accuracy improved from 0.31082 to 0.34258, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 66ms/step - loss: 1.4731 - accuracy: 0.6068 - val_loss: 3.5915 - val_accuracy: 0.3426\n",
            "Epoch 45/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4551 - accuracy: 0.6057\n",
            "Epoch 45: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.4568 - accuracy: 0.6051 - val_loss: 3.6542 - val_accuracy: 0.1758\n",
            "Epoch 46/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4505 - accuracy: 0.6127\n",
            "Epoch 46: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.4487 - accuracy: 0.6129 - val_loss: 4.4055 - val_accuracy: 0.2683\n",
            "Epoch 47/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.4196 - accuracy: 0.6307\n",
            "Epoch 47: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.4199 - accuracy: 0.6300 - val_loss: 4.5944 - val_accuracy: 0.1749\n",
            "Epoch 48/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3936 - accuracy: 0.6428\n",
            "Epoch 48: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.3962 - accuracy: 0.6413 - val_loss: 3.6891 - val_accuracy: 0.1722\n",
            "Epoch 49/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.3759 - accuracy: 0.6568\n",
            "Epoch 49: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.3782 - accuracy: 0.6555 - val_loss: 3.7383 - val_accuracy: 0.2818\n",
            "Epoch 50/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.3370 - accuracy: 0.6693\n",
            "Epoch 50: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.3370 - accuracy: 0.6693 - val_loss: 3.5270 - val_accuracy: 0.1972\n",
            "reg_param=0.001, dr=0.25, test_acc=0.1972\n",
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_88 (Conv1D)          (None, 301, 128)          512       \n",
            "                                                                 \n",
            " batch_normalization_147 (Ba  (None, 301, 128)         512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_147 (Activation)  (None, 301, 128)         0         \n",
            "                                                                 \n",
            " max_pooling1d_88 (MaxPoolin  (None, 150, 128)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_89 (Conv1D)          (None, 148, 256)          98560     \n",
            "                                                                 \n",
            " batch_normalization_148 (Ba  (None, 148, 256)         1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_148 (Activation)  (None, 148, 256)         0         \n",
            "                                                                 \n",
            " max_pooling1d_89 (MaxPoolin  (None, 74, 256)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_90 (Conv1D)          (None, 72, 512)           393728    \n",
            "                                                                 \n",
            " batch_normalization_149 (Ba  (None, 72, 512)          2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_149 (Activation)  (None, 72, 512)          0         \n",
            "                                                                 \n",
            " max_pooling1d_90 (MaxPoolin  (None, 36, 512)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_91 (Conv1D)          (None, 34, 1024)          1573888   \n",
            "                                                                 \n",
            " batch_normalization_150 (Ba  (None, 34, 1024)         4096      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_150 (Activation)  (None, 34, 1024)         0         \n",
            "                                                                 \n",
            " max_pooling1d_91 (MaxPoolin  (None, 17, 1024)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_22 (Flatten)        (None, 17408)             0         \n",
            "                                                                 \n",
            " dense_88 (Dense)            (None, 512)               8913408   \n",
            "                                                                 \n",
            " batch_normalization_151 (Ba  (None, 512)              2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_151 (Activation)  (None, 512)              0         \n",
            "                                                                 \n",
            " dropout_66 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_89 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " batch_normalization_152 (Ba  (None, 256)              1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_152 (Activation)  (None, 256)              0         \n",
            "                                                                 \n",
            " dropout_67 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_90 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " batch_normalization_153 (Ba  (None, 128)              512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_153 (Activation)  (None, 128)              0         \n",
            "                                                                 \n",
            " dropout_68 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_91 (Dense)            (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,156,358\n",
            "Trainable params: 11,150,726\n",
            "Non-trainable params: 5,632\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 4.0446 - accuracy: 0.3292\n",
            "Epoch 1: val_accuracy improved from -inf to 0.17084, saving model to best_model.h5\n",
            "41/41 [==============================] - 6s 80ms/step - loss: 4.0446 - accuracy: 0.3292 - val_loss: 4.1589 - val_accuracy: 0.1708\n",
            "Epoch 2/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 3.5181 - accuracy: 0.3633\n",
            "Epoch 2: val_accuracy did not improve from 0.17084\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 3.5153 - accuracy: 0.3631 - val_loss: 7.7356 - val_accuracy: 0.1704\n",
            "Epoch 3/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 3.2343 - accuracy: 0.3854\n",
            "Epoch 3: val_accuracy did not improve from 0.17084\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 3.2317 - accuracy: 0.3857 - val_loss: 8.9206 - val_accuracy: 0.1704\n",
            "Epoch 4/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 3.0281 - accuracy: 0.3965\n",
            "Epoch 4: val_accuracy did not improve from 0.17084\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 3.0287 - accuracy: 0.3961 - val_loss: 8.8055 - val_accuracy: 0.1704\n",
            "Epoch 5/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.8736 - accuracy: 0.3986\n",
            "Epoch 5: val_accuracy did not improve from 0.17084\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.8753 - accuracy: 0.3974 - val_loss: 8.3915 - val_accuracy: 0.1704\n",
            "Epoch 6/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.7415 - accuracy: 0.4041\n",
            "Epoch 6: val_accuracy improved from 0.17084 to 0.21869, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 68ms/step - loss: 2.7438 - accuracy: 0.4039 - val_loss: 7.4500 - val_accuracy: 0.2187\n",
            "Epoch 7/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.6278 - accuracy: 0.4178\n",
            "Epoch 7: val_accuracy improved from 0.21869 to 0.22764, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 69ms/step - loss: 2.6264 - accuracy: 0.4176 - val_loss: 6.3233 - val_accuracy: 0.2276\n",
            "Epoch 8/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.5192 - accuracy: 0.4184\n",
            "Epoch 8: val_accuracy improved from 0.22764 to 0.24553, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 67ms/step - loss: 2.5194 - accuracy: 0.4183 - val_loss: 6.2208 - val_accuracy: 0.2455\n",
            "Epoch 9/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.4450 - accuracy: 0.4095\n",
            "Epoch 9: val_accuracy did not improve from 0.24553\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 2.4450 - accuracy: 0.4095 - val_loss: 4.5419 - val_accuracy: 0.1713\n",
            "Epoch 10/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.3816 - accuracy: 0.4216\n",
            "Epoch 10: val_accuracy improved from 0.24553 to 0.26073, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 69ms/step - loss: 2.3816 - accuracy: 0.4216 - val_loss: 4.4340 - val_accuracy: 0.2607\n",
            "Epoch 11/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.3203 - accuracy: 0.4211\n",
            "Epoch 11: val_accuracy did not improve from 0.26073\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.3201 - accuracy: 0.4201 - val_loss: 4.7151 - val_accuracy: 0.1704\n",
            "Epoch 12/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.2541 - accuracy: 0.4307\n",
            "Epoch 12: val_accuracy did not improve from 0.26073\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.2552 - accuracy: 0.4298 - val_loss: 4.0510 - val_accuracy: 0.1789\n",
            "Epoch 13/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.1967 - accuracy: 0.4318\n",
            "Epoch 13: val_accuracy did not improve from 0.26073\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.1978 - accuracy: 0.4323 - val_loss: 4.3560 - val_accuracy: 0.1708\n",
            "Epoch 14/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.1682 - accuracy: 0.4239\n",
            "Epoch 14: val_accuracy did not improve from 0.26073\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.1682 - accuracy: 0.4239 - val_loss: 4.8394 - val_accuracy: 0.1704\n",
            "Epoch 15/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.1092 - accuracy: 0.4428\n",
            "Epoch 15: val_accuracy did not improve from 0.26073\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 2.1084 - accuracy: 0.4425 - val_loss: 4.3171 - val_accuracy: 0.1789\n",
            "Epoch 16/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.0772 - accuracy: 0.4441\n",
            "Epoch 16: val_accuracy improved from 0.26073 to 0.27773, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 69ms/step - loss: 2.0746 - accuracy: 0.4450 - val_loss: 3.4927 - val_accuracy: 0.2777\n",
            "Epoch 17/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.0385 - accuracy: 0.4418\n",
            "Epoch 17: val_accuracy did not improve from 0.27773\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 2.0416 - accuracy: 0.4408 - val_loss: 3.8175 - val_accuracy: 0.1950\n",
            "Epoch 18/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.0145 - accuracy: 0.4423\n",
            "Epoch 18: val_accuracy improved from 0.27773 to 0.28309, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 69ms/step - loss: 2.0145 - accuracy: 0.4423 - val_loss: 3.2649 - val_accuracy: 0.2831\n",
            "Epoch 19/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.9849 - accuracy: 0.4428\n",
            "Epoch 19: val_accuracy did not improve from 0.28309\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.9836 - accuracy: 0.4431 - val_loss: 4.1793 - val_accuracy: 0.1717\n",
            "Epoch 20/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.9478 - accuracy: 0.4520\n",
            "Epoch 20: val_accuracy did not improve from 0.28309\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.9484 - accuracy: 0.4505 - val_loss: 4.3535 - val_accuracy: 0.1798\n",
            "Epoch 21/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.9280 - accuracy: 0.4492\n",
            "Epoch 21: val_accuracy did not improve from 0.28309\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.9240 - accuracy: 0.4517 - val_loss: 3.1522 - val_accuracy: 0.2223\n",
            "Epoch 22/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.9021 - accuracy: 0.4502\n",
            "Epoch 22: val_accuracy did not improve from 0.28309\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.9036 - accuracy: 0.4500 - val_loss: 3.4205 - val_accuracy: 0.2710\n",
            "Epoch 23/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8789 - accuracy: 0.4545\n",
            "Epoch 23: val_accuracy did not improve from 0.28309\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.8773 - accuracy: 0.4555 - val_loss: 3.6718 - val_accuracy: 0.2089\n",
            "Epoch 24/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8592 - accuracy: 0.4584\n",
            "Epoch 24: val_accuracy did not improve from 0.28309\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.8601 - accuracy: 0.4576 - val_loss: 4.0049 - val_accuracy: 0.1708\n",
            "Epoch 25/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8507 - accuracy: 0.4539\n",
            "Epoch 25: val_accuracy did not improve from 0.28309\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.8486 - accuracy: 0.4546 - val_loss: 3.6885 - val_accuracy: 0.2330\n",
            "Epoch 26/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8226 - accuracy: 0.4576\n",
            "Epoch 26: val_accuracy did not improve from 0.28309\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.8231 - accuracy: 0.4578 - val_loss: 4.1074 - val_accuracy: 0.1753\n",
            "Epoch 27/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8088 - accuracy: 0.4631\n",
            "Epoch 27: val_accuracy improved from 0.28309 to 0.31619, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 68ms/step - loss: 1.8077 - accuracy: 0.4645 - val_loss: 3.1552 - val_accuracy: 0.3162\n",
            "Epoch 28/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.7905 - accuracy: 0.4618\n",
            "Epoch 28: val_accuracy improved from 0.31619 to 0.31977, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 69ms/step - loss: 1.7905 - accuracy: 0.4618 - val_loss: 2.9307 - val_accuracy: 0.3198\n",
            "Epoch 29/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.7721 - accuracy: 0.4678\n",
            "Epoch 29: val_accuracy did not improve from 0.31977\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.7721 - accuracy: 0.4678 - val_loss: 3.2145 - val_accuracy: 0.2008\n",
            "Epoch 30/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7543 - accuracy: 0.4756\n",
            "Epoch 30: val_accuracy did not improve from 0.31977\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.7555 - accuracy: 0.4745 - val_loss: 4.2098 - val_accuracy: 0.1708\n",
            "Epoch 31/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7452 - accuracy: 0.4754\n",
            "Epoch 31: val_accuracy did not improve from 0.31977\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.7451 - accuracy: 0.4760 - val_loss: 2.7001 - val_accuracy: 0.2831\n",
            "Epoch 32/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.7285 - accuracy: 0.4778\n",
            "Epoch 32: val_accuracy improved from 0.31977 to 0.33050, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 70ms/step - loss: 1.7285 - accuracy: 0.4778 - val_loss: 2.6892 - val_accuracy: 0.3305\n",
            "Epoch 33/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6894 - accuracy: 0.4828\n",
            "Epoch 33: val_accuracy did not improve from 0.33050\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.6894 - accuracy: 0.4837 - val_loss: 2.8479 - val_accuracy: 0.2889\n",
            "Epoch 34/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6944 - accuracy: 0.4797\n",
            "Epoch 34: val_accuracy did not improve from 0.33050\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.6923 - accuracy: 0.4803 - val_loss: 3.3716 - val_accuracy: 0.3207\n",
            "Epoch 35/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6811 - accuracy: 0.4836\n",
            "Epoch 35: val_accuracy did not improve from 0.33050\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.6807 - accuracy: 0.4835 - val_loss: 3.8893 - val_accuracy: 0.1847\n",
            "Epoch 36/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6588 - accuracy: 0.4916\n",
            "Epoch 36: val_accuracy did not improve from 0.33050\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.6596 - accuracy: 0.4916 - val_loss: 2.8784 - val_accuracy: 0.3140\n",
            "Epoch 37/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6510 - accuracy: 0.4904\n",
            "Epoch 37: val_accuracy did not improve from 0.33050\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.6526 - accuracy: 0.4900 - val_loss: 3.0452 - val_accuracy: 0.2679\n",
            "Epoch 38/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6491 - accuracy: 0.4965\n",
            "Epoch 38: val_accuracy did not improve from 0.33050\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.6498 - accuracy: 0.4973 - val_loss: 4.4833 - val_accuracy: 0.1708\n",
            "Epoch 39/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6478 - accuracy: 0.4986\n",
            "Epoch 39: val_accuracy did not improve from 0.33050\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.6469 - accuracy: 0.4998 - val_loss: 5.2846 - val_accuracy: 0.1708\n",
            "Epoch 40/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.6276 - accuracy: 0.5019\n",
            "Epoch 40: val_accuracy did not improve from 0.33050\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.6276 - accuracy: 0.5019 - val_loss: 4.3726 - val_accuracy: 0.2021\n",
            "Epoch 41/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.6368 - accuracy: 0.4921\n",
            "Epoch 41: val_accuracy did not improve from 0.33050\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.6368 - accuracy: 0.4921 - val_loss: 3.7245 - val_accuracy: 0.3113\n",
            "Epoch 42/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6097 - accuracy: 0.5064\n",
            "Epoch 42: val_accuracy did not improve from 0.33050\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.6104 - accuracy: 0.5071 - val_loss: 3.2454 - val_accuracy: 0.2831\n",
            "Epoch 43/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5983 - accuracy: 0.5137\n",
            "Epoch 43: val_accuracy did not improve from 0.33050\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5994 - accuracy: 0.5125 - val_loss: 3.9108 - val_accuracy: 0.1896\n",
            "Epoch 44/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.5773 - accuracy: 0.5255\n",
            "Epoch 44: val_accuracy did not improve from 0.33050\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.5773 - accuracy: 0.5255 - val_loss: 3.3868 - val_accuracy: 0.3037\n",
            "Epoch 45/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5765 - accuracy: 0.5248\n",
            "Epoch 45: val_accuracy improved from 0.33050 to 0.33587, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 67ms/step - loss: 1.5792 - accuracy: 0.5242 - val_loss: 2.7037 - val_accuracy: 0.3359\n",
            "Epoch 46/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5665 - accuracy: 0.5273\n",
            "Epoch 46: val_accuracy improved from 0.33587 to 0.34258, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 67ms/step - loss: 1.5666 - accuracy: 0.5278 - val_loss: 2.9242 - val_accuracy: 0.3426\n",
            "Epoch 47/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5509 - accuracy: 0.5322\n",
            "Epoch 47: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.5520 - accuracy: 0.5316 - val_loss: 3.7862 - val_accuracy: 0.2585\n",
            "Epoch 48/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5390 - accuracy: 0.5424\n",
            "Epoch 48: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5381 - accuracy: 0.5428 - val_loss: 4.6695 - val_accuracy: 0.2603\n",
            "Epoch 49/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5298 - accuracy: 0.5463\n",
            "Epoch 49: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.5321 - accuracy: 0.5454 - val_loss: 4.0888 - val_accuracy: 0.2281\n",
            "Epoch 50/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5239 - accuracy: 0.5631\n",
            "Epoch 50: val_accuracy did not improve from 0.34258\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5238 - accuracy: 0.5627 - val_loss: 4.2446 - val_accuracy: 0.2214\n",
            "reg_param=0.001, dr=0.3, test_acc=0.2214\n",
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_92 (Conv1D)          (None, 301, 128)          512       \n",
            "                                                                 \n",
            " batch_normalization_154 (Ba  (None, 301, 128)         512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_154 (Activation)  (None, 301, 128)         0         \n",
            "                                                                 \n",
            " max_pooling1d_92 (MaxPoolin  (None, 150, 128)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_93 (Conv1D)          (None, 148, 256)          98560     \n",
            "                                                                 \n",
            " batch_normalization_155 (Ba  (None, 148, 256)         1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_155 (Activation)  (None, 148, 256)         0         \n",
            "                                                                 \n",
            " max_pooling1d_93 (MaxPoolin  (None, 74, 256)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_94 (Conv1D)          (None, 72, 512)           393728    \n",
            "                                                                 \n",
            " batch_normalization_156 (Ba  (None, 72, 512)          2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_156 (Activation)  (None, 72, 512)          0         \n",
            "                                                                 \n",
            " max_pooling1d_94 (MaxPoolin  (None, 36, 512)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_95 (Conv1D)          (None, 34, 1024)          1573888   \n",
            "                                                                 \n",
            " batch_normalization_157 (Ba  (None, 34, 1024)         4096      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_157 (Activation)  (None, 34, 1024)         0         \n",
            "                                                                 \n",
            " max_pooling1d_95 (MaxPoolin  (None, 17, 1024)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_23 (Flatten)        (None, 17408)             0         \n",
            "                                                                 \n",
            " dense_92 (Dense)            (None, 512)               8913408   \n",
            "                                                                 \n",
            " batch_normalization_158 (Ba  (None, 512)              2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_158 (Activation)  (None, 512)              0         \n",
            "                                                                 \n",
            " dropout_69 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_93 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " batch_normalization_159 (Ba  (None, 256)              1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_159 (Activation)  (None, 256)              0         \n",
            "                                                                 \n",
            " dropout_70 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_94 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " batch_normalization_160 (Ba  (None, 128)              512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_160 (Activation)  (None, 128)              0         \n",
            "                                                                 \n",
            " dropout_71 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_95 (Dense)            (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,156,358\n",
            "Trainable params: 11,150,726\n",
            "Non-trainable params: 5,632\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 4.1863 - accuracy: 0.3021\n",
            "Epoch 1: val_accuracy improved from -inf to 0.17621, saving model to best_model.h5\n",
            "41/41 [==============================] - 6s 81ms/step - loss: 4.1788 - accuracy: 0.3031 - val_loss: 4.3123 - val_accuracy: 0.1762\n",
            "Epoch 2/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 3.6297 - accuracy: 0.3445\n",
            "Epoch 2: val_accuracy did not improve from 0.17621\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 3.6297 - accuracy: 0.3445 - val_loss: 7.7856 - val_accuracy: 0.1704\n",
            "Epoch 3/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 3.3284 - accuracy: 0.3658\n",
            "Epoch 3: val_accuracy did not improve from 0.17621\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 3.3267 - accuracy: 0.3652 - val_loss: 10.9348 - val_accuracy: 0.1704\n",
            "Epoch 4/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 3.1022 - accuracy: 0.3688\n",
            "Epoch 4: val_accuracy did not improve from 0.17621\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 3.1005 - accuracy: 0.3691 - val_loss: 11.6749 - val_accuracy: 0.1704\n",
            "Epoch 5/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.9490 - accuracy: 0.3762\n",
            "Epoch 5: val_accuracy did not improve from 0.17621\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 2.9476 - accuracy: 0.3769 - val_loss: 11.6483 - val_accuracy: 0.1704\n",
            "Epoch 6/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.8249 - accuracy: 0.3801\n",
            "Epoch 6: val_accuracy did not improve from 0.17621\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.8247 - accuracy: 0.3794 - val_loss: 9.9517 - val_accuracy: 0.1704\n",
            "Epoch 7/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.6916 - accuracy: 0.3936\n",
            "Epoch 7: val_accuracy did not improve from 0.17621\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.6916 - accuracy: 0.3936 - val_loss: 7.7484 - val_accuracy: 0.1704\n",
            "Epoch 8/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.6090 - accuracy: 0.3914\n",
            "Epoch 8: val_accuracy did not improve from 0.17621\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.6069 - accuracy: 0.3923 - val_loss: 6.4362 - val_accuracy: 0.1704\n",
            "Epoch 9/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.5213 - accuracy: 0.3943\n",
            "Epoch 9: val_accuracy did not improve from 0.17621\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 2.5211 - accuracy: 0.3946 - val_loss: 5.6486 - val_accuracy: 0.1740\n",
            "Epoch 10/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.4526 - accuracy: 0.3913\n",
            "Epoch 10: val_accuracy improved from 0.17621 to 0.19007, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 70ms/step - loss: 2.4526 - accuracy: 0.3913 - val_loss: 4.9686 - val_accuracy: 0.1901\n",
            "Epoch 11/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.3873 - accuracy: 0.4029\n",
            "Epoch 11: val_accuracy improved from 0.19007 to 0.20483, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 71ms/step - loss: 2.3874 - accuracy: 0.4028 - val_loss: 4.0997 - val_accuracy: 0.2048\n",
            "Epoch 12/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.3276 - accuracy: 0.4105\n",
            "Epoch 12: val_accuracy improved from 0.20483 to 0.21556, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 68ms/step - loss: 2.3240 - accuracy: 0.4112 - val_loss: 3.7636 - val_accuracy: 0.2156\n",
            "Epoch 13/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.2796 - accuracy: 0.4184\n",
            "Epoch 13: val_accuracy did not improve from 0.21556\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.2767 - accuracy: 0.4179 - val_loss: 3.7216 - val_accuracy: 0.1923\n",
            "Epoch 14/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.2335 - accuracy: 0.4119\n",
            "Epoch 14: val_accuracy did not improve from 0.21556\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 2.2342 - accuracy: 0.4110 - val_loss: 5.2370 - val_accuracy: 0.1708\n",
            "Epoch 15/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.1711 - accuracy: 0.4283\n",
            "Epoch 15: val_accuracy did not improve from 0.21556\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.1711 - accuracy: 0.4283 - val_loss: 5.3726 - val_accuracy: 0.1740\n",
            "Epoch 16/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.1470 - accuracy: 0.4168\n",
            "Epoch 16: val_accuracy did not improve from 0.21556\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.1470 - accuracy: 0.4168 - val_loss: 5.0046 - val_accuracy: 0.1708\n",
            "Epoch 17/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.1077 - accuracy: 0.4258\n",
            "Epoch 17: val_accuracy improved from 0.21556 to 0.25447, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 73ms/step - loss: 2.1068 - accuracy: 0.4262 - val_loss: 3.4109 - val_accuracy: 0.2545\n",
            "Epoch 18/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.0546 - accuracy: 0.4398\n",
            "Epoch 18: val_accuracy did not improve from 0.25447\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.0554 - accuracy: 0.4408 - val_loss: 5.5543 - val_accuracy: 0.1704\n",
            "Epoch 19/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.0467 - accuracy: 0.4240\n",
            "Epoch 19: val_accuracy did not improve from 0.25447\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.0466 - accuracy: 0.4243 - val_loss: 3.0955 - val_accuracy: 0.2509\n",
            "Epoch 20/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.0178 - accuracy: 0.4314\n",
            "Epoch 20: val_accuracy did not improve from 0.25447\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.0169 - accuracy: 0.4316 - val_loss: 2.8938 - val_accuracy: 0.2151\n",
            "Epoch 21/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.9926 - accuracy: 0.4275\n",
            "Epoch 21: val_accuracy improved from 0.25447 to 0.25671, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 67ms/step - loss: 1.9921 - accuracy: 0.4287 - val_loss: 2.5198 - val_accuracy: 0.2567\n",
            "Epoch 22/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.9682 - accuracy: 0.4410\n",
            "Epoch 22: val_accuracy did not improve from 0.25671\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.9675 - accuracy: 0.4410 - val_loss: 3.6809 - val_accuracy: 0.1874\n",
            "Epoch 23/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.9391 - accuracy: 0.4432\n",
            "Epoch 23: val_accuracy did not improve from 0.25671\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.9379 - accuracy: 0.4434 - val_loss: 3.1569 - val_accuracy: 0.2482\n",
            "Epoch 24/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.9137 - accuracy: 0.4383\n",
            "Epoch 24: val_accuracy did not improve from 0.25671\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.9137 - accuracy: 0.4383 - val_loss: 3.1057 - val_accuracy: 0.2294\n",
            "Epoch 25/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.9132 - accuracy: 0.4375\n",
            "Epoch 25: val_accuracy did not improve from 0.25671\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.9135 - accuracy: 0.4377 - val_loss: 4.7230 - val_accuracy: 0.1708\n",
            "Epoch 26/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8768 - accuracy: 0.4496\n",
            "Epoch 26: val_accuracy did not improve from 0.25671\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.8788 - accuracy: 0.4488 - val_loss: 4.5650 - val_accuracy: 0.1708\n",
            "Epoch 27/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8601 - accuracy: 0.4437\n",
            "Epoch 27: val_accuracy did not improve from 0.25671\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.8606 - accuracy: 0.4427 - val_loss: 3.3232 - val_accuracy: 0.1829\n",
            "Epoch 28/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8482 - accuracy: 0.4459\n",
            "Epoch 28: val_accuracy improved from 0.25671 to 0.28086, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 73ms/step - loss: 1.8465 - accuracy: 0.4465 - val_loss: 3.0148 - val_accuracy: 0.2809\n",
            "Epoch 29/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.8285 - accuracy: 0.4390\n",
            "Epoch 29: val_accuracy did not improve from 0.28086\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.8285 - accuracy: 0.4390 - val_loss: 4.6421 - val_accuracy: 0.1749\n",
            "Epoch 30/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.8039 - accuracy: 0.4544\n",
            "Epoch 30: val_accuracy did not improve from 0.28086\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.8039 - accuracy: 0.4544 - val_loss: 3.4218 - val_accuracy: 0.2285\n",
            "Epoch 31/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7930 - accuracy: 0.4543\n",
            "Epoch 31: val_accuracy did not improve from 0.28086\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.7926 - accuracy: 0.4546 - val_loss: 3.8107 - val_accuracy: 0.1740\n",
            "Epoch 32/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7764 - accuracy: 0.4523\n",
            "Epoch 32: val_accuracy did not improve from 0.28086\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.7749 - accuracy: 0.4530 - val_loss: 6.4361 - val_accuracy: 0.1704\n",
            "Epoch 33/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7556 - accuracy: 0.4570\n",
            "Epoch 33: val_accuracy did not improve from 0.28086\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.7557 - accuracy: 0.4572 - val_loss: 2.9463 - val_accuracy: 0.2379\n",
            "Epoch 34/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7651 - accuracy: 0.4566\n",
            "Epoch 34: val_accuracy did not improve from 0.28086\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.7656 - accuracy: 0.4571 - val_loss: 3.3446 - val_accuracy: 0.2746\n",
            "Epoch 35/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.7393 - accuracy: 0.4628\n",
            "Epoch 35: val_accuracy did not improve from 0.28086\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.7393 - accuracy: 0.4628 - val_loss: 3.6257 - val_accuracy: 0.2733\n",
            "Epoch 36/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7437 - accuracy: 0.4551\n",
            "Epoch 36: val_accuracy improved from 0.28086 to 0.29517, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 68ms/step - loss: 1.7428 - accuracy: 0.4551 - val_loss: 3.2678 - val_accuracy: 0.2952\n",
            "Epoch 37/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7156 - accuracy: 0.4670\n",
            "Epoch 37: val_accuracy did not improve from 0.29517\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.7189 - accuracy: 0.4663 - val_loss: 4.8963 - val_accuracy: 0.1708\n",
            "Epoch 38/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7018 - accuracy: 0.4676\n",
            "Epoch 38: val_accuracy did not improve from 0.29517\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.7018 - accuracy: 0.4674 - val_loss: 3.8253 - val_accuracy: 0.1708\n",
            "Epoch 39/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6909 - accuracy: 0.4693\n",
            "Epoch 39: val_accuracy did not improve from 0.29517\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.6950 - accuracy: 0.4672 - val_loss: 3.6782 - val_accuracy: 0.2097\n",
            "Epoch 40/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6822 - accuracy: 0.4736\n",
            "Epoch 40: val_accuracy did not improve from 0.29517\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.6821 - accuracy: 0.4730 - val_loss: 3.8099 - val_accuracy: 0.2156\n",
            "Epoch 41/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6580 - accuracy: 0.4756\n",
            "Epoch 41: val_accuracy did not improve from 0.29517\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.6571 - accuracy: 0.4766 - val_loss: 4.9138 - val_accuracy: 0.1708\n",
            "Epoch 42/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6595 - accuracy: 0.4701\n",
            "Epoch 42: val_accuracy did not improve from 0.29517\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.6616 - accuracy: 0.4688 - val_loss: 4.3773 - val_accuracy: 0.1717\n",
            "Epoch 43/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6634 - accuracy: 0.4721\n",
            "Epoch 43: val_accuracy did not improve from 0.29517\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.6640 - accuracy: 0.4722 - val_loss: 4.7413 - val_accuracy: 0.1708\n",
            "Epoch 44/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6508 - accuracy: 0.4771\n",
            "Epoch 44: val_accuracy improved from 0.29517 to 0.31485, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 70ms/step - loss: 1.6524 - accuracy: 0.4770 - val_loss: 2.9277 - val_accuracy: 0.3148\n",
            "Epoch 45/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6378 - accuracy: 0.4789\n",
            "Epoch 45: val_accuracy did not improve from 0.31485\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.6389 - accuracy: 0.4781 - val_loss: 3.3912 - val_accuracy: 0.2303\n",
            "Epoch 46/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6336 - accuracy: 0.4781\n",
            "Epoch 46: val_accuracy did not improve from 0.31485\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.6346 - accuracy: 0.4776 - val_loss: 3.4456 - val_accuracy: 0.3055\n",
            "Epoch 47/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6125 - accuracy: 0.4898\n",
            "Epoch 47: val_accuracy did not improve from 0.31485\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.6126 - accuracy: 0.4895 - val_loss: 4.8580 - val_accuracy: 0.2102\n",
            "Epoch 48/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5991 - accuracy: 0.4932\n",
            "Epoch 48: val_accuracy did not improve from 0.31485\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.6023 - accuracy: 0.4916 - val_loss: 3.7273 - val_accuracy: 0.2343\n",
            "Epoch 49/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5860 - accuracy: 0.5020\n",
            "Epoch 49: val_accuracy did not improve from 0.31485\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5880 - accuracy: 0.5015 - val_loss: 3.3617 - val_accuracy: 0.3108\n",
            "Epoch 50/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5817 - accuracy: 0.4920\n",
            "Epoch 50: val_accuracy improved from 0.31485 to 0.32558, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 73ms/step - loss: 1.5818 - accuracy: 0.4912 - val_loss: 3.2576 - val_accuracy: 0.3256\n",
            "reg_param=0.001, dr=0.4, test_acc=0.3256\n",
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_96 (Conv1D)          (None, 301, 128)          512       \n",
            "                                                                 \n",
            " batch_normalization_161 (Ba  (None, 301, 128)         512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_161 (Activation)  (None, 301, 128)         0         \n",
            "                                                                 \n",
            " max_pooling1d_96 (MaxPoolin  (None, 150, 128)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_97 (Conv1D)          (None, 148, 256)          98560     \n",
            "                                                                 \n",
            " batch_normalization_162 (Ba  (None, 148, 256)         1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_162 (Activation)  (None, 148, 256)         0         \n",
            "                                                                 \n",
            " max_pooling1d_97 (MaxPoolin  (None, 74, 256)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_98 (Conv1D)          (None, 72, 512)           393728    \n",
            "                                                                 \n",
            " batch_normalization_163 (Ba  (None, 72, 512)          2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_163 (Activation)  (None, 72, 512)          0         \n",
            "                                                                 \n",
            " max_pooling1d_98 (MaxPoolin  (None, 36, 512)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_99 (Conv1D)          (None, 34, 1024)          1573888   \n",
            "                                                                 \n",
            " batch_normalization_164 (Ba  (None, 34, 1024)         4096      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_164 (Activation)  (None, 34, 1024)         0         \n",
            "                                                                 \n",
            " max_pooling1d_99 (MaxPoolin  (None, 17, 1024)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_24 (Flatten)        (None, 17408)             0         \n",
            "                                                                 \n",
            " dense_96 (Dense)            (None, 512)               8913408   \n",
            "                                                                 \n",
            " batch_normalization_165 (Ba  (None, 512)              2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_165 (Activation)  (None, 512)              0         \n",
            "                                                                 \n",
            " dropout_72 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_97 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " batch_normalization_166 (Ba  (None, 256)              1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_166 (Activation)  (None, 256)              0         \n",
            "                                                                 \n",
            " dropout_73 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_98 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " batch_normalization_167 (Ba  (None, 128)              512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_167 (Activation)  (None, 128)              0         \n",
            "                                                                 \n",
            " dropout_74 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_99 (Dense)            (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,156,358\n",
            "Trainable params: 11,150,726\n",
            "Non-trainable params: 5,632\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 4.2663 - accuracy: 0.2607\n",
            "Epoch 1: val_accuracy improved from -inf to 0.20394, saving model to best_model.h5\n",
            "41/41 [==============================] - 6s 86ms/step - loss: 4.2565 - accuracy: 0.2628 - val_loss: 3.7521 - val_accuracy: 0.2039\n",
            "Epoch 2/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 3.6957 - accuracy: 0.3283\n",
            "Epoch 2: val_accuracy did not improve from 0.20394\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 3.6920 - accuracy: 0.3284 - val_loss: 8.0994 - val_accuracy: 0.1704\n",
            "Epoch 3/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 3.4137 - accuracy: 0.3359\n",
            "Epoch 3: val_accuracy did not improve from 0.20394\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 3.4153 - accuracy: 0.3357 - val_loss: 9.1786 - val_accuracy: 0.1704\n",
            "Epoch 4/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 3.2011 - accuracy: 0.3467\n",
            "Epoch 4: val_accuracy did not improve from 0.20394\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 3.2019 - accuracy: 0.3459 - val_loss: 10.0464 - val_accuracy: 0.1704\n",
            "Epoch 5/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 3.0095 - accuracy: 0.3576\n",
            "Epoch 5: val_accuracy did not improve from 0.20394\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 3.0065 - accuracy: 0.3593 - val_loss: 9.2151 - val_accuracy: 0.1704\n",
            "Epoch 6/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.8775 - accuracy: 0.3666\n",
            "Epoch 6: val_accuracy did not improve from 0.20394\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 2.8757 - accuracy: 0.3673 - val_loss: 9.1245 - val_accuracy: 0.1704\n",
            "Epoch 7/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.7463 - accuracy: 0.3678\n",
            "Epoch 7: val_accuracy did not improve from 0.20394\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 2.7459 - accuracy: 0.3671 - val_loss: 8.7892 - val_accuracy: 0.1704\n",
            "Epoch 8/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.6561 - accuracy: 0.3779\n",
            "Epoch 8: val_accuracy did not improve from 0.20394\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.6544 - accuracy: 0.3798 - val_loss: 7.0724 - val_accuracy: 0.1704\n",
            "Epoch 9/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.5755 - accuracy: 0.3878\n",
            "Epoch 9: val_accuracy did not improve from 0.20394\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.5755 - accuracy: 0.3878 - val_loss: 6.9061 - val_accuracy: 0.1704\n",
            "Epoch 10/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.4967 - accuracy: 0.3863\n",
            "Epoch 10: val_accuracy did not improve from 0.20394\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.4942 - accuracy: 0.3871 - val_loss: 6.9232 - val_accuracy: 0.1704\n",
            "Epoch 11/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.4371 - accuracy: 0.3959\n",
            "Epoch 11: val_accuracy did not improve from 0.20394\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.4371 - accuracy: 0.3959 - val_loss: 6.2841 - val_accuracy: 0.1708\n",
            "Epoch 12/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.3737 - accuracy: 0.3896\n",
            "Epoch 12: val_accuracy did not improve from 0.20394\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.3728 - accuracy: 0.3894 - val_loss: 3.7122 - val_accuracy: 0.1843\n",
            "Epoch 13/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.3292 - accuracy: 0.3942\n",
            "Epoch 13: val_accuracy did not improve from 0.20394\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 2.3292 - accuracy: 0.3942 - val_loss: 4.2238 - val_accuracy: 0.1708\n",
            "Epoch 14/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.2661 - accuracy: 0.4041\n",
            "Epoch 14: val_accuracy improved from 0.20394 to 0.26208, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 68ms/step - loss: 2.2655 - accuracy: 0.4045 - val_loss: 3.0619 - val_accuracy: 0.2621\n",
            "Epoch 15/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.2373 - accuracy: 0.4006\n",
            "Epoch 15: val_accuracy did not improve from 0.26208\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.2351 - accuracy: 0.4015 - val_loss: 4.1057 - val_accuracy: 0.1713\n",
            "Epoch 16/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.1962 - accuracy: 0.4043\n",
            "Epoch 16: val_accuracy did not improve from 0.26208\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 2.1956 - accuracy: 0.4053 - val_loss: 4.5421 - val_accuracy: 0.1731\n",
            "Epoch 17/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.1513 - accuracy: 0.4152\n",
            "Epoch 17: val_accuracy did not improve from 0.26208\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.1495 - accuracy: 0.4153 - val_loss: 4.4086 - val_accuracy: 0.1704\n",
            "Epoch 18/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.1253 - accuracy: 0.4135\n",
            "Epoch 18: val_accuracy did not improve from 0.26208\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 2.1253 - accuracy: 0.4135 - val_loss: 4.3916 - val_accuracy: 0.1704\n",
            "Epoch 19/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.1019 - accuracy: 0.4135\n",
            "Epoch 19: val_accuracy did not improve from 0.26208\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.1027 - accuracy: 0.4130 - val_loss: 2.6030 - val_accuracy: 0.2218\n",
            "Epoch 20/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.0717 - accuracy: 0.4119\n",
            "Epoch 20: val_accuracy did not improve from 0.26208\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 2.0729 - accuracy: 0.4120 - val_loss: 3.7805 - val_accuracy: 0.1704\n",
            "Epoch 21/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.0420 - accuracy: 0.4178\n",
            "Epoch 21: val_accuracy did not improve from 0.26208\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 2.0420 - accuracy: 0.4178 - val_loss: 2.9449 - val_accuracy: 0.2424\n",
            "Epoch 22/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.0102 - accuracy: 0.4246\n",
            "Epoch 22: val_accuracy did not improve from 0.26208\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 2.0109 - accuracy: 0.4250 - val_loss: 4.3122 - val_accuracy: 0.1708\n",
            "Epoch 23/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.9830 - accuracy: 0.4239\n",
            "Epoch 23: val_accuracy did not improve from 0.26208\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.9830 - accuracy: 0.4239 - val_loss: 3.7288 - val_accuracy: 0.1708\n",
            "Epoch 24/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.9589 - accuracy: 0.4329\n",
            "Epoch 24: val_accuracy did not improve from 0.26208\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.9589 - accuracy: 0.4329 - val_loss: 3.1235 - val_accuracy: 0.1905\n",
            "Epoch 25/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.9395 - accuracy: 0.4270\n",
            "Epoch 25: val_accuracy did not improve from 0.26208\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.9395 - accuracy: 0.4270 - val_loss: 3.6998 - val_accuracy: 0.1843\n",
            "Epoch 26/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.9240 - accuracy: 0.4309\n",
            "Epoch 26: val_accuracy did not improve from 0.26208\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.9248 - accuracy: 0.4314 - val_loss: 4.8197 - val_accuracy: 0.1708\n",
            "Epoch 27/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8991 - accuracy: 0.4352\n",
            "Epoch 27: val_accuracy did not improve from 0.26208\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.8994 - accuracy: 0.4352 - val_loss: 5.0368 - val_accuracy: 0.1704\n",
            "Epoch 28/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8885 - accuracy: 0.4330\n",
            "Epoch 28: val_accuracy did not improve from 0.26208\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.8889 - accuracy: 0.4327 - val_loss: 3.4854 - val_accuracy: 0.1708\n",
            "Epoch 29/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8648 - accuracy: 0.4318\n",
            "Epoch 29: val_accuracy improved from 0.26208 to 0.28354, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 72ms/step - loss: 1.8661 - accuracy: 0.4308 - val_loss: 2.8274 - val_accuracy: 0.2835\n",
            "Epoch 30/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.8572 - accuracy: 0.4398\n",
            "Epoch 30: val_accuracy did not improve from 0.28354\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.8572 - accuracy: 0.4398 - val_loss: 4.1669 - val_accuracy: 0.1708\n",
            "Epoch 31/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8335 - accuracy: 0.4457\n",
            "Epoch 31: val_accuracy did not improve from 0.28354\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.8336 - accuracy: 0.4450 - val_loss: 4.3430 - val_accuracy: 0.1704\n",
            "Epoch 32/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8199 - accuracy: 0.4412\n",
            "Epoch 32: val_accuracy did not improve from 0.28354\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.8216 - accuracy: 0.4406 - val_loss: 4.0980 - val_accuracy: 0.1704\n",
            "Epoch 33/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8172 - accuracy: 0.4363\n",
            "Epoch 33: val_accuracy did not improve from 0.28354\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.8171 - accuracy: 0.4362 - val_loss: 4.0067 - val_accuracy: 0.1704\n",
            "Epoch 34/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.8022 - accuracy: 0.4367\n",
            "Epoch 34: val_accuracy did not improve from 0.28354\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.8022 - accuracy: 0.4367 - val_loss: 3.7982 - val_accuracy: 0.1708\n",
            "Epoch 35/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7957 - accuracy: 0.4361\n",
            "Epoch 35: val_accuracy did not improve from 0.28354\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.7951 - accuracy: 0.4369 - val_loss: 5.2226 - val_accuracy: 0.1704\n",
            "Epoch 36/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7762 - accuracy: 0.4467\n",
            "Epoch 36: val_accuracy did not improve from 0.28354\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.7779 - accuracy: 0.4469 - val_loss: 4.6916 - val_accuracy: 0.1708\n",
            "Epoch 37/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7614 - accuracy: 0.4410\n",
            "Epoch 37: val_accuracy did not improve from 0.28354\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.7614 - accuracy: 0.4411 - val_loss: 3.1089 - val_accuracy: 0.1869\n",
            "Epoch 38/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.7470 - accuracy: 0.4417\n",
            "Epoch 38: val_accuracy did not improve from 0.28354\n",
            "41/41 [==============================] - 2s 61ms/step - loss: 1.7470 - accuracy: 0.4417 - val_loss: 4.6203 - val_accuracy: 0.1704\n",
            "Epoch 39/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7429 - accuracy: 0.4414\n",
            "Epoch 39: val_accuracy did not improve from 0.28354\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.7433 - accuracy: 0.4417 - val_loss: 5.4038 - val_accuracy: 0.1704\n",
            "Epoch 40/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7178 - accuracy: 0.4547\n",
            "Epoch 40: val_accuracy did not improve from 0.28354\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.7188 - accuracy: 0.4549 - val_loss: 4.9969 - val_accuracy: 0.1704\n",
            "Epoch 41/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7177 - accuracy: 0.4465\n",
            "Epoch 41: val_accuracy did not improve from 0.28354\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.7152 - accuracy: 0.4473 - val_loss: 3.8450 - val_accuracy: 0.1735\n",
            "Epoch 42/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7284 - accuracy: 0.4377\n",
            "Epoch 42: val_accuracy did not improve from 0.28354\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.7252 - accuracy: 0.4402 - val_loss: 3.7323 - val_accuracy: 0.2531\n",
            "Epoch 43/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7044 - accuracy: 0.4479\n",
            "Epoch 43: val_accuracy did not improve from 0.28354\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.7034 - accuracy: 0.4492 - val_loss: 5.4462 - val_accuracy: 0.1704\n",
            "Epoch 44/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6923 - accuracy: 0.4520\n",
            "Epoch 44: val_accuracy did not improve from 0.28354\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.6940 - accuracy: 0.4523 - val_loss: 3.8897 - val_accuracy: 0.2437\n",
            "Epoch 45/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6905 - accuracy: 0.4457\n",
            "Epoch 45: val_accuracy did not improve from 0.28354\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.6905 - accuracy: 0.4465 - val_loss: 6.3910 - val_accuracy: 0.1704\n",
            "Epoch 46/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6745 - accuracy: 0.4447\n",
            "Epoch 46: val_accuracy did not improve from 0.28354\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.6761 - accuracy: 0.4438 - val_loss: 4.8183 - val_accuracy: 0.1704\n",
            "Epoch 47/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6668 - accuracy: 0.4496\n",
            "Epoch 47: val_accuracy did not improve from 0.28354\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.6674 - accuracy: 0.4496 - val_loss: 4.2537 - val_accuracy: 0.1704\n",
            "Epoch 48/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6662 - accuracy: 0.4449\n",
            "Epoch 48: val_accuracy did not improve from 0.28354\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.6681 - accuracy: 0.4434 - val_loss: 5.5972 - val_accuracy: 0.1704\n",
            "Epoch 49/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6615 - accuracy: 0.4479\n",
            "Epoch 49: val_accuracy did not improve from 0.28354\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.6609 - accuracy: 0.4480 - val_loss: 3.6255 - val_accuracy: 0.2442\n",
            "Epoch 50/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.6579 - accuracy: 0.4477\n",
            "Epoch 50: val_accuracy improved from 0.28354 to 0.32737, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 70ms/step - loss: 1.6579 - accuracy: 0.4477 - val_loss: 3.2311 - val_accuracy: 0.3274\n",
            "reg_param=0.001, dr=0.5, test_acc=0.3274\n",
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_100 (Conv1D)         (None, 301, 128)          512       \n",
            "                                                                 \n",
            " batch_normalization_168 (Ba  (None, 301, 128)         512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_168 (Activation)  (None, 301, 128)         0         \n",
            "                                                                 \n",
            " max_pooling1d_100 (MaxPooli  (None, 150, 128)         0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_101 (Conv1D)         (None, 148, 256)          98560     \n",
            "                                                                 \n",
            " batch_normalization_169 (Ba  (None, 148, 256)         1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_169 (Activation)  (None, 148, 256)         0         \n",
            "                                                                 \n",
            " max_pooling1d_101 (MaxPooli  (None, 74, 256)          0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_102 (Conv1D)         (None, 72, 512)           393728    \n",
            "                                                                 \n",
            " batch_normalization_170 (Ba  (None, 72, 512)          2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_170 (Activation)  (None, 72, 512)          0         \n",
            "                                                                 \n",
            " max_pooling1d_102 (MaxPooli  (None, 36, 512)          0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_103 (Conv1D)         (None, 34, 1024)          1573888   \n",
            "                                                                 \n",
            " batch_normalization_171 (Ba  (None, 34, 1024)         4096      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_171 (Activation)  (None, 34, 1024)         0         \n",
            "                                                                 \n",
            " max_pooling1d_103 (MaxPooli  (None, 17, 1024)         0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " flatten_25 (Flatten)        (None, 17408)             0         \n",
            "                                                                 \n",
            " dense_100 (Dense)           (None, 512)               8913408   \n",
            "                                                                 \n",
            " batch_normalization_172 (Ba  (None, 512)              2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_172 (Activation)  (None, 512)              0         \n",
            "                                                                 \n",
            " dropout_75 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_101 (Dense)           (None, 256)               131328    \n",
            "                                                                 \n",
            " batch_normalization_173 (Ba  (None, 256)              1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_173 (Activation)  (None, 256)              0         \n",
            "                                                                 \n",
            " dropout_76 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_102 (Dense)           (None, 128)               32896     \n",
            "                                                                 \n",
            " batch_normalization_174 (Ba  (None, 128)              512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_174 (Activation)  (None, 128)              0         \n",
            "                                                                 \n",
            " dropout_77 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_103 (Dense)           (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,156,358\n",
            "Trainable params: 11,150,726\n",
            "Non-trainable params: 5,632\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 20.3630 - accuracy: 0.3664\n",
            "Epoch 1: val_accuracy improved from -inf to 0.17039, saving model to best_model.h5\n",
            "41/41 [==============================] - 6s 78ms/step - loss: 20.2548 - accuracy: 0.3666 - val_loss: 15.2350 - val_accuracy: 0.1704\n",
            "Epoch 2/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 11.4788 - accuracy: 0.4024\n",
            "Epoch 2: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 11.4788 - accuracy: 0.4024 - val_loss: 13.6654 - val_accuracy: 0.1704\n",
            "Epoch 3/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 8.0411 - accuracy: 0.4053\n",
            "Epoch 3: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 8.0227 - accuracy: 0.4043 - val_loss: 13.0142 - val_accuracy: 0.1704\n",
            "Epoch 4/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 6.2202 - accuracy: 0.4086\n",
            "Epoch 4: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 6.2082 - accuracy: 0.4095 - val_loss: 11.5236 - val_accuracy: 0.1704\n",
            "Epoch 5/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 5.0677 - accuracy: 0.4160\n",
            "Epoch 5: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 5.0677 - accuracy: 0.4160 - val_loss: 11.6821 - val_accuracy: 0.1704\n",
            "Epoch 6/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 4.2856 - accuracy: 0.4170\n",
            "Epoch 6: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 4.2790 - accuracy: 0.4181 - val_loss: 12.4710 - val_accuracy: 0.1704\n",
            "Epoch 7/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 3.6930 - accuracy: 0.4266\n",
            "Epoch 7: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 3.6930 - accuracy: 0.4266 - val_loss: 11.4150 - val_accuracy: 0.1704\n",
            "Epoch 8/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 3.2823 - accuracy: 0.4160\n",
            "Epoch 8: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 3.2781 - accuracy: 0.4170 - val_loss: 10.9483 - val_accuracy: 0.1704\n",
            "Epoch 9/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.9437 - accuracy: 0.4281\n",
            "Epoch 9: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 2.9437 - accuracy: 0.4281 - val_loss: 9.3572 - val_accuracy: 0.1704\n",
            "Epoch 10/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.7054 - accuracy: 0.4201\n",
            "Epoch 10: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 2.7038 - accuracy: 0.4204 - val_loss: 10.1567 - val_accuracy: 0.1704\n",
            "Epoch 11/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.4869 - accuracy: 0.4330\n",
            "Epoch 11: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 2.4877 - accuracy: 0.4308 - val_loss: 12.1788 - val_accuracy: 0.1704\n",
            "Epoch 12/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.3366 - accuracy: 0.4327\n",
            "Epoch 12: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 2.3366 - accuracy: 0.4327 - val_loss: 8.3548 - val_accuracy: 0.1704\n",
            "Epoch 13/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.1992 - accuracy: 0.4306\n",
            "Epoch 13: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 2.1992 - accuracy: 0.4306 - val_loss: 8.0734 - val_accuracy: 0.1704\n",
            "Epoch 14/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.0992 - accuracy: 0.4252\n",
            "Epoch 14: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.0967 - accuracy: 0.4270 - val_loss: 7.4801 - val_accuracy: 0.1704\n",
            "Epoch 15/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.0092 - accuracy: 0.4326\n",
            "Epoch 15: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.0076 - accuracy: 0.4335 - val_loss: 10.0933 - val_accuracy: 0.1704\n",
            "Epoch 16/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.9600 - accuracy: 0.4256\n",
            "Epoch 16: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.9595 - accuracy: 0.4256 - val_loss: 8.4292 - val_accuracy: 0.1704\n",
            "Epoch 17/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8881 - accuracy: 0.4264\n",
            "Epoch 17: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.8876 - accuracy: 0.4270 - val_loss: 10.3280 - val_accuracy: 0.1704\n",
            "Epoch 18/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8265 - accuracy: 0.4357\n",
            "Epoch 18: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.8281 - accuracy: 0.4352 - val_loss: 7.9399 - val_accuracy: 0.1704\n",
            "Epoch 19/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7866 - accuracy: 0.4373\n",
            "Epoch 19: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.7882 - accuracy: 0.4352 - val_loss: 7.8027 - val_accuracy: 0.1704\n",
            "Epoch 20/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7578 - accuracy: 0.4283\n",
            "Epoch 20: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.7596 - accuracy: 0.4275 - val_loss: 7.4580 - val_accuracy: 0.1704\n",
            "Epoch 21/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7329 - accuracy: 0.4301\n",
            "Epoch 21: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.7301 - accuracy: 0.4314 - val_loss: 10.5919 - val_accuracy: 0.1704\n",
            "Epoch 22/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.7208 - accuracy: 0.4298\n",
            "Epoch 22: val_accuracy improved from 0.17039 to 0.18560, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 69ms/step - loss: 1.7208 - accuracy: 0.4298 - val_loss: 9.1028 - val_accuracy: 0.1856\n",
            "Epoch 23/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6915 - accuracy: 0.4314\n",
            "Epoch 23: val_accuracy improved from 0.18560 to 0.22719, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 70ms/step - loss: 1.6898 - accuracy: 0.4319 - val_loss: 6.7264 - val_accuracy: 0.2272\n",
            "Epoch 24/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6769 - accuracy: 0.4248\n",
            "Epoch 24: val_accuracy did not improve from 0.22719\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.6773 - accuracy: 0.4247 - val_loss: 10.6047 - val_accuracy: 0.1704\n",
            "Epoch 25/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6549 - accuracy: 0.4379\n",
            "Epoch 25: val_accuracy improved from 0.22719 to 0.25134, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 74ms/step - loss: 1.6526 - accuracy: 0.4385 - val_loss: 5.0130 - val_accuracy: 0.2513\n",
            "Epoch 26/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.6327 - accuracy: 0.4287\n",
            "Epoch 26: val_accuracy did not improve from 0.25134\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.6327 - accuracy: 0.4287 - val_loss: 7.4728 - val_accuracy: 0.1704\n",
            "Epoch 27/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6288 - accuracy: 0.4383\n",
            "Epoch 27: val_accuracy did not improve from 0.25134\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.6291 - accuracy: 0.4371 - val_loss: 8.2315 - val_accuracy: 0.1704\n",
            "Epoch 28/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6018 - accuracy: 0.4328\n",
            "Epoch 28: val_accuracy did not improve from 0.25134\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.6033 - accuracy: 0.4325 - val_loss: 4.5892 - val_accuracy: 0.2455\n",
            "Epoch 29/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5963 - accuracy: 0.4346\n",
            "Epoch 29: val_accuracy improved from 0.25134 to 0.25894, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 70ms/step - loss: 1.5982 - accuracy: 0.4340 - val_loss: 7.3754 - val_accuracy: 0.2589\n",
            "Epoch 30/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.5895 - accuracy: 0.4392\n",
            "Epoch 30: val_accuracy did not improve from 0.25894\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.5895 - accuracy: 0.4392 - val_loss: 7.2249 - val_accuracy: 0.1704\n",
            "Epoch 31/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5909 - accuracy: 0.4318\n",
            "Epoch 31: val_accuracy did not improve from 0.25894\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5908 - accuracy: 0.4321 - val_loss: 11.8504 - val_accuracy: 0.1704\n",
            "Epoch 32/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.6123 - accuracy: 0.4319\n",
            "Epoch 32: val_accuracy did not improve from 0.25894\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.6123 - accuracy: 0.4319 - val_loss: 9.2717 - val_accuracy: 0.1704\n",
            "Epoch 33/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5784 - accuracy: 0.4322\n",
            "Epoch 33: val_accuracy did not improve from 0.25894\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5759 - accuracy: 0.4342 - val_loss: 9.7030 - val_accuracy: 0.1704\n",
            "Epoch 34/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5677 - accuracy: 0.4338\n",
            "Epoch 34: val_accuracy did not improve from 0.25894\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5668 - accuracy: 0.4346 - val_loss: 12.8439 - val_accuracy: 0.1704\n",
            "Epoch 35/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5742 - accuracy: 0.4313\n",
            "Epoch 35: val_accuracy did not improve from 0.25894\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.5733 - accuracy: 0.4319 - val_loss: 6.7208 - val_accuracy: 0.1735\n",
            "Epoch 36/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5592 - accuracy: 0.4408\n",
            "Epoch 36: val_accuracy did not improve from 0.25894\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5605 - accuracy: 0.4394 - val_loss: 8.5920 - val_accuracy: 0.2352\n",
            "Epoch 37/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.5475 - accuracy: 0.4402\n",
            "Epoch 37: val_accuracy did not improve from 0.25894\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5475 - accuracy: 0.4402 - val_loss: 10.7595 - val_accuracy: 0.1704\n",
            "Epoch 38/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5552 - accuracy: 0.4314\n",
            "Epoch 38: val_accuracy did not improve from 0.25894\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.5552 - accuracy: 0.4314 - val_loss: 9.1923 - val_accuracy: 0.1704\n",
            "Epoch 39/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5395 - accuracy: 0.4395\n",
            "Epoch 39: val_accuracy did not improve from 0.25894\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.5407 - accuracy: 0.4390 - val_loss: 9.4221 - val_accuracy: 0.1704\n",
            "Epoch 40/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5379 - accuracy: 0.4328\n",
            "Epoch 40: val_accuracy improved from 0.25894 to 0.26252, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 70ms/step - loss: 1.5375 - accuracy: 0.4333 - val_loss: 6.5358 - val_accuracy: 0.2625\n",
            "Epoch 41/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5425 - accuracy: 0.4367\n",
            "Epoch 41: val_accuracy improved from 0.26252 to 0.28667, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 72ms/step - loss: 1.5433 - accuracy: 0.4356 - val_loss: 4.2467 - val_accuracy: 0.2867\n",
            "Epoch 42/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5552 - accuracy: 0.4467\n",
            "Epoch 42: val_accuracy improved from 0.28667 to 0.31172, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 74ms/step - loss: 1.5550 - accuracy: 0.4475 - val_loss: 4.1828 - val_accuracy: 0.3117\n",
            "Epoch 43/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5412 - accuracy: 0.4441\n",
            "Epoch 43: val_accuracy did not improve from 0.31172\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.5416 - accuracy: 0.4442 - val_loss: 4.0088 - val_accuracy: 0.2670\n",
            "Epoch 44/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5359 - accuracy: 0.4371\n",
            "Epoch 44: val_accuracy did not improve from 0.31172\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5348 - accuracy: 0.4394 - val_loss: 8.2091 - val_accuracy: 0.1704\n",
            "Epoch 45/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5331 - accuracy: 0.4406\n",
            "Epoch 45: val_accuracy did not improve from 0.31172\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.5313 - accuracy: 0.4427 - val_loss: 6.5290 - val_accuracy: 0.1704\n",
            "Epoch 46/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5491 - accuracy: 0.4439\n",
            "Epoch 46: val_accuracy did not improve from 0.31172\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5488 - accuracy: 0.4433 - val_loss: 6.6271 - val_accuracy: 0.1767\n",
            "Epoch 47/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5191 - accuracy: 0.4486\n",
            "Epoch 47: val_accuracy did not improve from 0.31172\n",
            "41/41 [==============================] - 2s 61ms/step - loss: 1.5190 - accuracy: 0.4480 - val_loss: 7.4495 - val_accuracy: 0.1704\n",
            "Epoch 48/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5231 - accuracy: 0.4445\n",
            "Epoch 48: val_accuracy did not improve from 0.31172\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.5206 - accuracy: 0.4456 - val_loss: 8.3284 - val_accuracy: 0.1704\n",
            "Epoch 49/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5463 - accuracy: 0.4434\n",
            "Epoch 49: val_accuracy did not improve from 0.31172\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5457 - accuracy: 0.4429 - val_loss: 10.1211 - val_accuracy: 0.1726\n",
            "Epoch 50/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5386 - accuracy: 0.4498\n",
            "Epoch 50: val_accuracy did not improve from 0.31172\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5359 - accuracy: 0.4507 - val_loss: 5.0541 - val_accuracy: 0.1704\n",
            "reg_param=0.01, dr=0.1, test_acc=0.1704\n",
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_104 (Conv1D)         (None, 301, 128)          512       \n",
            "                                                                 \n",
            " batch_normalization_175 (Ba  (None, 301, 128)         512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_175 (Activation)  (None, 301, 128)         0         \n",
            "                                                                 \n",
            " max_pooling1d_104 (MaxPooli  (None, 150, 128)         0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_105 (Conv1D)         (None, 148, 256)          98560     \n",
            "                                                                 \n",
            " batch_normalization_176 (Ba  (None, 148, 256)         1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_176 (Activation)  (None, 148, 256)         0         \n",
            "                                                                 \n",
            " max_pooling1d_105 (MaxPooli  (None, 74, 256)          0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_106 (Conv1D)         (None, 72, 512)           393728    \n",
            "                                                                 \n",
            " batch_normalization_177 (Ba  (None, 72, 512)          2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_177 (Activation)  (None, 72, 512)          0         \n",
            "                                                                 \n",
            " max_pooling1d_106 (MaxPooli  (None, 36, 512)          0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_107 (Conv1D)         (None, 34, 1024)          1573888   \n",
            "                                                                 \n",
            " batch_normalization_178 (Ba  (None, 34, 1024)         4096      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_178 (Activation)  (None, 34, 1024)         0         \n",
            "                                                                 \n",
            " max_pooling1d_107 (MaxPooli  (None, 17, 1024)         0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " flatten_26 (Flatten)        (None, 17408)             0         \n",
            "                                                                 \n",
            " dense_104 (Dense)           (None, 512)               8913408   \n",
            "                                                                 \n",
            " batch_normalization_179 (Ba  (None, 512)              2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_179 (Activation)  (None, 512)              0         \n",
            "                                                                 \n",
            " dropout_78 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_105 (Dense)           (None, 256)               131328    \n",
            "                                                                 \n",
            " batch_normalization_180 (Ba  (None, 256)              1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_180 (Activation)  (None, 256)              0         \n",
            "                                                                 \n",
            " dropout_79 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_106 (Dense)           (None, 128)               32896     \n",
            "                                                                 \n",
            " batch_normalization_181 (Ba  (None, 128)              512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_181 (Activation)  (None, 128)              0         \n",
            "                                                                 \n",
            " dropout_80 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_107 (Dense)           (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,156,358\n",
            "Trainable params: 11,150,726\n",
            "Non-trainable params: 5,632\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 20.3032 - accuracy: 0.3443\n",
            "Epoch 1: val_accuracy improved from -inf to 0.17039, saving model to best_model.h5\n",
            "41/41 [==============================] - 6s 86ms/step - loss: 20.1961 - accuracy: 0.3441 - val_loss: 15.2292 - val_accuracy: 0.1704\n",
            "Epoch 2/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 11.5809 - accuracy: 0.3715\n",
            "Epoch 2: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 11.5809 - accuracy: 0.3715 - val_loss: 15.7197 - val_accuracy: 0.1704\n",
            "Epoch 3/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 8.1852 - accuracy: 0.3875\n",
            "Epoch 3: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 8.1641 - accuracy: 0.3878 - val_loss: 13.5983 - val_accuracy: 0.1704\n",
            "Epoch 4/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 6.3651 - accuracy: 0.3880\n",
            "Epoch 4: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 6.3651 - accuracy: 0.3880 - val_loss: 12.2068 - val_accuracy: 0.1704\n",
            "Epoch 5/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 5.2266 - accuracy: 0.4080\n",
            "Epoch 5: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 5.2192 - accuracy: 0.4068 - val_loss: 14.2065 - val_accuracy: 0.1704\n",
            "Epoch 6/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 4.4203 - accuracy: 0.4059\n",
            "Epoch 6: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 4.4145 - accuracy: 0.4053 - val_loss: 11.7984 - val_accuracy: 0.1704\n",
            "Epoch 7/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 3.8342 - accuracy: 0.4061\n",
            "Epoch 7: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 61ms/step - loss: 3.8289 - accuracy: 0.4070 - val_loss: 13.1368 - val_accuracy: 0.1704\n",
            "Epoch 8/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 3.3892 - accuracy: 0.4133\n",
            "Epoch 8: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 3.3890 - accuracy: 0.4124 - val_loss: 11.1365 - val_accuracy: 0.1704\n",
            "Epoch 9/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 3.0629 - accuracy: 0.4146\n",
            "Epoch 9: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 3.0590 - accuracy: 0.4135 - val_loss: 11.0174 - val_accuracy: 0.1704\n",
            "Epoch 10/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.7820 - accuracy: 0.4156\n",
            "Epoch 10: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 2.7782 - accuracy: 0.4160 - val_loss: 13.2241 - val_accuracy: 0.1704\n",
            "Epoch 11/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.5703 - accuracy: 0.4154\n",
            "Epoch 11: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 2.5690 - accuracy: 0.4153 - val_loss: 11.0183 - val_accuracy: 0.1704\n",
            "Epoch 12/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.4007 - accuracy: 0.4201\n",
            "Epoch 12: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.4017 - accuracy: 0.4201 - val_loss: 10.3729 - val_accuracy: 0.1704\n",
            "Epoch 13/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.2541 - accuracy: 0.4270\n",
            "Epoch 13: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.2559 - accuracy: 0.4245 - val_loss: 9.7297 - val_accuracy: 0.1704\n",
            "Epoch 14/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.1409 - accuracy: 0.4176\n",
            "Epoch 14: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.1427 - accuracy: 0.4176 - val_loss: 10.6263 - val_accuracy: 0.1704\n",
            "Epoch 15/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.0676 - accuracy: 0.4121\n",
            "Epoch 15: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 2.0659 - accuracy: 0.4128 - val_loss: 8.5307 - val_accuracy: 0.1704\n",
            "Epoch 16/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.9790 - accuracy: 0.4264\n",
            "Epoch 16: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.9841 - accuracy: 0.4239 - val_loss: 10.8226 - val_accuracy: 0.1704\n",
            "Epoch 17/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.9170 - accuracy: 0.4221\n",
            "Epoch 17: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.9177 - accuracy: 0.4214 - val_loss: 8.6773 - val_accuracy: 0.1704\n",
            "Epoch 18/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8591 - accuracy: 0.4334\n",
            "Epoch 18: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.8590 - accuracy: 0.4339 - val_loss: 10.7047 - val_accuracy: 0.1704\n",
            "Epoch 19/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8037 - accuracy: 0.4234\n",
            "Epoch 19: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.8036 - accuracy: 0.4220 - val_loss: 8.8344 - val_accuracy: 0.1704\n",
            "Epoch 20/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7735 - accuracy: 0.4223\n",
            "Epoch 20: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.7741 - accuracy: 0.4220 - val_loss: 7.3407 - val_accuracy: 0.1704\n",
            "Epoch 21/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.7477 - accuracy: 0.4287\n",
            "Epoch 21: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.7477 - accuracy: 0.4287 - val_loss: 7.6714 - val_accuracy: 0.1704\n",
            "Epoch 22/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7268 - accuracy: 0.4271\n",
            "Epoch 22: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.7271 - accuracy: 0.4266 - val_loss: 7.2624 - val_accuracy: 0.1704\n",
            "Epoch 23/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.7029 - accuracy: 0.4256\n",
            "Epoch 23: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.7029 - accuracy: 0.4256 - val_loss: 8.8110 - val_accuracy: 0.1704\n",
            "Epoch 24/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6968 - accuracy: 0.4174\n",
            "Epoch 24: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.6979 - accuracy: 0.4168 - val_loss: 6.6344 - val_accuracy: 0.1704\n",
            "Epoch 25/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.6727 - accuracy: 0.4371\n",
            "Epoch 25: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.6727 - accuracy: 0.4371 - val_loss: 7.3184 - val_accuracy: 0.1704\n",
            "Epoch 26/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6489 - accuracy: 0.4297\n",
            "Epoch 26: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.6553 - accuracy: 0.4275 - val_loss: 7.4226 - val_accuracy: 0.1704\n",
            "Epoch 27/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.6470 - accuracy: 0.4294\n",
            "Epoch 27: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.6470 - accuracy: 0.4294 - val_loss: 8.5906 - val_accuracy: 0.1704\n",
            "Epoch 28/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6276 - accuracy: 0.4307\n",
            "Epoch 28: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.6288 - accuracy: 0.4298 - val_loss: 8.3999 - val_accuracy: 0.1704\n",
            "Epoch 29/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6145 - accuracy: 0.4391\n",
            "Epoch 29: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.6151 - accuracy: 0.4408 - val_loss: 6.2173 - val_accuracy: 0.1704\n",
            "Epoch 30/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6149 - accuracy: 0.4244\n",
            "Epoch 30: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.6160 - accuracy: 0.4248 - val_loss: 8.0298 - val_accuracy: 0.1704\n",
            "Epoch 31/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.6058 - accuracy: 0.4360\n",
            "Epoch 31: val_accuracy improved from 0.17039 to 0.19633, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 69ms/step - loss: 1.6058 - accuracy: 0.4360 - val_loss: 4.8909 - val_accuracy: 0.1963\n",
            "Epoch 32/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6060 - accuracy: 0.4275\n",
            "Epoch 32: val_accuracy did not improve from 0.19633\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.6070 - accuracy: 0.4258 - val_loss: 5.1017 - val_accuracy: 0.1704\n",
            "Epoch 33/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5778 - accuracy: 0.4340\n",
            "Epoch 33: val_accuracy did not improve from 0.19633\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.5788 - accuracy: 0.4337 - val_loss: 11.2173 - val_accuracy: 0.1704\n",
            "Epoch 34/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5685 - accuracy: 0.4322\n",
            "Epoch 34: val_accuracy did not improve from 0.19633\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5687 - accuracy: 0.4327 - val_loss: 6.6684 - val_accuracy: 0.1704\n",
            "Epoch 35/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5624 - accuracy: 0.4340\n",
            "Epoch 35: val_accuracy did not improve from 0.19633\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5646 - accuracy: 0.4329 - val_loss: 8.8938 - val_accuracy: 0.1704\n",
            "Epoch 36/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5716 - accuracy: 0.4357\n",
            "Epoch 36: val_accuracy did not improve from 0.19633\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.5719 - accuracy: 0.4356 - val_loss: 7.5349 - val_accuracy: 0.1704\n",
            "Epoch 37/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5593 - accuracy: 0.4367\n",
            "Epoch 37: val_accuracy did not improve from 0.19633\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5587 - accuracy: 0.4379 - val_loss: 5.9445 - val_accuracy: 0.1704\n",
            "Epoch 38/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5637 - accuracy: 0.4404\n",
            "Epoch 38: val_accuracy did not improve from 0.19633\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5649 - accuracy: 0.4402 - val_loss: 5.8438 - val_accuracy: 0.1704\n",
            "Epoch 39/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.5717 - accuracy: 0.4392\n",
            "Epoch 39: val_accuracy did not improve from 0.19633\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5717 - accuracy: 0.4392 - val_loss: 7.3713 - val_accuracy: 0.1704\n",
            "Epoch 40/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.5633 - accuracy: 0.4408\n",
            "Epoch 40: val_accuracy did not improve from 0.19633\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5633 - accuracy: 0.4408 - val_loss: 9.4350 - val_accuracy: 0.1704\n",
            "Epoch 41/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5581 - accuracy: 0.4324\n",
            "Epoch 41: val_accuracy did not improve from 0.19633\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.5585 - accuracy: 0.4319 - val_loss: 5.2242 - val_accuracy: 0.1704\n",
            "Epoch 42/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5421 - accuracy: 0.4439\n",
            "Epoch 42: val_accuracy did not improve from 0.19633\n",
            "41/41 [==============================] - 2s 61ms/step - loss: 1.5426 - accuracy: 0.4425 - val_loss: 8.3717 - val_accuracy: 0.1704\n",
            "Epoch 43/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5650 - accuracy: 0.4369\n",
            "Epoch 43: val_accuracy did not improve from 0.19633\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5652 - accuracy: 0.4369 - val_loss: 7.6732 - val_accuracy: 0.1704\n",
            "Epoch 44/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5427 - accuracy: 0.4346\n",
            "Epoch 44: val_accuracy did not improve from 0.19633\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5431 - accuracy: 0.4337 - val_loss: 5.5046 - val_accuracy: 0.1704\n",
            "Epoch 45/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5606 - accuracy: 0.4301\n",
            "Epoch 45: val_accuracy improved from 0.19633 to 0.25089, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 71ms/step - loss: 1.5606 - accuracy: 0.4298 - val_loss: 4.5298 - val_accuracy: 0.2509\n",
            "Epoch 46/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5296 - accuracy: 0.4350\n",
            "Epoch 46: val_accuracy did not improve from 0.25089\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.5325 - accuracy: 0.4329 - val_loss: 4.8565 - val_accuracy: 0.1713\n",
            "Epoch 47/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5451 - accuracy: 0.4361\n",
            "Epoch 47: val_accuracy did not improve from 0.25089\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5428 - accuracy: 0.4367 - val_loss: 6.3708 - val_accuracy: 0.1704\n",
            "Epoch 48/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5296 - accuracy: 0.4441\n",
            "Epoch 48: val_accuracy did not improve from 0.25089\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.5317 - accuracy: 0.4431 - val_loss: 12.7626 - val_accuracy: 0.1704\n",
            "Epoch 49/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5258 - accuracy: 0.4443\n",
            "Epoch 49: val_accuracy did not improve from 0.25089\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5270 - accuracy: 0.4440 - val_loss: 6.8509 - val_accuracy: 0.1704\n",
            "Epoch 50/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5308 - accuracy: 0.4334\n",
            "Epoch 50: val_accuracy did not improve from 0.25089\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5334 - accuracy: 0.4327 - val_loss: 8.9154 - val_accuracy: 0.1704\n",
            "reg_param=0.01, dr=0.2, test_acc=0.1704\n",
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_108 (Conv1D)         (None, 301, 128)          512       \n",
            "                                                                 \n",
            " batch_normalization_182 (Ba  (None, 301, 128)         512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_182 (Activation)  (None, 301, 128)         0         \n",
            "                                                                 \n",
            " max_pooling1d_108 (MaxPooli  (None, 150, 128)         0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_109 (Conv1D)         (None, 148, 256)          98560     \n",
            "                                                                 \n",
            " batch_normalization_183 (Ba  (None, 148, 256)         1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_183 (Activation)  (None, 148, 256)         0         \n",
            "                                                                 \n",
            " max_pooling1d_109 (MaxPooli  (None, 74, 256)          0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_110 (Conv1D)         (None, 72, 512)           393728    \n",
            "                                                                 \n",
            " batch_normalization_184 (Ba  (None, 72, 512)          2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_184 (Activation)  (None, 72, 512)          0         \n",
            "                                                                 \n",
            " max_pooling1d_110 (MaxPooli  (None, 36, 512)          0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_111 (Conv1D)         (None, 34, 1024)          1573888   \n",
            "                                                                 \n",
            " batch_normalization_185 (Ba  (None, 34, 1024)         4096      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_185 (Activation)  (None, 34, 1024)         0         \n",
            "                                                                 \n",
            " max_pooling1d_111 (MaxPooli  (None, 17, 1024)         0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " flatten_27 (Flatten)        (None, 17408)             0         \n",
            "                                                                 \n",
            " dense_108 (Dense)           (None, 512)               8913408   \n",
            "                                                                 \n",
            " batch_normalization_186 (Ba  (None, 512)              2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_186 (Activation)  (None, 512)              0         \n",
            "                                                                 \n",
            " dropout_81 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_109 (Dense)           (None, 256)               131328    \n",
            "                                                                 \n",
            " batch_normalization_187 (Ba  (None, 256)              1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_187 (Activation)  (None, 256)              0         \n",
            "                                                                 \n",
            " dropout_82 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_110 (Dense)           (None, 128)               32896     \n",
            "                                                                 \n",
            " batch_normalization_188 (Ba  (None, 128)              512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_188 (Activation)  (None, 128)              0         \n",
            "                                                                 \n",
            " dropout_83 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_111 (Dense)           (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,156,358\n",
            "Trainable params: 11,150,726\n",
            "Non-trainable params: 5,632\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 20.7298 - accuracy: 0.3381\n",
            "Epoch 1: val_accuracy improved from -inf to 0.17039, saving model to best_model.h5\n",
            "41/41 [==============================] - 6s 80ms/step - loss: 20.6251 - accuracy: 0.3393 - val_loss: 15.8659 - val_accuracy: 0.1704\n",
            "Epoch 2/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 12.2192 - accuracy: 0.3656\n",
            "Epoch 2: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 12.1799 - accuracy: 0.3666 - val_loss: 15.5052 - val_accuracy: 0.1704\n",
            "Epoch 3/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 8.7089 - accuracy: 0.3822\n",
            "Epoch 3: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 8.6914 - accuracy: 0.3809 - val_loss: 14.7171 - val_accuracy: 0.1704\n",
            "Epoch 4/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 6.7990 - accuracy: 0.3918\n",
            "Epoch 4: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 61ms/step - loss: 6.7854 - accuracy: 0.3917 - val_loss: 17.0421 - val_accuracy: 0.1704\n",
            "Epoch 5/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 5.5776 - accuracy: 0.3890\n",
            "Epoch 5: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 5.5776 - accuracy: 0.3890 - val_loss: 16.0977 - val_accuracy: 0.1704\n",
            "Epoch 6/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 4.7570 - accuracy: 0.3938\n",
            "Epoch 6: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 4.7503 - accuracy: 0.3924 - val_loss: 12.7375 - val_accuracy: 0.1704\n",
            "Epoch 7/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 4.1280 - accuracy: 0.3951\n",
            "Epoch 7: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 4.1228 - accuracy: 0.3949 - val_loss: 13.1912 - val_accuracy: 0.1704\n",
            "Epoch 8/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 3.6479 - accuracy: 0.4014\n",
            "Epoch 8: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 3.6470 - accuracy: 0.4005 - val_loss: 13.1334 - val_accuracy: 0.1704\n",
            "Epoch 9/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 3.2610 - accuracy: 0.4037\n",
            "Epoch 9: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 3.2578 - accuracy: 0.4026 - val_loss: 13.1679 - val_accuracy: 0.1704\n",
            "Epoch 10/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.9736 - accuracy: 0.4133\n",
            "Epoch 10: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.9736 - accuracy: 0.4133 - val_loss: 10.6547 - val_accuracy: 0.1704\n",
            "Epoch 11/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.7275 - accuracy: 0.4101\n",
            "Epoch 11: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.7275 - accuracy: 0.4101 - val_loss: 10.1815 - val_accuracy: 0.1704\n",
            "Epoch 12/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.5351 - accuracy: 0.4160\n",
            "Epoch 12: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.5328 - accuracy: 0.4158 - val_loss: 12.3886 - val_accuracy: 0.1704\n",
            "Epoch 13/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.3945 - accuracy: 0.4112\n",
            "Epoch 13: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 2.3945 - accuracy: 0.4112 - val_loss: 13.0917 - val_accuracy: 0.1704\n",
            "Epoch 14/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.2695 - accuracy: 0.4193\n",
            "Epoch 14: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.2688 - accuracy: 0.4181 - val_loss: 9.3406 - val_accuracy: 0.1704\n",
            "Epoch 15/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.1542 - accuracy: 0.4287\n",
            "Epoch 15: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.1538 - accuracy: 0.4277 - val_loss: 11.1115 - val_accuracy: 0.1704\n",
            "Epoch 16/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.0648 - accuracy: 0.4203\n",
            "Epoch 16: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.0648 - accuracy: 0.4191 - val_loss: 9.3254 - val_accuracy: 0.1704\n",
            "Epoch 17/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.0008 - accuracy: 0.4125\n",
            "Epoch 17: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.9981 - accuracy: 0.4145 - val_loss: 7.5351 - val_accuracy: 0.1704\n",
            "Epoch 18/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.9395 - accuracy: 0.4141\n",
            "Epoch 18: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.9393 - accuracy: 0.4143 - val_loss: 12.0985 - val_accuracy: 0.1704\n",
            "Epoch 19/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8925 - accuracy: 0.4279\n",
            "Epoch 19: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.8918 - accuracy: 0.4287 - val_loss: 8.6614 - val_accuracy: 0.1704\n",
            "Epoch 20/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8458 - accuracy: 0.4189\n",
            "Epoch 20: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.8423 - accuracy: 0.4201 - val_loss: 10.5856 - val_accuracy: 0.1704\n",
            "Epoch 21/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8019 - accuracy: 0.4199\n",
            "Epoch 21: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.8020 - accuracy: 0.4210 - val_loss: 7.3261 - val_accuracy: 0.1704\n",
            "Epoch 22/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7696 - accuracy: 0.4299\n",
            "Epoch 22: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.7695 - accuracy: 0.4300 - val_loss: 8.5117 - val_accuracy: 0.1704\n",
            "Epoch 23/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.7510 - accuracy: 0.4279\n",
            "Epoch 23: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.7510 - accuracy: 0.4279 - val_loss: 5.7290 - val_accuracy: 0.1704\n",
            "Epoch 24/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7172 - accuracy: 0.4162\n",
            "Epoch 24: val_accuracy improved from 0.17039 to 0.17084, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 67ms/step - loss: 1.7162 - accuracy: 0.4164 - val_loss: 4.8759 - val_accuracy: 0.1708\n",
            "Epoch 25/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6907 - accuracy: 0.4293\n",
            "Epoch 25: val_accuracy did not improve from 0.17084\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.6906 - accuracy: 0.4294 - val_loss: 7.9612 - val_accuracy: 0.1704\n",
            "Epoch 26/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6843 - accuracy: 0.4205\n",
            "Epoch 26: val_accuracy did not improve from 0.17084\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.6846 - accuracy: 0.4197 - val_loss: 5.8474 - val_accuracy: 0.1704\n",
            "Epoch 27/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6551 - accuracy: 0.4283\n",
            "Epoch 27: val_accuracy did not improve from 0.17084\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.6553 - accuracy: 0.4283 - val_loss: 7.3069 - val_accuracy: 0.1704\n",
            "Epoch 28/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.6692 - accuracy: 0.4210\n",
            "Epoch 28: val_accuracy did not improve from 0.17084\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.6692 - accuracy: 0.4210 - val_loss: 10.8056 - val_accuracy: 0.1704\n",
            "Epoch 29/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.6443 - accuracy: 0.4308\n",
            "Epoch 29: val_accuracy did not improve from 0.17084\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.6443 - accuracy: 0.4308 - val_loss: 9.5305 - val_accuracy: 0.1704\n",
            "Epoch 30/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6457 - accuracy: 0.4330\n",
            "Epoch 30: val_accuracy did not improve from 0.17084\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.6456 - accuracy: 0.4329 - val_loss: 5.6543 - val_accuracy: 0.1704\n",
            "Epoch 31/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6241 - accuracy: 0.4322\n",
            "Epoch 31: val_accuracy did not improve from 0.17084\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.6242 - accuracy: 0.4319 - val_loss: 10.0455 - val_accuracy: 0.1704\n",
            "Epoch 32/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6133 - accuracy: 0.4279\n",
            "Epoch 32: val_accuracy improved from 0.17084 to 0.20572, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 74ms/step - loss: 1.6128 - accuracy: 0.4277 - val_loss: 6.0711 - val_accuracy: 0.2057\n",
            "Epoch 33/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6176 - accuracy: 0.4367\n",
            "Epoch 33: val_accuracy did not improve from 0.20572\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.6186 - accuracy: 0.4356 - val_loss: 10.5467 - val_accuracy: 0.1704\n",
            "Epoch 34/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6077 - accuracy: 0.4318\n",
            "Epoch 34: val_accuracy did not improve from 0.20572\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.6067 - accuracy: 0.4327 - val_loss: 12.8281 - val_accuracy: 0.1704\n",
            "Epoch 35/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.6058 - accuracy: 0.4314\n",
            "Epoch 35: val_accuracy did not improve from 0.20572\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.6057 - accuracy: 0.4319 - val_loss: 10.6672 - val_accuracy: 0.1704\n",
            "Epoch 36/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5961 - accuracy: 0.4354\n",
            "Epoch 36: val_accuracy did not improve from 0.20572\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.5973 - accuracy: 0.4354 - val_loss: 6.7470 - val_accuracy: 0.1704\n",
            "Epoch 37/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5814 - accuracy: 0.4336\n",
            "Epoch 37: val_accuracy did not improve from 0.20572\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5806 - accuracy: 0.4329 - val_loss: 11.1513 - val_accuracy: 0.1704\n",
            "Epoch 38/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5844 - accuracy: 0.4320\n",
            "Epoch 38: val_accuracy did not improve from 0.20572\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5826 - accuracy: 0.4344 - val_loss: 8.5962 - val_accuracy: 0.1704\n",
            "Epoch 39/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5823 - accuracy: 0.4357\n",
            "Epoch 39: val_accuracy did not improve from 0.20572\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5812 - accuracy: 0.4363 - val_loss: 12.9029 - val_accuracy: 0.1704\n",
            "Epoch 40/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5803 - accuracy: 0.4363\n",
            "Epoch 40: val_accuracy did not improve from 0.20572\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5789 - accuracy: 0.4373 - val_loss: 7.2315 - val_accuracy: 0.1704\n",
            "Epoch 41/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.5640 - accuracy: 0.4329\n",
            "Epoch 41: val_accuracy did not improve from 0.20572\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.5640 - accuracy: 0.4329 - val_loss: 10.8557 - val_accuracy: 0.1704\n",
            "Epoch 42/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5780 - accuracy: 0.4273\n",
            "Epoch 42: val_accuracy did not improve from 0.20572\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5761 - accuracy: 0.4281 - val_loss: 10.9668 - val_accuracy: 0.1704\n",
            "Epoch 43/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5590 - accuracy: 0.4311\n",
            "Epoch 43: val_accuracy did not improve from 0.20572\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5632 - accuracy: 0.4304 - val_loss: 8.8155 - val_accuracy: 0.1704\n",
            "Epoch 44/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5605 - accuracy: 0.4410\n",
            "Epoch 44: val_accuracy did not improve from 0.20572\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5623 - accuracy: 0.4396 - val_loss: 6.4853 - val_accuracy: 0.1704\n",
            "Epoch 45/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.5541 - accuracy: 0.4417\n",
            "Epoch 45: val_accuracy did not improve from 0.20572\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.5541 - accuracy: 0.4417 - val_loss: 10.3146 - val_accuracy: 0.1704\n",
            "Epoch 46/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5602 - accuracy: 0.4365\n",
            "Epoch 46: val_accuracy did not improve from 0.20572\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5621 - accuracy: 0.4360 - val_loss: 8.1932 - val_accuracy: 0.1704\n",
            "Epoch 47/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5429 - accuracy: 0.4355\n",
            "Epoch 47: val_accuracy did not improve from 0.20572\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5459 - accuracy: 0.4354 - val_loss: 8.6283 - val_accuracy: 0.1704\n",
            "Epoch 48/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5576 - accuracy: 0.4408\n",
            "Epoch 48: val_accuracy did not improve from 0.20572\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.5582 - accuracy: 0.4394 - val_loss: 9.8798 - val_accuracy: 0.1704\n",
            "Epoch 49/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.5541 - accuracy: 0.4423\n",
            "Epoch 49: val_accuracy did not improve from 0.20572\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.5541 - accuracy: 0.4423 - val_loss: 11.7707 - val_accuracy: 0.1704\n",
            "Epoch 50/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.5558 - accuracy: 0.4330\n",
            "Epoch 50: val_accuracy improved from 0.20572 to 0.26029, saving model to best_model.h5\n",
            "41/41 [==============================] - 3s 75ms/step - loss: 1.5559 - accuracy: 0.4342 - val_loss: 4.6686 - val_accuracy: 0.2603\n",
            "reg_param=0.01, dr=0.25, test_acc=0.2603\n",
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_112 (Conv1D)         (None, 301, 128)          512       \n",
            "                                                                 \n",
            " batch_normalization_189 (Ba  (None, 301, 128)         512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_189 (Activation)  (None, 301, 128)         0         \n",
            "                                                                 \n",
            " max_pooling1d_112 (MaxPooli  (None, 150, 128)         0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_113 (Conv1D)         (None, 148, 256)          98560     \n",
            "                                                                 \n",
            " batch_normalization_190 (Ba  (None, 148, 256)         1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_190 (Activation)  (None, 148, 256)         0         \n",
            "                                                                 \n",
            " max_pooling1d_113 (MaxPooli  (None, 74, 256)          0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_114 (Conv1D)         (None, 72, 512)           393728    \n",
            "                                                                 \n",
            " batch_normalization_191 (Ba  (None, 72, 512)          2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_191 (Activation)  (None, 72, 512)          0         \n",
            "                                                                 \n",
            " max_pooling1d_114 (MaxPooli  (None, 36, 512)          0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_115 (Conv1D)         (None, 34, 1024)          1573888   \n",
            "                                                                 \n",
            " batch_normalization_192 (Ba  (None, 34, 1024)         4096      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_192 (Activation)  (None, 34, 1024)         0         \n",
            "                                                                 \n",
            " max_pooling1d_115 (MaxPooli  (None, 17, 1024)         0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " flatten_28 (Flatten)        (None, 17408)             0         \n",
            "                                                                 \n",
            " dense_112 (Dense)           (None, 512)               8913408   \n",
            "                                                                 \n",
            " batch_normalization_193 (Ba  (None, 512)              2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_193 (Activation)  (None, 512)              0         \n",
            "                                                                 \n",
            " dropout_84 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_113 (Dense)           (None, 256)               131328    \n",
            "                                                                 \n",
            " batch_normalization_194 (Ba  (None, 256)              1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_194 (Activation)  (None, 256)              0         \n",
            "                                                                 \n",
            " dropout_85 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_114 (Dense)           (None, 128)               32896     \n",
            "                                                                 \n",
            " batch_normalization_195 (Ba  (None, 128)              512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_195 (Activation)  (None, 128)              0         \n",
            "                                                                 \n",
            " dropout_86 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_115 (Dense)           (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,156,358\n",
            "Trainable params: 11,150,726\n",
            "Non-trainable params: 5,632\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 20.5208 - accuracy: 0.3352\n",
            "Epoch 1: val_accuracy improved from -inf to 0.17039, saving model to best_model.h5\n",
            "41/41 [==============================] - 6s 79ms/step - loss: 20.4141 - accuracy: 0.3361 - val_loss: 15.8443 - val_accuracy: 0.1704\n",
            "Epoch 2/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 11.9762 - accuracy: 0.3719\n",
            "Epoch 2: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 11.9379 - accuracy: 0.3723 - val_loss: 15.3865 - val_accuracy: 0.1704\n",
            "Epoch 3/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 8.5667 - accuracy: 0.3693\n",
            "Epoch 3: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 8.5472 - accuracy: 0.3700 - val_loss: 14.3124 - val_accuracy: 0.1704\n",
            "Epoch 4/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 6.7314 - accuracy: 0.3787\n",
            "Epoch 4: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 6.7193 - accuracy: 0.3796 - val_loss: 14.9541 - val_accuracy: 0.1704\n",
            "Epoch 5/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 5.5833 - accuracy: 0.3773\n",
            "Epoch 5: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 5.5729 - accuracy: 0.3790 - val_loss: 14.0374 - val_accuracy: 0.1704\n",
            "Epoch 6/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 4.7585 - accuracy: 0.3947\n",
            "Epoch 6: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 4.7535 - accuracy: 0.3951 - val_loss: 14.6222 - val_accuracy: 0.1704\n",
            "Epoch 7/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 4.1306 - accuracy: 0.3961\n",
            "Epoch 7: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 4.1306 - accuracy: 0.3961 - val_loss: 12.9838 - val_accuracy: 0.1704\n",
            "Epoch 8/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 3.6583 - accuracy: 0.4029\n",
            "Epoch 8: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 3.6547 - accuracy: 0.4028 - val_loss: 10.2899 - val_accuracy: 0.1704\n",
            "Epoch 9/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 3.3062 - accuracy: 0.3980\n",
            "Epoch 9: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 3.3040 - accuracy: 0.3974 - val_loss: 13.2316 - val_accuracy: 0.1704\n",
            "Epoch 10/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.9964 - accuracy: 0.4051\n",
            "Epoch 10: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.9964 - accuracy: 0.4051 - val_loss: 10.7752 - val_accuracy: 0.1704\n",
            "Epoch 11/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.7459 - accuracy: 0.4096\n",
            "Epoch 11: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 2.7471 - accuracy: 0.4084 - val_loss: 9.7293 - val_accuracy: 0.1704\n",
            "Epoch 12/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.5667 - accuracy: 0.4096\n",
            "Epoch 12: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.5664 - accuracy: 0.4086 - val_loss: 10.6079 - val_accuracy: 0.1704\n",
            "Epoch 13/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 2.4079 - accuracy: 0.4122\n",
            "Epoch 13: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 2.4079 - accuracy: 0.4122 - val_loss: 10.1724 - val_accuracy: 0.1704\n",
            "Epoch 14/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.2678 - accuracy: 0.4102\n",
            "Epoch 14: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 2.2651 - accuracy: 0.4103 - val_loss: 11.4603 - val_accuracy: 0.1704\n",
            "Epoch 15/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.1686 - accuracy: 0.4211\n",
            "Epoch 15: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.1660 - accuracy: 0.4212 - val_loss: 11.1288 - val_accuracy: 0.1704\n",
            "Epoch 16/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 2.0699 - accuracy: 0.4123\n",
            "Epoch 16: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.0697 - accuracy: 0.4128 - val_loss: 10.8638 - val_accuracy: 0.1704\n",
            "Epoch 17/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.9951 - accuracy: 0.4189\n",
            "Epoch 17: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.9942 - accuracy: 0.4183 - val_loss: 10.9693 - val_accuracy: 0.1704\n",
            "Epoch 18/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.9446 - accuracy: 0.4186\n",
            "Epoch 18: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.9429 - accuracy: 0.4193 - val_loss: 8.8231 - val_accuracy: 0.1704\n",
            "Epoch 19/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8913 - accuracy: 0.4139\n",
            "Epoch 19: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.8906 - accuracy: 0.4128 - val_loss: 10.2096 - val_accuracy: 0.1704\n",
            "Epoch 20/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.8439 - accuracy: 0.4102\n",
            "Epoch 20: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.8422 - accuracy: 0.4107 - val_loss: 7.7581 - val_accuracy: 0.1704\n",
            "Epoch 21/50\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.8019 - accuracy: 0.4204\n",
            "Epoch 21: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.8019 - accuracy: 0.4204 - val_loss: 12.1848 - val_accuracy: 0.1704\n",
            "Epoch 22/50\n",
            "40/41 [============================>.] - ETA: 0s - loss: 1.7901 - accuracy: 0.4104\n",
            "Epoch 22: val_accuracy did not improve from 0.17039\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 1.7900 - accuracy: 0.4126 - val_loss: 6.6404 - val_accuracy: 0.1704\n",
            "Epoch 23/50\n",
            "34/41 [=======================>......] - ETA: 0s - loss: 1.7445 - accuracy: 0.4285"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-b7c4015b1490>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m# Train the model with the current hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_model.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             history = model.fit(X_train, y_train,\n\u001b[0m\u001b[1;32m     61\u001b[0m                                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1689\u001b[0m                             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m                             \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1691\u001b[0;31m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1692\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \"\"\"\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m             \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;31m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;31m# as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1158\u001b[0m     \"\"\"\n\u001b[1;32m   1159\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1124\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1126\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1127\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    batch_size=128,\n",
        "                    validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "FBFwnhHPNlCj",
        "outputId": "df01235a-1ca9-4935-b93c-e0c4309cc8ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "41/41 [==============================] - 6s 68ms/step - loss: 3.9749 - accuracy: 0.3393 - val_loss: 3.8817 - val_accuracy: 0.1923\n",
            "Epoch 2/50\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 3.4028 - accuracy: 0.3848 - val_loss: 7.2775 - val_accuracy: 0.1704\n",
            "Epoch 3/50\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 3.1223 - accuracy: 0.3965 - val_loss: 8.6258 - val_accuracy: 0.1704\n",
            "Epoch 4/50\n",
            "41/41 [==============================] - 2s 55ms/step - loss: 2.9198 - accuracy: 0.4038 - val_loss: 9.7806 - val_accuracy: 0.1704\n",
            "Epoch 5/50\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.7593 - accuracy: 0.4124 - val_loss: 7.4483 - val_accuracy: 0.1704\n",
            "Epoch 6/50\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.6431 - accuracy: 0.4181 - val_loss: 7.3595 - val_accuracy: 0.1704\n",
            "Epoch 7/50\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 2.5289 - accuracy: 0.4260 - val_loss: 8.6219 - val_accuracy: 0.1704\n",
            "Epoch 8/50\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.4521 - accuracy: 0.4273 - val_loss: 5.6891 - val_accuracy: 0.2267\n",
            "Epoch 9/50\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.3781 - accuracy: 0.4256 - val_loss: 7.3070 - val_accuracy: 0.1704\n",
            "Epoch 10/50\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.3022 - accuracy: 0.4358 - val_loss: 4.8228 - val_accuracy: 0.1780\n",
            "Epoch 11/50\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 2.2261 - accuracy: 0.4471 - val_loss: 5.5158 - val_accuracy: 0.1704\n",
            "Epoch 12/50\n",
            "41/41 [==============================] - 3s 62ms/step - loss: 2.1820 - accuracy: 0.4440 - val_loss: 3.9015 - val_accuracy: 0.1811\n",
            "Epoch 13/50\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 2.1451 - accuracy: 0.4540 - val_loss: 3.6822 - val_accuracy: 0.2665\n",
            "Epoch 14/50\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 2.0957 - accuracy: 0.4463 - val_loss: 4.6462 - val_accuracy: 0.2156\n",
            "Epoch 15/50\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 2.0583 - accuracy: 0.4551 - val_loss: 5.5004 - val_accuracy: 0.1717\n",
            "Epoch 16/50\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 2.0259 - accuracy: 0.4542 - val_loss: 3.8075 - val_accuracy: 0.2156\n",
            "Epoch 17/50\n",
            "41/41 [==============================] - 2s 60ms/step - loss: 2.0055 - accuracy: 0.4507 - val_loss: 4.8296 - val_accuracy: 0.2272\n",
            "Epoch 18/50\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.9677 - accuracy: 0.4613 - val_loss: 3.7041 - val_accuracy: 0.2728\n",
            "Epoch 19/50\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.9378 - accuracy: 0.4712 - val_loss: 3.4038 - val_accuracy: 0.2661\n",
            "Epoch 20/50\n",
            "41/41 [==============================] - 2s 61ms/step - loss: 1.9034 - accuracy: 0.4724 - val_loss: 2.7850 - val_accuracy: 0.2724\n",
            "Epoch 21/50\n",
            "41/41 [==============================] - 3s 63ms/step - loss: 1.8756 - accuracy: 0.4781 - val_loss: 4.4009 - val_accuracy: 0.2187\n",
            "Epoch 22/50\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.8509 - accuracy: 0.4783 - val_loss: 3.0445 - val_accuracy: 0.2710\n",
            "Epoch 23/50\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.8365 - accuracy: 0.4831 - val_loss: 3.9942 - val_accuracy: 0.2030\n",
            "Epoch 24/50\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.8108 - accuracy: 0.4921 - val_loss: 4.9534 - val_accuracy: 0.2657\n",
            "Epoch 25/50\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.7932 - accuracy: 0.4866 - val_loss: 4.0795 - val_accuracy: 0.2393\n",
            "Epoch 26/50\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.7655 - accuracy: 0.5002 - val_loss: 3.1814 - val_accuracy: 0.3354\n",
            "Epoch 27/50\n",
            "41/41 [==============================] - 2s 55ms/step - loss: 1.7378 - accuracy: 0.5040 - val_loss: 3.7569 - val_accuracy: 0.2196\n",
            "Epoch 28/50\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.7243 - accuracy: 0.5052 - val_loss: 4.7190 - val_accuracy: 0.1775\n",
            "Epoch 29/50\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.7215 - accuracy: 0.5084 - val_loss: 5.8269 - val_accuracy: 0.2115\n",
            "Epoch 30/50\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.7144 - accuracy: 0.5121 - val_loss: 3.5853 - val_accuracy: 0.2299\n",
            "Epoch 31/50\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.6936 - accuracy: 0.5136 - val_loss: 3.0460 - val_accuracy: 0.2567\n",
            "Epoch 32/50\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.6705 - accuracy: 0.5299 - val_loss: 3.3148 - val_accuracy: 0.2683\n",
            "Epoch 33/50\n",
            "41/41 [==============================] - 2s 55ms/step - loss: 1.6673 - accuracy: 0.5257 - val_loss: 4.3220 - val_accuracy: 0.2634\n",
            "Epoch 34/50\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.6465 - accuracy: 0.5368 - val_loss: 4.2576 - val_accuracy: 0.2674\n",
            "Epoch 35/50\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.6185 - accuracy: 0.5451 - val_loss: 3.8793 - val_accuracy: 0.2742\n",
            "Epoch 36/50\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.5863 - accuracy: 0.5617 - val_loss: 3.3623 - val_accuracy: 0.3305\n",
            "Epoch 37/50\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.5704 - accuracy: 0.5690 - val_loss: 3.7627 - val_accuracy: 0.2885\n",
            "Epoch 38/50\n",
            "41/41 [==============================] - 2s 55ms/step - loss: 1.5504 - accuracy: 0.5677 - val_loss: 3.5627 - val_accuracy: 0.3238\n",
            "Epoch 39/50\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.5215 - accuracy: 0.5824 - val_loss: 3.4599 - val_accuracy: 0.2719\n",
            "Epoch 40/50\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5081 - accuracy: 0.5903 - val_loss: 4.6634 - val_accuracy: 0.2120\n",
            "Epoch 41/50\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.5083 - accuracy: 0.5974 - val_loss: 3.3212 - val_accuracy: 0.2469\n",
            "Epoch 42/50\n",
            "41/41 [==============================] - 2s 57ms/step - loss: 1.4712 - accuracy: 0.6145 - val_loss: 3.9765 - val_accuracy: 0.2657\n",
            "Epoch 43/50\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.4515 - accuracy: 0.6225 - val_loss: 3.8213 - val_accuracy: 0.2165\n",
            "Epoch 44/50\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.4367 - accuracy: 0.6292 - val_loss: 3.8027 - val_accuracy: 0.2124\n",
            "Epoch 45/50\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.4088 - accuracy: 0.6467 - val_loss: 2.9652 - val_accuracy: 0.2903\n",
            "Epoch 46/50\n",
            "41/41 [==============================] - 2s 59ms/step - loss: 1.3960 - accuracy: 0.6465 - val_loss: 3.6172 - val_accuracy: 0.2563\n",
            "Epoch 47/50\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.3549 - accuracy: 0.6752 - val_loss: 4.5434 - val_accuracy: 0.1990\n",
            "Epoch 48/50\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.3044 - accuracy: 0.7002 - val_loss: 4.2984 - val_accuracy: 0.2133\n",
            "Epoch 49/50\n",
            "41/41 [==============================] - 2s 58ms/step - loss: 1.2878 - accuracy: 0.7051 - val_loss: 3.5843 - val_accuracy: 0.2446\n",
            "Epoch 50/50\n",
            "41/41 [==============================] - 2s 56ms/step - loss: 1.2665 - accuracy: 0.7168 - val_loss: 3.2799 - val_accuracy: 0.3189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8S6b6gH4NsDC"
      },
      "execution_count": 56,
      "outputs": []
    }
  ]
}